<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>KeyPears Blog</title>
        <link>https://keypears.com</link>
        <description>Updates and insights from the KeyPears team</description>
        <lastBuildDate>Sun, 30 Nov 2025 20:39:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <copyright>Copyright 2025 Identellica LLC</copyright>
        <atom:link href="https://keypears.com/blog/feed.xml" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Building Secure Cross-Device Sync for a Decentralized Password Manager]]></title>
            <link>https://keypears.com/blog/2025-11-30-cross-device-sync</link>
            <guid isPermaLink="false">https://keypears.com/blog/2025-11-30-cross-device-sync</guid>
            <pubDate>Mon, 01 Dec 2025 00:00:00 GMT</pubDate>
            <description><![CDATA[<p>After weeks of intensive development, KeyPears now has a fully functional
cross-device synchronization system. This wasn't just about making data appear
on multiple devices—it was about building a ...]]></description>
            <content:encoded><![CDATA[<p>After weeks of intensive development, KeyPears now has a fully functional
cross-device synchronization system. This wasn't just about making data appear
on multiple devices—it was about building a secure, privacy-preserving sync
architecture that works in a decentralized environment where users can run their
own servers. Here's how we did it.</p>
<h2>The Challenge</h2>
<p>Building sync for a password manager is fundamentally different from typical app
synchronization. Every design decision has security implications. When you're
also committed to a decentralized architecture where users might run their own
servers, the complexity multiplies. We needed to solve several interconnected
problems:</p>
<ol>
<li><strong>Authentication without centralization</strong> - No global user accounts or OAuth
providers</li>
<li><strong>Device identity with privacy</strong> - Track devices without cross-domain
correlation</li>
<li><strong>Session management at scale</strong> - Support hundreds of API calls without
exposing long-term credentials</li>
<li><strong>Sync conflict resolution</strong> - Handle concurrent edits from multiple devices</li>
<li><strong>Zero-knowledge architecture</strong> - Servers should never see passwords or
encryption keys</li>
</ol>
<h2>The Solution: 2,000+ Lines of Carefully Crafted Code</h2>
<p>Over the past week, we've implemented a comprehensive solution spanning 40 files
with over 2,000 lines of new code. Here's what we built:</p>
<h3>1. Session-Based Authentication System</h3>
<p>The biggest change was moving from a "login key with every request" model to
proper session-based authentication. This reduced our attack surface
dramatically:</p>
<p><strong>Before:</strong> Login key sent 100+ times per session <strong>After:</strong> Login key sent once
per 24 hours</p>
<p>The new authentication flow works like this:</p>
<pre><code class="language-typescript">// Login once per day
const { sessionToken, expiresAt } = await client.login({
  vaultId,
  loginKey,
  deviceId,
  deviceDescription: "macOS 14.1 (aarch64)"
});

// Use session token for all subsequent requests
const secrets = await client.getSecretUpdates({
  vaultId,
  lastUpdatedAt: lastSync
});
</code></pre>
<p>But here's the security innovation: we store session tokens as Blake3 hashes in
the database. Even if someone breaches the server database, they can't use the
stolen hashes—they'd need the original 32-byte random tokens, which exist only
in client memory.</p>
<h3>2. Privacy-Preserving Device Tracking</h3>
<p>Unlike centralized password managers that assign global device IDs, KeyPears
generates a unique device ID for each vault. This means:</p>
<ul>
<li>Your work vault and personal vault have different device IDs</li>
<li>Servers can't correlate devices across domains</li>
<li>Complete privacy preservation in a decentralized architecture</li>
</ul>
<p>Each device gets identified with:</p>
<ul>
<li>A ULID (Universally Unique Lexicographically Sortable Identifier) per vault</li>
<li>Auto-detected OS information: "iPhone (iOS 17.2)" or "Windows 11 (x86_64)"</li>
<li>User-editable friendly names: "Ryan's MacBook Pro"</li>
</ul>
<h3>3. Background Sync Service</h3>
<p>We built a robust background synchronization service that polls for changes
every 5 seconds:</p>
<pre><code class="language-typescript">// Start sync when vault is unlocked
startBackgroundSync(vaultId, vaultDomain, vaultKey, getSession);

// Automatic sync every 5 seconds
// Manual sync after creating/editing secrets
await triggerManualSync();
</code></pre>
<p>The sync service includes sophisticated error handling:</p>
<ul>
<li><strong>401 Unauthorized</strong>: Stop syncing, prompt for re-authentication</li>
<li><strong>500+ Server Error</strong>: Exponential backoff (5s → 10s → 20s)</li>
<li><strong>Network Error</strong>: Keep retrying at normal interval</li>
<li><strong>Session Expiring</strong>: Skip sync, avoid unnecessary 401s</li>
</ul>
<h3>4. Three-Tier Key Derivation System</h3>
<p>We implemented a sophisticated key hierarchy that separates authentication from
encryption:</p>
<pre><code>Master Password
    ↓ (100k rounds PBKDF)
Password Key (cached, PIN-encrypted)
    ├→ Encryption Key (device only, decrypts vault)
    └→ Login Key (sent to server for auth)
        ↓ (1k rounds on server)
    Hashed Login Key (database storage)
</code></pre>
<p>This asymmetric round count (100k client, 1k server) is intentional—the heavy
computation happens client-side for security, while the server just needs to
prevent raw token storage.</p>
<h3>5. Comprehensive Test Coverage</h3>
<p>We didn't just write code; we wrote tests. Lots of them:</p>
<ul>
<li><strong>283 lines</strong> of authentication tests</li>
<li><strong>278 lines</strong> of device session tests</li>
<li>Integration tests for the complete sync flow</li>
<li>Database migration tests for schema changes</li>
</ul>
<h2>Implementation Highlights</h2>
<h3>React Closure Bug Fix</h3>
<p>One of the trickiest bugs involved React closures capturing stale state. The
sync service was always getting a <code>null</code> session token because the closure
captured the initial value:</p>
<pre><code class="language-typescript">// BUG: Closure captures initial null value
startBackgroundSync(vaultId, domain, key, () => session?.token);

// FIX: Use ref to get current value
const sessionRef = useRef(session);
useEffect(() => { sessionRef.current = session; }, [session]);
startBackgroundSync(vaultId, domain, key, () => sessionRef.current?.token);
</code></pre>
<h3>Tauri Plugin Integration</h3>
<p>We integrated Tauri's OS detection plugin to automatically identify devices:</p>
<pre><code class="language-rust">// Rust side
.plugin(tauri_plugin_os::init())

// TypeScript side
import { platform, version, arch } from "@tauri-apps/plugin-os";
const description = `${platform()} ${version()} (${arch()})`;
</code></pre>
<h3>Database Schema Evolution</h3>
<p>Added a new <code>device_session</code> table with careful constraints:</p>
<pre><code class="language-sql">CREATE TABLE device_session (
  id TEXT PRIMARY KEY,           -- ULID
  vault_id TEXT NOT NULL,
  device_id TEXT NOT NULL,        -- Per-vault device ULID
  hashed_session_token TEXT,      -- Blake3 hash, not raw token
  expires_at INTEGER NOT NULL,
  last_activity INTEGER NOT NULL,
  UNIQUE(vault_id, device_id),    -- One session per device
  FOREIGN KEY(vault_id) REFERENCES vault(id) ON DELETE CASCADE
);
</code></pre>
<h2>Security Improvements</h2>
<p>The new system dramatically improves our security posture:</p>
<p>| Attack Vector         | Before             | After                  |
| --------------------- | ------------------ | ---------------------- |
| Token Interception    | Permanent access   | 24-hour maximum window |
| Database Breach       | Login keys exposed | Only unusable hashes   |
| Device Compromise     | No revocation      | Per-device logout      |
| Replay Attacks        | Vulnerable         | Time-limited tokens    |
| Cross-Domain Tracking | Possible           | Prevented by design    |</p>
<h2>Performance Optimizations</h2>
<p>Beyond security, we optimized for performance:</p>
<ol>
<li><strong>Smart Polling</strong>: Only sync when there's an active session</li>
<li><strong>Exponential Backoff</strong>: Reduce server load during errors</li>
<li><strong>Debounced Updates</strong>: Batch rapid changes together</li>
<li><strong>Selective Sync</strong>: Only fetch changes since last sync timestamp</li>
</ol>
<h2>What's Next</h2>
<p>With cross-device sync complete, KeyPears is approaching feature parity with
centralized password managers—while maintaining its decentralized,
privacy-preserving architecture. The foundation we've built enables future
features like:</p>
<ul>
<li>Multi-factor authentication (MFA)</li>
<li>Device trust levels and approval workflows</li>
<li>Geographic anomaly detection</li>
<li>Granular access controls</li>
<li>Offline-first mobile apps</li>
</ul>
<h2>Technical Details</h2>
<p>For the curious, here's the full scope of changes:</p>
<ul>
<li><strong>2,018 lines</strong> of code across <strong>40 files</strong></li>
<li><strong>13 new API endpoints</strong> for authentication and sync</li>
<li><strong>2 new database tables</strong> with migration scripts</li>
<li><strong>166 lines</strong> of sync service implementation</li>
<li><strong>Test coverage</strong> for all critical paths</li>
</ul>
<p>The complete implementation is open source and available in our
<a href="https://github.com/identellica/keypears">GitHub repository</a>.</p>
<h2>Conclusion</h2>
<p>Building secure cross-device sync for a decentralized password manager required
rethinking traditional approaches. By combining session-based authentication,
privacy-preserving device tracking, and sophisticated key derivation, we've
created a system that's both secure and user-friendly.</p>
<p>The key insight: security doesn't require sacrificing usability or privacy. With
careful architecture and attention to detail, we can build systems that protect
users without compromising their autonomy.</p>
<p>KeyPears now syncs your passwords across all your devices—instantly, securely,
and privately. Whether you're using our hosted service or running your own
server, your secrets stay yours.</p>
<p><em>Next up: Implementing the Diffie-Hellman key exchange protocol for secure
secret sharing between users. Stay tuned!</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TypeScript for the KeyPears MVP: Why We're Not Really Using Rust (Yet)]]></title>
            <link>https://keypears.com/blog/2025-11-16-typescript-for-mvp</link>
            <guid isPermaLink="false">https://keypears.com/blog/2025-11-16-typescript-for-mvp</guid>
            <pubDate>Sun, 16 Nov 2025 12:00:00 GMT</pubDate>
            <description><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and
cryptocurrency wallet. The design decisions described here represent our
development approach and may evolve b...]]></description>
            <content:encoded><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and
cryptocurrency wallet. The design decisions described here represent our
development approach and may evolve before our official release.</p>
<p>Three weeks ago, we published a blog post titled "Building KeyPears with Rust:
Backend Architecture and Blake3 Proof-of-Concept." We were excited about Rust's
performance, memory safety, and type system. We had a working <code>/api/blake3</code>
endpoint. We had plans for <code>rs-lib</code> and <code>rs-node</code> packages.</p>
<p>Today, we're writing to tell you we've changed direction.</p>
<p>The current KeyPears codebase is <strong>almost entirely TypeScript</strong>. The Rust
backend from our October post—<code>rs-lib</code> for cryptography and <code>rs-node</code> for the
API server—was fully built and working. And then we deleted it. After several
weeks of development with both implementations side by side, we concluded that
TypeScript is the right architecture for our MVP.</p>
<p>This post explains why we made that decision.</p>
<h2>What Actually Happened</h2>
<p>Let's start with the facts. Here's what we did:</p>
<p><strong>October 2025:</strong> Built a complete Rust backend</p>
<ul>
<li><code>rs-lib</code>: Full cryptography library (Blake3, ACB3, key derivation)</li>
<li><code>rs-node</code>: Axum-based API server with OpenAPI via utoipa</li>
<li>Dual-server deployment: Node.js webapp proxying to Rust API</li>
<li>Everything worked as described in the October blog post</li>
</ul>
<p><strong>November 2025:</strong> Removed the entire Rust backend</p>
<ul>
<li>Deleted <code>rs-lib</code> package completely</li>
<li>Deleted <code>rs-node</code> package completely</li>
<li>Rewrote cryptography in TypeScript using <code>@webbuf</code> WASM packages</li>
<li>Rewrote API server in TypeScript using orpc</li>
<li>Integrated API server directly into Express webapp</li>
</ul>
<p><strong>Current state:</strong></p>
<ul>
<li><strong>Rust code:</strong> 33 lines total (just the minimal Tauri shell)</li>
<li><strong>TypeScript code:</strong> ~5,400 lines (lib, api-server, tauri app, webapp)</li>
<li>All cryptography now TypeScript + WASM</li>
<li>All API endpoints now orpc (TypeScript RPC)</li>
<li>Single-server deployment (no more Node → Rust proxy)</li>
</ul>
<p>This wasn't a case of the Rust backend "not working out." It worked perfectly.
We had working Blake3 hashing, working key derivation, working API endpoints. We
deleted it anyway because <strong>TypeScript simplifies development in ways that
matter more than Rust's advantages for our MVP.</strong></p>
<h2>Why We Removed the Rust Backend</h2>
<p>With both implementations working, we had to make a choice: continue maintaining
two parallel implementations (Rust for crypto/API, TypeScript for UI) or
consolidate on one language. We chose TypeScript for three critical reasons:</p>
<p><strong>1. Better API tooling</strong> - orpc provides superior type safety compared to
Axum + utoipa + openapi-generator</p>
<p><strong>2. Better database tooling</strong> - Drizzle ORM supports both SQLite and PostgreSQL
with the same API (no Rust equivalent exists)</p>
<p><strong>3. Single-language simplicity</strong> - Avoiding context switching between Rust and
TypeScript saves mental overhead on a side project</p>
<p>Here's what we learned by building and then removing the Rust backend:</p>
<h3>1. orpc vs Axum + utoipa: Type Safety Without Codegen</h3>
<p>We built the Rust API server with Axum and <code>utoipa</code> for OpenAPI generation. It
worked, but the workflow had friction:</p>
<p><strong>The Rust approach we actually used:</strong></p>
<ol>
<li>Define routes in Rust with Axum</li>
<li>Generate OpenAPI spec with <code>utoipa</code> macros</li>
<li>Run <code>openapi-generator</code> to create TypeScript client</li>
<li>Discover generated client doesn't match our TypeScript patterns</li>
<li>Manually adjust generated code or fix Rust annotations</li>
<li>Repeat on every schema change</li>
</ol>
<p><strong>The TypeScript approach (orpc) we switched to:</strong></p>
<pre><code class="language-typescript">// Define the procedure
export const blake3Procedure = os
  .input(Blake3RequestSchema)
  .output(Blake3ResponseSchema)
  .handler(async ({ input }) => {
    const data = WebBuf.fromBase64(input.data);
    const hash = blake3Hash(data);
    return { hash: hash.buf.toHex() };
  });

// Use it in the client with full type safety
const client = createClient({ url: "/api" });
const result = await client.blake3({ data: "..." });
// TypeScript knows `result.hash` is a string
</code></pre>
<p><strong>Zero codegen. Complete type safety. Instant IDE autocomplete.</strong></p>
<p>The difference is night and day. With orpc, the client knows every endpoint,
every parameter type, every response shape—all inferred directly from the server
code. Change the server? Client errors appear immediately in your IDE, not at
runtime. No build step, no generated files, no version mismatches.</p>
<p>This is what made us delete working Rust code. The Axum + utoipa + codegen
workflow worked, but orpc's zero-codegen type safety is so much better that
maintaining the Rust version wasn't worth it.</p>
<h3>2. No Rust ORM Supports Both SQLite and PostgreSQL Well</h3>
<p>KeyPears needs two databases:</p>
<ul>
<li><strong>SQLite</strong> in the Tauri desktop app (client-side storage)</li>
<li><strong>PostgreSQL</strong> on the server (multi-user vault synchronization)</li>
</ul>
<p>In TypeScript, <strong>Drizzle ORM</strong> handles both with the same API:</p>
<pre><code class="language-typescript">// Client (SQLite)
import { drizzle } from "drizzle-orm/sqlite-proxy";
const db = drizzle(/* Tauri SQL plugin */);

// Server (PostgreSQL)
import { drizzle } from "drizzle-orm/node-postgres";
const db = drizzle(/* pg connection */);

// Same schema definition works for both
export const TableVault = sqliteTable("vault", {
  id: text("id").primaryKey(),
  name: text("name").notNull(),
  // ...
});
</code></pre>
<p>We looked for Rust equivalents. <strong>Diesel</strong> supports Postgres and MySQL but has
poor SQLite support. <strong>SeaORM</strong> is newer but still requires separate schema
definitions for different databases. Neither provides the unified, type-safe
query builder that Drizzle gives us.</p>
<p>When you're building a sync protocol where the client and server need matching
schemas, having one ORM that works everywhere is critical. This was the second
reason we deleted the Rust backend—we would have needed two separate database
implementations (one for Tauri's SQLite, one for the server's Postgres) with
manual work to keep them in sync.</p>
<h3>3. Single Language Reduces Mental Overhead</h3>
<p>The final reason we removed the Rust backend: <strong>context switching costs.</strong></p>
<p>With the dual-language architecture, every feature required:</p>
<ul>
<li>Writing Rust for crypto/API logic</li>
<li>Writing TypeScript for UI/database logic</li>
<li>Translating between Rust and TypeScript idioms</li>
<li>Maintaining two build systems (Cargo + pnpm)</li>
<li>Debugging across language boundaries</li>
<li>Different testing frameworks (Cargo test + Vitest)</li>
</ul>
<p>For a side project where development happens in short evening sessions, this
mental overhead compounds. You spend the first 10 minutes remembering whether
you're writing Rust or TypeScript, and the last 10 minutes before bed context
switching back.</p>
<p>With TypeScript-only:</p>
<ul>
<li>One type system</li>
<li>One package manager</li>
<li>One testing framework</li>
<li>One set of idioms</li>
<li>Hot reload in ~100ms (vs 3-10s Rust recompile)</li>
</ul>
<p>The productivity gain isn't just about compile times. It's about flow state.
When you're not context switching between languages, you write more code and
make fewer mistakes.</p>
<h3>4. Deployment Simplification</h3>
<p>The Rust backend also complicated deployment:</p>
<p><strong>With Rust (October architecture):</strong></p>
<ul>
<li>Dual-server: Node.js webapp (port 4273) + Rust API (port 4274)</li>
<li>HTTP proxy from webapp to Rust server</li>
<li>Docker image: Node + Rust toolchain + cross-compilation</li>
<li>Larger image size (~500MB with Rust)</li>
<li>More complex service coordination</li>
</ul>
<p><strong>With TypeScript-only (current):</strong></p>
<ul>
<li>Single Express server (port 4273)</li>
<li>orpc API mounted directly at <code>/api</code></li>
<li>Docker image: Just Node.js (~200MB)</li>
<li>Simpler deployment (one service, one port)</li>
<li>No HTTP proxy overhead</li>
</ul>
<p>Removing the Rust backend made deployment cleaner and faster.</p>
<h2>What We Didn't Lose: Rust Cryptography via WASM</h2>
<p>Here's the critical insight that made removing the Rust backend viable: <strong>We
still use Rust for cryptography. We just use it through WebAssembly instead of
writing it ourselves.</strong></p>
<p>When we deleted <code>rs-lib</code> (our Rust cryptography library), we didn't rewrite
crypto in pure JavaScript. We switched to the <code>@webbuf</code> packages, which compile
Rust cryptography to WebAssembly:</p>
<ul>
<li><strong><code>@webbuf/blake3</code></strong>: Blake3 hashing (Rust → WASM)</li>
<li><strong><code>@webbuf/acb3</code></strong>: AES-256-CBC + Blake3-MAC (Rust → WASM)</li>
<li><strong><code>@webbuf/webbuf</code></strong>: Binary data utilities (Rust → WASM)</li>
<li><strong><code>@webbuf/fixedbuf</code></strong>: Fixed-size buffers (Rust → WASM)</li>
</ul>
<p>These packages compile Rust cryptography to WebAssembly. We get:</p>
<p>✅ <strong>Rust's memory safety</strong> (WASM sandbox) ✅ <strong>Rust's performance</strong>
(near-native speed) ✅ <strong>Cross-platform consistency</strong> (works in Node, browsers,
Tauri) ✅ <strong>TypeScript ergonomics</strong> (native <code>Uint8Array</code> integration)</p>
<p>Here's our complete three-tier key derivation system in TypeScript:</p>
<pre><code class="language-typescript">// 100,000 rounds of Blake3-based PBKDF
export function blake3Pbkdf(
  password: string | WebBuf,
  salt: FixedBuf&#x3C;32>,
  rounds: number = 100_000,
): FixedBuf&#x3C;32> {
  const passwordBuf = typeof password === "string"
    ? WebBuf.fromUtf8(password)
    : password;

  let result = blake3Mac(salt, passwordBuf);
  for (let i = 1; i &#x3C; rounds; i++) {
    result = blake3Mac(salt, result.buf);
  }
  return result;
}

// Derive password key from user's master password
export function derivePasswordKey(password: string): FixedBuf&#x3C;32> {
  const salt = derivePasswordSalt(password);
  return blake3Pbkdf(password, salt, 100_000);
}

// Derive encryption key (for vault data)
export function deriveEncryptionKey(passwordKey: FixedBuf&#x3C;32>): FixedBuf&#x3C;32> {
  const salt = deriveEncryptionSalt();
  return blake3Pbkdf(passwordKey.buf, salt, 100_000);
}

// Derive login key (sent to server)
export function deriveLoginKey(passwordKey: FixedBuf&#x3C;32>): FixedBuf&#x3C;32> {
  const salt = deriveLoginSalt();
  return blake3Pbkdf(passwordKey.buf, salt, 100_000);
}
</code></pre>
<p>This is production-ready cryptography. It's type-safe. It's fast (200,000 Blake3
operations complete in milliseconds). And the actual hashing happens in
Rust-compiled WASM—the same Rust cryptography we had in <code>rs-lib</code>, just
packaged differently.</p>
<p><strong>We didn't abandon Rust's security properties. We just stopped maintaining our
own Rust codebase.</strong> The cryptography is still Rust. It's just compiled to WASM
and consumed as TypeScript packages, which eliminates the build complexity of a
dual-language project.</p>
<h2>The Architecture That Emerged</h2>
<p>Here's what the current KeyPears stack looks like:</p>
<h3>Package Structure</h3>
<pre><code>@keypears/lib (TypeScript)
├── Blake3 hashing via @webbuf/blake3 (Rust→WASM)
├── ACB3 encryption via @webbuf/acb3 (Rust→WASM)
├── Three-tier key derivation (100k rounds each)
├── Password generation with entropy calculation
└── Zod schemas for validation

@keypears/api-server (TypeScript)
├── orpc router with type-safe procedures
├── Blake3 endpoint (working proof-of-concept)
├── Drizzle ORM + PostgreSQL schema (ready for server DB)
└── Client factory for end-to-end type safety

keypears-tauri (TypeScript + Rust shell)
├── Tauri 2.0 app (33 lines of Rust)
├── Full vault management UI (~5,020 lines TypeScript)
├── SQLite with Drizzle ORM
├── React Router 7 for navigation
├── Shadcn components + Catppuccin theme
└── Calls production API server for crypto endpoints

@keypears/webapp (TypeScript)
├── Production website + blog
├── Integrated API server (orpc mounted at /api)
├── Single Express server on port 4273
└── Deployed on AWS Fargate
</code></pre>
<h3>What Works Today</h3>
<p>The Tauri app has a complete vault management workflow:</p>
<p>✅ Create vault with password ✅ Unlock vault with password verification ✅
Store passwords with encryption ✅ Generate secure passwords ✅ SQLite
persistence via Drizzle ✅ Three-tier key derivation working ✅ Vault encryption
with ACB3 ✅ Multi-step wizards (name → password → confirm → success) ✅ Test
page calling production Blake3 API</p>
<p>The webapp has:</p>
<p>✅ Landing page with blog system ✅ Working <code>/api/blake3</code> endpoint ✅ orpc
integrated with Express ✅ Docker deployment to AWS Fargate ✅ Canonical URL
redirects ✅ Blog posts with TOML frontmatter + Markdown</p>
<h3>What's Not Built (Intentionally Deferred)</h3>
<p>We haven't built server-side features yet because the MVP is <strong>local-first</strong>:</p>
<ul>
<li>⏸️ User authentication (login/logout)</li>
<li>⏸️ Vault synchronization protocol</li>
<li>⏸️ Multi-user server support</li>
<li>⏸️ Diffie-Hellman key exchange across domains</li>
<li>⏸️ Public key infrastructure</li>
</ul>
<p>These are v2 features. The MVP is a password manager that works 100% offline in
the Tauri app. The server is only needed for multi-device sync, which we'll add
after validating the core product.</p>
<h2>The TypeScript Ecosystem Has Caught Up</h2>
<p>Five years ago, this blog post would have been different. Rust was the only way
to get type-safe backends with good performance. But the TypeScript ecosystem
has evolved dramatically:</p>
<p><strong>orpc</strong> gives us end-to-end type safety that Rust can't match (no codegen,
instant IDE feedback)</p>
<p><strong>Drizzle</strong> provides type-safe SQL for both SQLite and PostgreSQL (no Rust ORM
does this well)</p>
<p><strong>WASM</strong> lets us use Rust crypto without writing Rust applications (best of both
worlds)</p>
<p><strong>Vitest</strong> gives us fast ESM-native testing (simpler than Cargo's test framework
for web apps)</p>
<p><strong>React Router 7</strong> provides SSR + type-safe routing (no Rust equivalent)</p>
<p>For building web applications with cryptography, TypeScript + WASM is now a
better choice than native Rust. You get comparable performance, better tooling,
and a much larger ecosystem of web-focused libraries.</p>
<h2>When Would We Use Rust?</h2>
<p>This isn't a rejection of Rust. It's a recognition that <strong>Rust solves the wrong
problems for our MVP.</strong></p>
<p>Rust makes sense when you need:</p>
<ol>
<li><strong>Extreme performance</strong> - Handling 10k+ concurrent WebSocket connections</li>
<li><strong>Embedded systems</strong> - Running on IoT devices with 64MB of RAM</li>
<li><strong>Custom crypto</strong> - Implementing novel cryptographic algorithms</li>
<li><strong>Kernel-level code</strong> - Writing device drivers or OS components</li>
</ol>
<p>KeyPears doesn't need any of these yet. Our server will handle dozens of
concurrent users, not thousands. Our desktop app runs on modern laptops with
gigabytes of RAM. Our cryptography comes from well-tested libraries (Blake3,
AES-256). We're building a user-facing application, not infrastructure.</p>
<p><strong>Later, Rust might make sense for:</strong></p>
<ul>
<li>High-throughput sync server (if we grow to enterprise scale)</li>
<li>Mobile performance optimization (if WASM proves too slow)</li>
<li>Custom Diffie-Hellman implementation (if existing libraries don't fit)</li>
</ul>
<p>But even then, we'd keep the API layer in TypeScript (orpc is too good to give
up) and only move performance-critical sync logic to Rust via FFI.</p>
<h2>The Right Tool for the Right Job</h2>
<p>Software architecture isn't about using the "best" language—it's about using the
right tool for the constraints you're facing.</p>
<p>Our constraints:</p>
<ul>
<li><strong>Side project timeline</strong>: Limited evening/weekend hours</li>
<li><strong>Solo developer</strong>: No team to split Rust vs TypeScript work</li>
<li><strong>MVP goal</strong>: Prove the concept before scaling</li>
<li><strong>Rapid iteration</strong>: Features change based on user feedback</li>
</ul>
<p>For these constraints, TypeScript is objectively better:</p>
<ul>
<li>Faster iteration (100ms hot reload vs 5s compile)</li>
<li>Single mental model (no context switching)</li>
<li>Richer ecosystem (orpc, Drizzle, React Router)</li>
<li>Lower cognitive overhead (one type system, one package manager)</li>
</ul>
<p>We still get Rust's security properties through WASM. We still get type safety
through TypeScript. We still get performance (crypto is WASM, API is fast
enough).</p>
<h2>What We Learned</h2>
<p><strong>1. Working code isn't always the right code</strong></p>
<p>The Rust backend worked perfectly. Blake3 hashing worked. Key derivation worked.
The API server worked. We shipped it to production. But "working" doesn't mean
"optimal for the constraints." When we evaluated developer experience vs
performance gains, TypeScript won decisively for our MVP.</p>
<p><strong>2. Ecosystem maturity matters more than language performance</strong></p>
<p>The Rust language is excellent. But for web applications, the TypeScript
ecosystem is years ahead. orpc's zero-codegen type safety is revolutionary.
Drizzle's unified SQLite + Postgres support is essential for our architecture.
These don't exist in Rust.</p>
<p><strong>3. WASM changes the game</strong></p>
<p>Ten years ago, you had to choose: safe languages (Ruby, Python, JavaScript) or
fast languages (C, C++, Rust). Today, you can write your performance-critical
code in Rust, compile it to WASM, and use it from any language. This is what
made deleting our Rust backend viable—we didn't lose Rust's performance, we just
stopped writing it ourselves.</p>
<p><strong>4. Deleting working code is liberating</strong></p>
<p>We spent weeks building <code>rs-lib</code> and <code>rs-node</code>. They worked. They were deployed.
And we deleted them anyway because the TypeScript alternative was better for our
constraints. This felt wrong at first—"But we already built it!"—but the
productivity gain from consolidating on one language was immediate and
substantial.</p>
<p><strong>5. Side project constraints are different</strong></p>
<p>If KeyPears were a VC-funded startup with a team of 5 engineers, we'd keep the
Rust backend. Someone could own the Rust API while others work on the TypeScript
UI. But for a solo side project with limited evening/weekend hours, the mental
overhead of context switching between Rust and TypeScript was too high. One
language means more velocity.</p>
<h2>The Current Priority: Shipping the MVP</h2>
<p>With this architecture decision settled, we're focused on shipping a working
product:</p>
<p><strong>Next milestones:</strong></p>
<ol>
<li><strong>Server vault CRUD</strong> - Create/read/update vaults via API</li>
<li><strong>User authentication</strong> - Session-based login with hashed login key</li>
<li><strong>Basic sync protocol</strong> - Last-write-wins synchronization</li>
<li><strong>Mobile Tauri build</strong> - iOS + Android apps</li>
<li><strong>Import/export</strong> - Backup and restore vaults</li>
</ol>
<p>All of this will be TypeScript. The API server will use orpc. The database will
use Drizzle (Postgres on server, SQLite on clients). The cryptography will
remain Rust-compiled WASM.</p>
<p>And if we're wrong—if we hit performance walls or need Rust for specific
features—we can always add Rust modules later. The architecture supports it. But
we're not starting there.</p>
<h2>Try It Yourself</h2>
<p>The Blake3 endpoint is live:</p>
<pre><code class="language-bash">curl -X POST https://keypears.com/api/blake3 \
  -H "Content-Type: application/json" \
  -d '{"data": "SGVsbG8sIEtleVBlYXJzIQ=="}'
</code></pre>
<p>That <code>data</code> field is base64-encoded "Hello, KeyPears!". The API will return the
Blake3 hash computed by Rust (via WASM) running in Node.js on our TypeScript
server.</p>
<p>It's a small proof-of-concept, but it validates the entire architecture:
TypeScript for the API layer, Rust-via-WASM for cryptography, type safety
end-to-end.</p>
<h2>Conclusion</h2>
<p>We built a Rust backend. It worked. We deployed it. And then we deleted it.</p>
<p>This wasn't a failure of Rust or a mistake in architecture. It was a deliberate
choice to optimize for <strong>developer velocity over theoretical performance</strong> at
this stage of the project. The Rust backend would have been fine for production,
but the TypeScript backend is better for rapid MVP development.</p>
<p>We're building KeyPears with <strong>TypeScript + WASM</strong>, which gives us Rust's
security properties (via WASM crypto) without the complexity of maintaining a
dual-language codebase.</p>
<p>For a solo side project with MVP goals, this is the right architecture. If we
scale to millions of users and need extreme performance, we can always bring
Rust back for specific hot paths. But we're not starting there.</p>
<p>Rust is an incredible language. We proved that by building a working backend
with it. But for this project, at this stage, TypeScript is the pragmatic
choice—and we're comfortable deleting working Rust code to prove it.</p>
<p>We'll keep sharing our progress—both the wins and the pivots. If you're
interested in following along:</p>
<ul>
<li><strong>Live demo</strong>: Try the Blake3 endpoint at https://keypears.com/api/blake3</li>
<li><strong>Source code</strong>: Coming soon on GitHub under Apache 2.0 license</li>
</ul>
<p>More updates coming soon. Next post: Implementing the vault synchronization
protocol.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building KeyPears with Rust: Backend Architecture and Blake3 Proof-of-Concept]]></title>
            <link>https://keypears.com/blog/2025-10-25-rust-backend-architecture</link>
            <guid isPermaLink="false">https://keypears.com/blog/2025-10-25-rust-backend-architecture</guid>
            <pubDate>Sat, 25 Oct 2025 11:00:00 GMT</pubDate>
            <description><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and cryptocurrency wallet. The design decisions described here represent our development approach and may evolve b...]]></description>
            <content:encoded><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and cryptocurrency wallet. The design decisions described here represent our development approach and may evolve before our official release.</p>
<p>We're excited to share a major architectural milestone: KeyPears now has a working Rust backend with our first proof-of-concept endpoint. This marks a significant shift in our technical approach, bringing the performance, security, and cross-platform benefits of Rust to our core cryptography and API layer.</p>
<h2>Why Rust for the Backend?</h2>
<p>When we started building KeyPears, we knew cryptography and security would be central to everything we do. After evaluating different approaches, we chose to build our backend entirely in Rust for several compelling reasons:</p>
<h3>Performance</h3>
<p>Cryptographic operations—hashing, encryption, key derivation—are CPU-intensive. Rust's zero-cost abstractions and lack of garbage collection mean we can achieve performance comparable to C/C++ without sacrificing safety. For operations users will perform thousands of times (encrypting secrets, computing hashes, deriving keys), this performance matters.</p>
<h3>Memory Safety</h3>
<p>Password managers and cryptocurrency wallets are high-value targets for attackers. Rust's ownership system and borrow checker eliminate entire classes of vulnerabilities at compile time:</p>
<ul>
<li>No buffer overflows</li>
<li>No use-after-free bugs</li>
<li>No data races in concurrent code</li>
<li>No null pointer dereferences</li>
</ul>
<p>These guarantees mean our cryptographic code has fewer attack surfaces by design.</p>
<h3>Cross-Platform Consistency</h3>
<p>KeyPears needs to run everywhere: Windows, macOS, Linux, Android, and iOS. Rust compiles to native code on all these platforms with consistent behavior. The same cryptographic library (<code>rs-lib</code>) that powers our server also powers our Tauri desktop app and will eventually power our mobile apps.</p>
<p>This eliminates the "works on my machine" problem and ensures that a secret encrypted on iOS can be decrypted on Windows with identical cryptographic operations.</p>
<h3>Strong Type System</h3>
<p>Rust's type system helps us encode security invariants at compile time. For example, we can use the type system to ensure that:</p>
<ul>
<li>Encryption keys are never accidentally logged or serialized</li>
<li>Sensitive data is properly zeroed after use</li>
<li>API responses match their OpenAPI specifications exactly</li>
</ul>
<p>This compile-time verification catches bugs before they reach production.</p>
<h2>Architecture Overview</h2>
<p>Our Rust backend consists of two main packages:</p>
<h3><code>rs-lib</code>: Core Cryptography Library</h3>
<p><code>rs-lib</code> is a shared Rust library containing all our cryptographic implementations:</p>
<ul>
<li><strong>Blake3</strong>: Fast, secure hashing and key derivation</li>
<li><strong>ACB3</strong>: AES-256-CBC + Blake3-MAC for authenticated encryption</li>
<li><strong>Key derivation</strong>: Three-tier system separating authentication from encryption</li>
<li><strong>Data structures</strong>: Core types for vaults, secrets, and synchronization</li>
</ul>
<p>This library is pure Rust with no external dependencies beyond well-audited cryptography crates. It's designed to be portable and reusable across all our platforms.</p>
<h3><code>rs-node</code>: KeyPears Node (API Server)</h3>
<p><code>rs-node</code> is our API server—what we call a "KeyPears node." It uses the Axum web framework to expose REST endpoints that clients can use for cryptographic operations and vault synchronization.</p>
<p>Key features:</p>
<ul>
<li><strong>Axum framework</strong>: Modern, type-safe HTTP server from the Tokio team</li>
<li><strong>OpenAPI 3.0</strong>: Full API specification generated from Rust code using <code>utoipa</code></li>
<li><strong>Swagger UI</strong>: Interactive API documentation at <code>/api/docs</code></li>
<li><strong>Type safety</strong>: Request/response types validated at compile time</li>
</ul>
<p>The node is designed to be self-hostable. Anyone can run their own KeyPears node for full sovereignty over their data.</p>
<h2>Blake3 Proof-of-Concept</h2>
<p>Our first working endpoint is a Blake3 hashing service at <code>/api/blake3</code>. You can try it right now:</p>
<pre><code class="language-bash">curl -X POST https://keypears.com/api/blake3 \
  -H "Content-Type: application/json" \
  -d '{"data": "Hello, KeyPears!"}'
</code></pre>
<p>This returns:</p>
<pre><code class="language-json">{
  "hash": "a1b2c3d4..."
}
</code></pre>
<p>Blake3 is our hashing algorithm of choice for KeyPears. It's:</p>
<ul>
<li><strong>Fast</strong>: Significantly faster than SHA-256 or SHA-3</li>
<li><strong>Secure</strong>: 256-bit security with no known attacks</li>
<li><strong>Versatile</strong>: Works as both a hash function and a key derivation function</li>
<li><strong>Modern</strong>: Designed in 2020 with modern CPU features in mind</li>
</ul>
<p>We use Blake3 throughout KeyPears:</p>
<ul>
<li>Deriving encryption keys from passwords</li>
<li>Generating message authentication codes (MACs)</li>
<li>Computing content hashes for deduplication</li>
<li>Creating deterministic IDs</li>
</ul>
<p>This proof-of-concept demonstrates the full stack working:</p>
<ol>
<li>Rust backend (<code>rs-node</code>) receives the request</li>
<li>Rust library (<code>rs-lib</code>) performs the Blake3 hash</li>
<li>Result is serialized and returned via Axum</li>
<li>OpenAPI documentation describes the endpoint</li>
<li>Node.js webapp proxies <code>/api/*</code> requests to the Rust node</li>
</ol>
<h2>TypeScript Frontend + Rust Backend</h2>
<p>While our backend is Rust, our frontend remains TypeScript. This gives us the best of both worlds:</p>
<ul>
<li><strong>Rust</strong>: Performance and security for cryptography and core logic</li>
<li><strong>TypeScript</strong>: Rapid development and rich ecosystem for UI</li>
</ul>
<p>Our architecture uses:</p>
<ul>
<li><strong>Tauri</strong>: Native desktop apps with Rust backend + web frontend</li>
<li><strong>React Router</strong>: Type-safe routing for web and desktop apps</li>
<li><strong>shadcn</strong>: UI components with Catppuccin theme</li>
<li><strong>Type-safe API client</strong>: Generated from OpenAPI spec for compile-time safety</li>
</ul>
<p>The Tauri app embeds the same <code>rs-lib</code> cryptography that powers the KeyPears node. This means the desktop app has full offline capability—it doesn't need a server for cryptographic operations. The server is only needed for synchronization across devices.</p>
<h2>Deployment Architecture</h2>
<p>In production, we run a dual-server setup:</p>
<ol>
<li><strong>KeyPears node (Rust)</strong>: Runs on port 4274, handles API requests</li>
<li><strong>Webapp server (Node.js)</strong>: Runs on port 4273, serves the landing page and proxies API requests</li>
</ol>
<p>The Node.js server forwards all <code>/api/*</code> requests to the Rust node via <code>http-proxy-middleware</code>. This gives us:</p>
<ul>
<li>Single-domain simplicity (no CORS issues)</li>
<li>Independent scaling of API and web traffic</li>
<li>Clean separation of concerns</li>
</ul>
<p>Both services run in a single Docker container on AWS Fargate, deployed via ECS.</p>
<h2>Interactive API Documentation</h2>
<p>One of the benefits of Rust's <code>utoipa</code> library is automatic OpenAPI documentation generation. You can explore our API interactively at:</p>
<p><strong>https://keypears.com/api/docs</strong></p>
<p>This Swagger UI is generated directly from our Rust code. Every endpoint, request type, and response type is documented with examples. As we add new endpoints, the documentation updates automatically.</p>
<h2>What's Next?</h2>
<p>The Blake3 endpoint is just the beginning. We're actively building:</p>
<h3>Vault Operations</h3>
<ul>
<li>Create and encrypt vaults</li>
<li>Derive encryption keys from passwords</li>
<li>Store and retrieve encrypted secrets</li>
</ul>
<h3>Synchronization Protocol</h3>
<ul>
<li>Append-only logs for conflict-free sync</li>
<li>Server-side coordination for multi-device sync</li>
<li>End-to-end encryption (servers never see plaintext)</li>
</ul>
<h3>Diffie-Hellman Key Exchange</h3>
<ul>
<li>Peer-to-peer secret sharing across domains</li>
<li>Email-style addressing (<code>alice@example.com</code> ↔ <code>bob@example2.com</code>)</li>
<li>Public key discovery via federated nodes</li>
</ul>
<h3>Cross-Platform Clients</h3>
<ul>
<li>Desktop apps (Windows, macOS, Linux) via Tauri</li>
<li>Mobile apps (Android, iOS) - coming soon</li>
<li>Web interface for emergency access</li>
</ul>
<p>All of these features will be built on the same foundation: Rust for security-critical operations, TypeScript for user interfaces.</p>
<h2>Open Source and Self-Hostable</h2>
<p>Everything we're building is open source under Apache 2.0. You can:</p>
<ul>
<li>Review the code for security</li>
<li>Run your own KeyPears node</li>
<li>Contribute improvements</li>
<li>Build custom clients</li>
</ul>
<p>The KeyPears node is designed to be self-hostable. Our deployment documentation walks through:</p>
<ul>
<li>Cross-compiling for Linux (even from macOS)</li>
<li>Docker containerization</li>
<li>AWS Fargate deployment</li>
<li>Domain configuration and SSL</li>
</ul>
<p>We want KeyPears to be decentralized by default. Anyone should be able to run a node, just like anyone can run an email server.</p>
<h2>Conclusion</h2>
<p>Building KeyPears with Rust has been an excellent decision. The language's emphasis on safety, performance, and correctness aligns perfectly with our security requirements. The Blake3 proof-of-concept validates our architecture: Rust backend for cryptography, TypeScript frontend for user experience, and a clean API boundary between them.</p>
<p>We're excited to continue building. If you're interested in following along, check out:</p>
<ul>
<li><strong>Live demo</strong>: Try the Blake3 endpoint at https://keypears.com/api/blake3</li>
<li><strong>API docs</strong>: Explore the OpenAPI spec at https://keypears.com/api/docs</li>
<li><strong>Source code</strong>: Coming soon on GitHub</li>
</ul>
<p>We'll continue sharing our progress through these blog posts. Next up: vault encryption and key derivation in Rust.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Progress on Secret Synchronization: A Future-Proof Schema]]></title>
            <link>https://keypears.com/blog/2025-10-07-progress-on-secret-synchronization</link>
            <guid isPermaLink="false">https://keypears.com/blog/2025-10-07-progress-on-secret-synchronization</guid>
            <pubDate>Tue, 07 Oct 2025 11:00:00 GMT</pubDate>
            <description><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The
design decisions described here represent our development approach and may
evolve before our official release...]]></description>
            <content:encoded><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The
design decisions described here represent our development approach and may
evolve before our official release.</p>
<p>We've made significant progress on KeyPears' secret synchronization
architecture. Today we're sharing how we redesigned our schema to support
diverse secret types while maintaining the small-sync-unit principle that makes
our synchronization protocol efficient and reliable.</p>
<h2>The Problem</h2>
<p>Our original schema was built around passwords. It had fields like
<code>encryptedPassword</code>, <code>username</code>, <code>domain</code>, and <code>notes</code>. This worked fine for
basic password management, but it created limitations:</p>
<ul>
<li><strong>Type inflexibility</strong>: How do you store an API key? A cryptocurrency wallet
with multiple components? Environment variables?</li>
<li><strong>No grouping</strong>: Secrets existed in isolation. There was no way to represent
"this API token belongs to this account" or "these 20 environment variables
form one .env file"</li>
<li><strong>No hierarchy</strong>: No folders, no organization beyond a flat list</li>
<li><strong>KeePass import impossible</strong>: KeePass has groups (folders) and custom fields
(additional key-value pairs per entry). We couldn't represent either.</li>
</ul>
<p>We needed a more flexible schema without abandoning our core architectural
principle: <strong>every secret must sync independently</strong> to keep network overhead
small and conflict resolution simple.</p>
<h2>The Solution: Three Changes</h2>
<p>We evolved the <code>SecretUpdate</code> schema with three key additions: multi-type
support, dual hierarchy mechanisms, and JSON-based storage.</p>
<h3>1. Multi-Type Support</h3>
<p>First, we made the schema generic enough to handle any small secret:</p>
<pre><code class="language-typescript">type: "password" | "envvar" | "apikey" | "walletkey" | "passkey"
encryptedData: string  // Previously: encryptedPassword
encryptedNotes: string // Previously: notes
</code></pre>
<p>The <code>type</code> field distinguishes what kind of secret this is. The generic
<code>encryptedData</code> field holds the actual secret value (password, API key, private
key, etc.). Password-specific fields like <code>domain</code>, <code>username</code>, and <code>email</code>
remain in the schema but are optional—used primarily when <code>type</code> is <code>password</code>.</p>
<p>This small change opens up KeyPears to handle:</p>
<ul>
<li><strong>Environment variables</strong>: Type <code>envvar</code>, name <code>DATABASE_URL</code>, encrypted value
in <code>encryptedData</code></li>
<li><strong>API keys</strong>: Type <code>apikey</code>, service name in a <code>label</code> field, key in
<code>encryptedData</code></li>
<li><strong>Wallet keys</strong>: Type <code>walletkey</code>, blockchain type in metadata, private key in
<code>encryptedData</code></li>
<li><strong>Passkeys</strong>: Type <code>passkey</code>, credential ID and public key in metadata,
private key in <code>encryptedData</code></li>
</ul>
<h3>2. Dual Hierarchy: Folders and ParentId</h3>
<p>The second change introduces two different hierarchy mechanisms, each serving a
specific purpose:</p>
<pre><code class="language-typescript">folders: string[]      // ["Work", "Projects", "Client A"]
tags: string[]         // ["production", "critical"]
parentId: string       // ULID of parent secret (max depth 1)
</code></pre>
<p><strong>Folders</strong> provide unlimited-depth organizational hierarchy. They're just an
array of strings representing the path:</p>
<pre><code class="language-typescript">folders: ["Work", "AWS", "Production"]
folders: ["Personal", "Banking"]
folders: []  // Root level
</code></pre>
<p>This maps perfectly to KeePass Groups and lets users organize thousands of
secrets into a familiar folder structure.</p>
<p><strong>Tags</strong> provide orthogonal categorization. A secret can have multiple tags for
cross-cutting concerns:</p>
<pre><code class="language-typescript">tags: ["production-env", "requires-rotation", "shared-with-team"]
</code></pre>
<p><strong>ParentId</strong> creates actual parent-child relationships between secrets. This is
where it gets interesting.</p>
<h2>ParentId: Secrets Containing Secrets</h2>
<p>The <code>parentId</code> field lets one secret "contain" other secrets. A simple example:</p>
<pre><code class="language-typescript">// Parent: The main account
{
  secretId: "abc123",
  name: "GitHub Account",
  type: "password",
  encryptedData: "&#x3C;main password>"
}

// Child: API token for the same account
{
  secretId: "def456",
  name: "API Token",
  type: "apikey",
  parentId: "abc123",
  encryptedData: "&#x3C;token>"
}
</code></pre>
<p>This models KeePass's custom fields—additional key-value pairs that belong to an
entry. In KeePass, you might have a GitHub entry with standard fields (username,
password, URL) plus custom fields for API tokens, 2FA backup codes, or recovery
emails.</p>
<p>In KeyPears, each custom field becomes its own secret with a <code>parentId</code> pointing
to the parent. Each syncs independently (small sync units!), but they're
logically grouped.</p>
<h3>The Depth Limit: Security Through Simplicity</h3>
<p>Here's the critical constraint: <strong>a secret can have a parent, but that parent
cannot have a parent</strong>. Maximum depth is 1. No grandparents allowed.</p>
<p>Why? Three reasons:</p>
<p><strong>1. Security</strong>: Client-generated IDs open an attack vector for malicious
clients creating circular references or extremely deep chains. With depth=1, the
validation is trivial:</p>
<pre><code class="language-typescript">async function validateParentChain(secretId: string, parentId?: string) {
  if (!parentId) return; // No parent, valid
  if (parentId === secretId) throw new Error("Cannot self-reference");

  const parent = await getSecretHistory(parentId);
  if (parent.length > 0 &#x26;&#x26; parent[0].parentId) {
    throw new Error("Cannot nest more than one level deep");
  }
}
</code></pre>
<p>One database lookup. No recursion. No visited sets. O(1) validation that
attackers can't exploit.</p>
<p><strong>2. Performance</strong>: Validating unlimited depth requires recursive queries.
Validating depth=1 requires one query. Simple.</p>
<p><strong>3. Sufficient for real use cases</strong>:</p>
<ul>
<li>Folder with secrets ✓</li>
<li>Entry with custom fields ✓</li>
<li>Environment variable group ✓</li>
</ul>
<p>What we lose: deeply nested folder hierarchies via <code>parentId</code>. But we have
<code>folders</code> for that! The two mechanisms complement each other perfectly.</p>
<h2>Why Two Hierarchy Systems?</h2>
<p>It might seem redundant to have both <code>folders</code> and <code>parentId</code>, but they serve
different purposes:</p>
<p><strong>Folders</strong> are for <strong>organizational hierarchy</strong>. They map to KeePass Groups.
They're pure metadata—just strings representing a path. They have unlimited
depth because they're just labels, not database relationships.</p>
<p><strong>ParentId</strong> is for <strong>data relationships</strong>. It maps to KeePass custom fields. It
creates actual parent-child relationships where one secret logically contains
others. Each child syncs independently, maintaining small sync units.</p>
<p>Together, they enable full KeePass import:</p>
<pre><code class="language-typescript">// KeePass structure:
// Work/Projects/GitHub (Group path)
//   - GitHub Account (Entry)
//     - Username: alice
//     - Password: ••••••
//     - Custom: API Token (protected)
//     - Custom: 2FA Codes (protected)

// KeyPears representation:
{
  secretId: "main",
  name: "GitHub Account",
  type: "password",
  folders: ["Work", "Projects", "GitHub"],
  username: "alice",
  encryptedData: "&#x3C;password>"
}
{
  secretId: "token",
  name: "API Token",
  type: "apikey",
  folders: ["Work", "Projects", "GitHub"], // Inherits folder
  parentId: "main",
  encryptedData: "&#x3C;token>"
}
{
  secretId: "codes",
  name: "2FA Codes",
  type: "password",
  folders: ["Work", "Projects", "GitHub"],
  parentId: "main",
  encryptedData: "&#x3C;codes>"
}
</code></pre>
<p>The folder path provides organization. The <code>parentId</code> relationships show which
secrets belong together. Each secret syncs independently.</p>
<h2>JSON-Based Storage: Migration-Proof Architecture</h2>
<p>The third major change is how we store secrets in the database. We moved to a
hybrid approach:</p>
<pre><code class="language-sql">CREATE TABLE secret_update (
  id TEXT PRIMARY KEY,
  vault_id TEXT NOT NULL,
  secret_id TEXT NOT NULL,
  name TEXT NOT NULL,
  type TEXT NOT NULL DEFAULT 'password',
  parent_id TEXT,
  created_at INTEGER NOT NULL,
  deleted INTEGER NOT NULL DEFAULT 0,

  -- Source of truth: full JSON object
  secret_update_json TEXT NOT NULL
);

CREATE INDEX idx_secret_updates_name ON secret_update(name);
CREATE INDEX idx_secret_updates_type ON secret_update(type);
CREATE INDEX idx_secret_updates_parent_id ON secret_update(parent_id);
</code></pre>
<p>Notice what's happening here. We store the <strong>entire <code>SecretUpdate</code> object</strong> as
JSON in <code>secret_update_json</code>. The other columns (<code>name</code>, <code>type</code>, <code>parent_id</code>,
etc.) are duplicates of data from the JSON, extracted for indexing.</p>
<p>The JSON is the source of truth. The columns are for performance.</p>
<h3>Why This Approach?</h3>
<p><strong>Adding fields requires no migration</strong>. Want to add a <code>label</code> field? Update the
Zod schema, start writing it to the JSON, and you're done. The database doesn't
care—it's just storing JSON.</p>
<p>When we added <code>parentId</code> to the schema, we:</p>
<ol>
<li>Updated the Zod schema in TypeScript</li>
<li>Added <code>parent_id</code> column to the database (for indexing)</li>
<li>Started serializing <code>parentId</code> to the JSON</li>
</ol>
<p>Users with existing vaults see <code>parentId: undefined</code> in their JSON. No
migration, no data transformation. Just works.</p>
<p>This architecture is <strong>future-proof</strong>. We can evolve the schema rapidly during
development without worrying about breaking existing databases.</p>
<h3>When We Ship v1.0</h3>
<p>Before our first production release, we'll generate one clean migration from the
final schema. That becomes our baseline. After that, we'll only add new
migrations—never delete old ones—because users will have the old migrations
applied.</p>
<p>But during development? We delete and regenerate migrations freely. The JSON
storage strategy makes this painless.</p>
<h2>What This Enables</h2>
<p>With these changes in place, KeyPears can now handle:</p>
<h3>KeePass Import (Future Feature)</h3>
<p>Full KeePass <code>.kdbx</code> import support, including:</p>
<ul>
<li>Nested groups → <code>folders</code> array</li>
<li>Entries → secrets with <code>type: "password"</code></li>
<li>Custom protected fields → child secrets with <code>parentId</code></li>
<li>Entry metadata → password-specific fields</li>
</ul>
<p>The only thing we won't import: file attachments. By design. We're optimizing
for small secrets that sync efficiently.</p>
<h3>Environment Variables</h3>
<p>Create a parent secret "Production Environment" and attach child secrets for
each variable:</p>
<pre><code class="language-typescript">{ name: "Production Env", type: "folder" }  // Parent
{ name: "DATABASE_URL", type: "envvar", parentId: "..." }
{ name: "API_SECRET", type: "envvar", parentId: "..." }
{ name: "STRIPE_KEY", type: "envvar", parentId: "..." }
</code></pre>
<p>Or use tags instead:</p>
<pre><code class="language-typescript">{ name: "DATABASE_URL", type: "envvar", tags: ["prod-env"] }
{ name: "API_SECRET", type: "envvar", tags: ["prod-env"] }
</code></pre>
<p>Both approaches work. <code>parentId</code> creates explicit grouping. Tags create implicit
sets.</p>
<h3>Cryptocurrency Wallets</h3>
<p>Store wallet keys with relevant metadata:</p>
<pre><code class="language-typescript">{
  name: "Ethereum Main Wallet",
  type: "walletkey",
  encryptedData: "&#x3C;private key>",
  folders: ["Crypto", "Ethereum"],
  tags: ["high-value", "cold-storage"]
}
</code></pre>
<h3>API Keys with Secrets</h3>
<p>Store API key pairs as parent-child:</p>
<pre><code class="language-typescript">{ name: "Stripe", type: "apikey", encryptedData: "&#x3C;public key>" }
{ name: "Secret Key", type: "apikey", parentId: "...", encryptedData: "&#x3C;secret>" }
</code></pre>
<h2>Synchronization Properties</h2>
<p>These changes maintain our core synchronization principles:</p>
<p><strong>Small sync units</strong>: Each secret syncs independently. A 50-entry KeePass import
becomes 50 individual secrets, each a few hundred bytes. If two users edit
different entries, no conflicts.</p>
<p><strong>Atomic updates</strong>: Each <code>SecretUpdate</code> is immutable once created. Updates
create new records in an append-only log. The latest update wins
(last-write-wins conflict resolution).</p>
<p><strong>Efficient</strong>: Only changed secrets sync. If you update one child secret, you
sync one small object, not the entire parent-child group.</p>
<p><strong>Validated</strong>: The <code>parentId</code> depth limit prevents malicious clients from
creating expensive recursive structures.</p>
<h2>Looking Ahead</h2>
<p>This schema evolution lays the groundwork for several future features:</p>
<ul>
<li><strong>UI for child secrets</strong>: Show "API Token" nested under "GitHub Account" in
the secret list</li>
<li><strong>KeePass import</strong>: Full <code>.kdbx</code> file import with groups and custom fields</li>
<li><strong>Environment variable templates</strong>: Quick creation of common .env structures</li>
<li><strong>Bulk operations</strong>: Delete/restore an entire group by operating on all
children</li>
</ul>
<p>The foundation is solid. The architecture is flexible. The sync protocol remains
simple and efficient.</p>
<p>We're building KeyPears to be more than a password manager—it's a secure,
self-custodied secret manager that handles everything from passwords to
environment variables to cryptocurrency keys. And it all syncs seamlessly across
your devices without trusting a central authority with your encryption keys.</p>
<h2>Technical Details</h2>
<p>For those interested in the implementation:</p>
<ul>
<li><strong>Schema definition</strong>: Zod validation in TypeScript, ensuring type safety</li>
<li><strong>Database</strong>: SQLite via Drizzle ORM with the sqlite-proxy adapter</li>
<li><strong>Migration</strong>: Custom migration runner that tracks applied migrations</li>
<li><strong>Validation</strong>: O(1) parent chain validation with single database lookup</li>
<li><strong>Indexes</strong>: <code>name</code>, <code>type</code>, <code>parent_id</code>, and composite
<code>vault_id + secret_id + created_at</code></li>
</ul>
<p>The code is Apache 2.0 licensed and available on GitHub. We're building in the
open, one commit at a time.</p>
<p>More updates coming soon.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How KeyPears Protects Your Vault: Encryption and Key Derivation]]></title>
            <link>https://keypears.com/blog/2025-10-05-vault-encryption-key-derivation</link>
            <guid isPermaLink="false">https://keypears.com/blog/2025-10-05-vault-encryption-key-derivation</guid>
            <pubDate>Sun, 05 Oct 2025 11:00:00 GMT</pubDate>
            <description><![CDATA[<p>One of the core security features of KeyPears is how we protect your vault
encryption keys. Today, we're diving deep into our three-tier key derivation
system and explaining how we ensure that even...]]></description>
            <content:encoded><![CDATA[<p>One of the core security features of KeyPears is how we protect your vault
encryption keys. Today, we're diving deep into our three-tier key derivation
system and explaining how we ensure that even if a server is compromised, your
encrypted data remains secure.</p>
<h2>The Problem: Authentication vs. Encryption</h2>
<p>Most password managers face a fundamental challenge: you need to send
<em>something</em> to the server to prove who you are, but you also need to keep your
encryption key secret so the server can't decrypt your vault. Using the same key
for both purposes creates a security vulnerability—if the server is compromised,
an attacker gains access to your encryption key.</p>
<p>KeyPears solves this by deriving two separate keys from your password: one for
logging in to the server, and one for encrypting your vault. The server only
ever sees the login key, never the encryption key.</p>
<h2>Three-Tier Key Derivation</h2>
<p>When you create a vault in KeyPears, we don't just hash your password once.
Instead, we use a three-tier key derivation system:</p>
<pre><code>Master Password
  ↓ blake3Pbkdf (100,000 rounds)
Password Key (stored encrypted with PIN on device)
  ↓
  ├→ blake3Pbkdf (100,000 rounds) → Encryption Key
  └→ blake3Pbkdf (100,000 rounds) → Login Key
</code></pre>
<h3>1. Password Key: The Root of Trust</h3>
<p>The first step derives a <strong>password key</strong> from your master password using
100,000 rounds of our Blake3-based PBKDF. This intermediate key is stored on
your device, encrypted with your PIN for quick unlock. It never leaves your
device and is never sent to any server.</p>
<p>The password key acts as the root of trust for deriving the other two keys.</p>
<h3>2. Encryption Key: Protecting Your Vault</h3>
<p>From the password key, we derive an <strong>encryption key</strong> through another 100,000
rounds of Blake3 PBKDF. This key is used for one purpose only: encrypting and
decrypting your master vault key.</p>
<p>Wait—encrypting a key with another key? Yes! Your vault itself is encrypted with
a randomly generated <strong>master vault key</strong>. This master key is immutable and
never changes. The encryption key derived from your password is used to encrypt
this master vault key before storing it in the database.</p>
<p>This architecture allows you to change your password without re-encrypting your
entire vault—we just re-encrypt the master vault key with the new encryption
key.</p>
<p>The encryption key is ephemeral. We derive it when needed, use it immediately,
and discard it. It is never persisted to disk and never sent anywhere.</p>
<h3>3. Login Key: Server Authentication</h3>
<p>The third key in our hierarchy is the <strong>login key</strong>, also derived from the
password key through 100,000 rounds of Blake3 PBKDF. This is the only key that
gets sent to the server for authentication.</p>
<p>Because the login key is derived separately from the encryption key,
compromising one doesn't compromise the other. Even if a server is breached and
the login key is stolen, the attacker cannot derive the encryption key needed to
decrypt your vault.</p>
<h2>Blake3 PBKDF: Fast and Secure</h2>
<p>You might notice we're using 100,000 rounds of Blake3 PBKDF rather than a
standard algorithm like PBKDF2. Blake3 is a modern, extremely fast cryptographic
hash function. Even at 100,000 rounds, the entire key derivation completes in
milliseconds on modern hardware.</p>
<p>Our Blake3-based PBKDF works by iteratively applying Blake3's keyed MAC mode:</p>
<pre><code>Round 1: result = blake3Mac(salt, password)
Round 2: result = blake3Mac(salt, result_from_round_1)
...
Round 100,000: result = blake3Mac(salt, result_from_round_99,999)
</code></pre>
<p>Each round adds computational cost for attackers trying to brute-force your
password, while remaining fast enough for legitimate use.</p>
<h2>Salt Derivation</h2>
<p>Each key derivation uses a different salt to ensure cryptographic separation:</p>
<ul>
<li>
<p><strong>Password Salt</strong>: Derived deterministically from your password using
<code>blake3Mac(blake3Hash("KeyPears password salt v1"), password)</code>. This ensures
the same password always produces the same password key.</p>
</li>
<li>
<p><strong>Encryption Salt</strong>: A global constant
<code>blake3Hash("KeyPears encryption salt v1")</code> used for all users. This is safe
because the encryption key is derived from the password key, not directly from
the password.</p>
</li>
<li>
<p><strong>Login Salt</strong>: Another global constant
<code>blake3Hash("KeyPears login salt v1")</code>. Again, safe because it's derived from
the password key.</p>
</li>
</ul>
<h2>Security Properties</h2>
<p>This architecture provides several important security guarantees:</p>
<h3>Defense Against Server Compromise</h3>
<p>If a KeyPears server is compromised, the attacker gains access to:</p>
<ul>
<li>Encrypted vault data</li>
<li>Login keys for authentication</li>
</ul>
<p>The attacker does NOT gain access to:</p>
<ul>
<li>Master passwords</li>
<li>Password keys</li>
<li>Encryption keys</li>
<li>Master vault keys</li>
<li>Decrypted vault contents</li>
</ul>
<p>Without the encryption key, the encrypted vault data is useless to the attacker.</p>
<h3>Defense Against Encrypted Data Theft</h3>
<p>If someone steals your encrypted vault data but doesn't have your credentials:</p>
<ul>
<li>They cannot decrypt it without the encryption key</li>
<li>The encryption key requires the password key</li>
<li>The password key requires your master password</li>
<li>100,000 rounds of Blake3 PBKDF make brute-forcing expensive</li>
</ul>
<h3>Key Separation</h3>
<p>The three keys are cryptographically isolated. Knowing the login key doesn't
help you derive the encryption key, and vice versa. Both require the password
key, which requires the master password.</p>
<h2>The Vault Key Hash: Verification</h2>
<p>When you enter your password to unlock a vault, KeyPears needs to verify you
entered it correctly. We do this by storing a Blake3 hash of the master vault
key in the database.</p>
<p>When you unlock:</p>
<ol>
<li>Derive password key from your password</li>
<li>Derive encryption key from password key</li>
<li>Decrypt the master vault key using the encryption key</li>
<li>Hash the decrypted master vault key</li>
<li>Compare with the stored hash</li>
</ol>
<p>If the hashes match, you entered the correct password. If not, the password is
wrong. This verification happens entirely on your device—the master vault key
never leaves your device, even temporarily.</p>
<h2>Putting It All Together</h2>
<p>Here's what happens when you create a new vault:</p>
<ol>
<li>You enter a master password</li>
<li>KeyPears derives a password key (100k rounds Blake3)</li>
<li>Derives an encryption key from the password key (100k rounds Blake3)</li>
<li>Generates a random master vault key</li>
<li>Encrypts the master vault key with the encryption key</li>
<li>Hashes the master vault key for verification</li>
<li>Stores the encrypted vault key and hash in your local database</li>
</ol>
<p>When you sync to a server:</p>
<ol>
<li>Derive the login key from your password key (100k rounds Blake3)</li>
<li>Send the login key to the server for authentication</li>
<li>Server returns your encrypted master vault key (and other encrypted vault
data)</li>
<li>Derive the encryption key (never sent to server)</li>
<li>Decrypt the master vault key locally</li>
<li>Use the master vault key to decrypt your secrets</li>
</ol>
<p>The server facilitates synchronization but never has the keys needed to decrypt
your data.</p>
<h2>Looking Ahead</h2>
<p>This architecture lays the foundation for secure sharing between users. In
future posts, we'll explore how KeyPears uses Diffie-Hellman key exchange to
share secrets securely between users, and how the master vault key enables
efficient re-encryption without re-deriving keys.</p>
<p>For now, the key takeaway is simple: KeyPears separates authentication from
encryption. Your server can verify who you are without ever having the ability
to decrypt your data. It's cryptography working exactly as it should.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Drizzle SQLite Database Migrations in Tauri 2.0]]></title>
            <link>https://keypears.com/blog/2025-10-04-drizzle-sqlite-tauri</link>
            <guid isPermaLink="false">https://keypears.com/blog/2025-10-04-drizzle-sqlite-tauri</guid>
            <pubDate>Sat, 04 Oct 2025 11:00:00 GMT</pubDate>
            <description><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The
solutions described here are part of our development process and may evolve
before our official release.</p>
...]]></description>
            <content:encoded><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The
solutions described here are part of our development process and may evolve
before our official release.</p>
<h2>The Problem</h2>
<p>Building a local-first application with Tauri 2.0, we needed a robust database
solution for storing encrypted vault data on users' devices. We wanted:</p>
<ul>
<li>Type-safe database queries</li>
<li>Proper schema migrations that work in production</li>
<li>Pure TypeScript implementation (no Rust for basic DB operations)</li>
<li>A solution that works across desktop and mobile platforms</li>
</ul>
<p>After evaluating options, we chose <strong>Drizzle ORM</strong> with <strong>SQLite</strong> via the
official <strong>tauri-plugin-sql</strong>. This combination gives us TypeScript-first
development with the reliability of SQLite.</p>
<h2>The Challenge</h2>
<p>Unlike traditional Node.js environments where you have direct filesystem access
and can use drivers like <code>better-sqlite3</code>, Tauri's sandboxed environment
requires a different approach. Drizzle's standard migration tools assume direct
database access, but with Tauri, we need to go through the plugin system.</p>
<p>Here's how we solved it.</p>
<h2>Tech Stack</h2>
<ul>
<li><strong>Tauri 2.0</strong> - Cross-platform app framework</li>
<li><strong>Drizzle ORM</strong> - TypeScript ORM</li>
<li><strong>drizzle-kit</strong> - Schema migration generator</li>
<li><strong>@tauri-apps/plugin-sql</strong> - Official Tauri SQLite plugin</li>
<li><strong>React Router</strong> - For app routing and loaders</li>
</ul>
<h2>Step 1: Install Dependencies</h2>
<p>First, add the necessary packages:</p>
<pre><code class="language-bash"># Production dependencies
pnpm add drizzle-orm @tauri-apps/plugin-sql

# Development dependencies
pnpm add -D drizzle-kit
</code></pre>
<p>Then add the Tauri plugin to your Rust dependencies in <code>src-tauri/Cargo.toml</code>:</p>
<pre><code class="language-toml">[dependencies]
tauri-plugin-sql = { version = "2", features = ["sqlite"] }
</code></pre>
<h2>Step 2: Configure Tauri Permissions</h2>
<p>Tauri 2.0 requires explicit permission grants. Add SQL permissions to
<code>src-tauri/capabilities/default.json</code>:</p>
<pre><code class="language-json">{
  "$schema": "../gen/schemas/desktop-schema.json",
  "identifier": "default",
  "description": "Capability for the main window",
  "windows": ["main"],
  "permissions": [
    "core:default",
    "sql:default",
    "sql:allow-load",
    "sql:allow-execute",
    "sql:allow-select",
    "sql:allow-close"
  ]
}
</code></pre>
<p>Without these permissions, you'll get "not allowed" errors when trying to access
the database.</p>
<h2>Step 3: Define Your Schema</h2>
<p>Create your Drizzle schema at <code>app/db/schema.ts</code>:</p>
<pre><code class="language-typescript">import { sqliteTable, text, integer } from "drizzle-orm/sqlite-core";

export const vaults = sqliteTable("vaults", {
  id: integer("id").primaryKey({ autoIncrement: true }),
  name: text("name").notNull().unique(),
});
</code></pre>
<h2>Step 4: Set Up the SQLite Proxy</h2>
<p>Since we can't use standard SQLite drivers in Tauri, we use Drizzle's
<code>sqlite-proxy</code> adapter. Create <code>app/db/index.ts</code>:</p>
<pre><code class="language-typescript">import { drizzle } from "drizzle-orm/sqlite-proxy";
import Database from "@tauri-apps/plugin-sql";
import * as schema from "./schema";

export async function getDb() {
  return await Database.load("sqlite:keypears.db");
}

function isSelectQuery(sql: string): boolean {
  return sql.trim().toLowerCase().startsWith("select");
}

export const db = drizzle&#x3C;typeof schema>(
  async (sql, params, method) => {
    const sqlite = await getDb();
    let rows: any = [];

    if (isSelectQuery(sql)) {
      rows = await sqlite.select(sql, params).catch((e) => {
        console.error("SQL Error:", e);
        return [];
      });
    } else {
      rows = await sqlite.execute(sql, params).catch((e) => {
        console.error("SQL Error:", e);
        return [];
      });
      return { rows: [] };
    }

    rows = rows.map((row: any) => Object.values(row));
    const results = method === "all" ? rows : rows[0];
    await sqlite.close();
    return { rows: results };
  },
  { schema: schema, logger: true }
);
</code></pre>
<p>The proxy adapter translates Drizzle queries into calls to the Tauri SQL plugin.</p>
<h2>Step 5: Configure Migration Generation</h2>
<p>Create <code>drizzle.config.ts</code>:</p>
<pre><code class="language-typescript">import type { Config } from "drizzle-kit";

export default {
  schema: "./app/db/schema.ts",
  out: "./app/db/migrations",
  dialect: "sqlite",
} satisfies Config;
</code></pre>
<p>Add a script to <code>package.json</code>:</p>
<pre><code class="language-json">{
  "scripts": {
    "db:migrate": "drizzle-kit generate"
  }
}
</code></pre>
<h2>Step 6: Implement Migration Runner</h2>
<p>Here's the key part - implementing our own migration system. Create
<code>app/db/migrate.ts</code>:</p>
<pre><code class="language-typescript">import { getDb } from "./index";

// Dynamically import all SQL migration files
const migrationFiles = import.meta.glob&#x3C;string>("./migrations/*.sql", {
  query: "?raw",
  import: "default",
  eager: true,
});

// Create migrations tracking table
async function ensureMigrationsTable() {
  const sqlite = await getDb();
  await sqlite.execute(`
    CREATE TABLE IF NOT EXISTS __drizzle_migrations (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      hash TEXT NOT NULL UNIQUE,
      created_at INTEGER NOT NULL
    )
  `);
  await sqlite.close();
}

// Get list of applied migrations
async function getAppliedMigrations(): Promise&#x3C;string[]> {
  const sqlite = await getDb();
  const rows = await sqlite
    .select&#x3C;Array&#x3C;{ hash: string }>>(
      "SELECT hash FROM __drizzle_migrations ORDER BY id"
    )
    .catch(() => []);
  await sqlite.close();
  return rows.map((row) => row.hash);
}

// Record migration as applied
async function recordMigration(hash: string) {
  const sqlite = await getDb();
  const timestamp = Date.now();
  await sqlite.execute(
    "INSERT INTO __drizzle_migrations (hash, created_at) VALUES (?, ?)",
    [hash, timestamp]
  );
  await sqlite.close();
}

// Execute SQL file
async function executeSqlFile(sqlContent: string) {
  const sqlite = await getDb();
  const statements = sqlContent
    .split("--> statement-breakpoint")
    .map((s) => s.trim())
    .filter((s) => s.length > 0);

  for (const statement of statements) {
    await sqlite.execute(statement).catch((e) => {
      console.error("Migration error:", e);
      throw e;
    });
  }

  await sqlite.close();
}

export async function runMigrations() {
  console.log("Running database migrations...");

  try {
    await ensureMigrationsTable();
    const appliedMigrations = await getAppliedMigrations();

    const migrationPaths = Object.keys(migrationFiles).sort();

    const pendingMigrations = migrationPaths.filter((path) => {
      const filename = path.split("/").pop() || path;
      return !appliedMigrations.includes(filename);
    });

    if (pendingMigrations.length === 0) {
      console.log("All migrations already applied");
      return;
    }

    for (const path of pendingMigrations) {
      const filename = path.split("/").pop() || path;
      const migrationContent = migrationFiles[path];

      console.log(`Executing migration: ${filename}`);
      await executeSqlFile(migrationContent);
      await recordMigration(filename);
      console.log(`✓ Applied: ${filename}`);
    }

    console.log(`Successfully completed ${pendingMigrations.length} migration(s)`);
  } catch (error) {
    console.error("Migration failed:", error);
    throw error;
  }
}
</code></pre>
<p>This implements Drizzle's migration tracking pattern:</p>
<ul>
<li>Uses <code>__drizzle_migrations</code> table to track applied migrations</li>
<li>Only runs new migrations on subsequent app launches</li>
<li>Supports incremental migrations as your schema evolves</li>
</ul>
<h2>Step 7: Run Migrations on App Startup</h2>
<p>In your root component (<code>app/root.tsx</code>), use a clientLoader to run migrations
before rendering:</p>
<pre><code class="language-typescript">import { runMigrations } from "./db/migrate";

export async function clientLoader() {
  await runMigrations();
  return null;
}

export function HydrateFallback() {
  return (
    &#x3C;div className="flex min-h-screen items-center justify-center">
      &#x3C;h1>Migrating the database...&#x3C;/h1>
    &#x3C;/div>
  );
}
</code></pre>
<p>React Router will show the fallback while migrations run, ensuring the database
is ready before any component renders.</p>
<h2>Step 8: Create Model Functions</h2>
<p>With everything set up, create type-safe model functions at
<code>app/db/models/vault.ts</code>:</p>
<pre><code class="language-typescript">import { db } from "../index";
import { vaults } from "../schema";
import { eq, count } from "drizzle-orm";

export interface Vault {
  id: number;
  name: string;
}

export async function createVault(name: string): Promise&#x3C;Vault> {
  const result = await db.insert(vaults).values({ name }).returning();
  return result[0];
}

export async function getVault(id: number): Promise&#x3C;Vault | undefined> {
  const result = await db.select().from(vaults).where(eq(vaults.id, id));
  return result[0];
}

export async function getVaults(): Promise&#x3C;Vault[]> {
  return await db.select().from(vaults);
}

export async function countVaults(): Promise&#x3C;number> {
  const result = await db.select({ count: count() }).from(vaults);
  return result[0]?.count ?? 0;
}
</code></pre>
<h2>Usage Workflow</h2>
<h3>Development</h3>
<p>When you modify your schema:</p>
<pre><code class="language-bash"># 1. Update app/db/schema.ts
# 2. Generate new migration
pnpm run db:migrate

# 3. Restart app - migration runs automatically
</code></pre>
<p>During development, you can safely delete all migrations and regenerate them
from scratch. Just delete the database file and migration files, then
regenerate.</p>
<h3>Production</h3>
<p>Before releasing v1.0:</p>
<ol>
<li>Delete all development migrations</li>
<li>Generate one clean migration from your final schema</li>
<li>Commit this as your baseline</li>
</ol>
<p>After release, <strong>never delete migrations</strong> - only add new ones. Users will have
the old migrations applied, and new migrations build incrementally.</p>
<h2>Database File Location</h2>
<p>The Tauri SQL plugin creates the database in the app's data directory:</p>
<ul>
<li><strong>macOS</strong>: <code>~/Library/Application Support/{app-identifier}/keypears.db</code></li>
<li><strong>Linux</strong>: <code>~/.local/share/{app-identifier}/keypears.db</code></li>
<li><strong>Windows</strong>: <code>%APPDATA%\{app-identifier}\keypears.db</code></li>
</ul>
<h2>Troubleshooting</h2>
<p><strong>Permission errors</strong>: Make sure you've added all SQL permissions to
<code>capabilities/default.json</code></p>
<p><strong>Migration fails</strong>: Check browser console in the Tauri webview for detailed
error messages</p>
<p><strong>Type errors</strong>: Run <code>pnpm run typecheck</code> to catch issues before runtime</p>
<h2>Conclusion</h2>
<p>This setup gives us:</p>
<ul>
<li>✅ Type-safe database queries with Drizzle</li>
<li>✅ Proper migration tracking that works in production</li>
<li>✅ Pure TypeScript - no Rust code needed for basic operations</li>
<li>✅ Cross-platform compatibility (desktop &#x26; mobile)</li>
<li>✅ Incremental migrations as the schema evolves</li>
</ul>
<p>The combination of Drizzle's <code>sqlite-proxy</code> adapter with Tauri's SQL plugin
provides a robust foundation for local-first data storage. While we had to
implement our own migration runner, we followed Drizzle's patterns to ensure
compatibility and maintainability.</p>
<h2>Resources</h2>
<ul>
<li><a href="https://orm.drizzle.team/">Drizzle ORM</a></li>
<li><a href="https://v2.tauri.app/plugin/sql/">Tauri SQL Plugin</a></li>
<li><a href="https://v2.tauri.app/security/capabilities/">Tauri Capabilities</a></li>
</ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introducing KeyPears: A New Approach to Password Management]]></title>
            <link>https://keypears.com/blog/2025-10-03-introducing-keypears</link>
            <guid isPermaLink="false">https://keypears.com/blog/2025-10-03-introducing-keypears</guid>
            <pubDate>Fri, 03 Oct 2025 11:00:00 GMT</pubDate>
            <description><![CDATA[<p>We're excited to announce KeyPears, a new password manager designed for the
modern era of digital security and self-custody.</p>
<h2>Why KeyPears?</h2>
<p>Traditional password managers have served ...]]></description>
            <content:encoded><![CDATA[<p>We're excited to announce KeyPears, a new password manager designed for the
modern era of digital security and self-custody.</p>
<h2>Why KeyPears?</h2>
<p>Traditional password managers have served us well, but they come with
limitations. Most rely on centralized services, creating single points of
failure and raising questions about who truly controls your most sensitive data.
KeyPears takes a different approach.</p>
<h2>Local-First, Sync-Enabled</h2>
<p>KeyPears is built on a local-first architecture. Your secrets live on your
devices, encrypted with keys only you control. But unlike purely local
solutions, KeyPears solves the synchronization problem through a permissionless
marketplace of third-party service providers using an open protocol—similar to
how email works.</p>
<p>Anyone can run a KeyPears node. The protocol is open source. You maintain full
self-custody while enjoying seamless synchronization across all your devices.</p>
<h2>Built for Sharing</h2>
<p>Modern work requires secure secret sharing. KeyPears uses end-to-end encryption
with public/private key pairs for each user. When alice@example.com needs to
share a secret with bob@example2.com, they use Diffie-Hellman key exchange to
derive a shared secret that only they know. The architecture mirrors email, but
with cryptography-first design.</p>
<h2>More Than Passwords</h2>
<p>While we call it a password manager, KeyPears is designed to handle:</p>
<ul>
<li>Passwords</li>
<li>Cryptocurrency wallet keys</li>
<li>API keys</li>
<li>Environment variables</li>
<li>SSH keys</li>
<li>PGP keys</li>
</ul>
<p>For cryptocurrency users seeking self-custody and businesses that need secure
secret sharing without expensive enterprise subscriptions, KeyPears offers a
compelling alternative.</p>
<h2>What's Next</h2>
<p>KeyPears is in active development. We're building native applications for
Windows, macOS, Linux, Android, and iOS using Tauri. The project is Apache 2.0
licensed and open source.</p>
<p>This is just the beginning. We're excited to build KeyPears with the community
and create a new standard for secure, self-custodied secret management.</p>
<p>Stay tuned for more updates as we continue development.</p>]]></content:encoded>
        </item>
    </channel>
</rss>