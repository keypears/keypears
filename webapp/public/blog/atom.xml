<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://keypears.com</id>
    <title>KeyPears Blog</title>
    <updated>2025-12-20T22:39:12.868Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://keypears.com"/>
    <link rel="self" href="https://keypears.com/blog/atom.xml"/>
    <subtitle>Updates and insights from the KeyPears team</subtitle>
    <icon>https://keypears.com/favicon.ico</icon>
    <rights>Copyright 2025 Identellica LLC</rights>
    <entry>
        <title type="html"><![CDATA[A Proof-of-Concept of a Diffie-Hellman-Based Messaging System for KeyPears]]></title>
        <id>https://keypears.com/blog/2025-12-20-dh-messaging-poc</id>
        <link href="https://keypears.com/blog/2025-12-20-dh-messaging-poc"/>
        <updated>2025-12-20T18:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>KeyPears is a password manager, but its true foundation is something more
fundamental: a federated Diffie-Hellman key exchange protocol. Today, we're
announcing the completion of a proof-of-concept...]]></summary>
        <content type="html"><![CDATA[<p>KeyPears is a password manager, but its true foundation is something more
fundamental: a federated Diffie-Hellman key exchange protocol. Today, we're
announcing the completion of a proof-of-concept messaging system built on this
foundation—the first step toward secure password sharing between users.</p>
<h2>Why Build Messaging First?</h2>
<p>To share a password from <code>alice@keypears.com</code> to <code>bob@example.com</code>, we need to:</p>
<ol>
<li>Establish a shared secret between Alice and Bob (DH key exchange)</li>
<li>Encrypt the password with that shared secret</li>
<li>Deliver it securely to Bob's inbox</li>
<li>Sync it to Bob's vault across all his devices</li>
</ol>
<p>That's essentially messaging with an attachment. So we started with plain text
messages. Once messaging works, adding password attachments is straightforward.</p>
<h2>The Privacy Challenge</h2>
<p>Every KeyPears vault has a master key (a secp256k1 private key). The naive
approach would be to publish each vault's public key and let users compute
shared secrets via ECDH. But this is problematic:</p>
<ul>
<li><strong>Correlation</strong>: Your public key becomes a global identifier linking all your
relationships</li>
<li><strong>Quantum risk</strong>: A quantum computer could derive your private key from the
public key, compromising everything at once</li>
<li><strong>Metadata leakage</strong>: Observers can see who you're communicating with based on
which public keys you fetch</li>
</ul>
<p>Our solution: generate unique "engagement keys" for each relationship. Alice and
Bob each get a relationship-specific keypair that reveals nothing about their
underlying vault keys.</p>
<h2>Server-Side Key Derivation: The Key Innovation</h2>
<p>Here's the problem: Bob might be offline when Alice wants to message him. She
needs his public key, but Bob isn't there to generate it.</p>
<p>Our solution uses a mathematical property of elliptic curve cryptography: if you
add two private keys together, the result corresponds to adding the two public
keys together.</p>
<pre><code>(privateKey_A + privateKey_B) * G = publicKey_A + publicKey_B
</code></pre>
<p>This enables server-assisted key generation:</p>
<ol>
<li>Server generates random <code>derivationPrivKey</code> and computes <code>derivationPubKey</code></li>
<li>Server adds Bob's vault public key:
<code>engagementPubKey = vaultPubKey + derivationPubKey</code></li>
<li>Server gives <code>derivationPrivKey</code> to Bob when he comes online</li>
<li>Bob computes: <code>engagementPrivKey = vaultPrivKey + derivationPrivKey</code></li>
</ol>
<p>The server never learns Bob's vault private key, yet can generate valid
engagement public keys on his behalf. Bob can derive the corresponding private
keys whenever he needs them.</p>
<h2>Spam Prevention with Proof-of-Work</h2>
<p>Without spam controls, anyone could flood inboxes with garbage. We use the same
proof-of-work system as vault registration: every message requires solving a
computational puzzle.</p>
<p>The difficulty is configurable at three levels:</p>
<ol>
<li><strong>Per-channel</strong>: Set different difficulty for specific contacts (low for
trusted friends)</li>
<li><strong>Per-vault</strong>: Your global setting for unknown senders</li>
<li><strong>System default</strong>: ~4 million hashes if nothing else is set</li>
</ol>
<p>When you trust someone, you can lower their difficulty to make replies instant.
Spammers face the full difficulty—minutes of computation per message.</p>
<h2>Architecture: Per-Participant Channel Views</h2>
<p>In a federated system, there's no central database. Alice's server
(<code>keypears.com</code>) and Bob's server (<code>example.com</code>) are completely separate. So
instead of a shared "channel" record, each participant has their own view:</p>
<pre><code>Alice's server stores:
┌─────────────────────────────────────┐
│ channel_view                        │
│ owner: alice@keypears.com           │
│ counterparty: bob@example.com      │
│ secretId: "01JFX..." (for vault)    │
└─────────────────────────────────────┘

Bob's server stores:
┌─────────────────────────────────────┐
│ channel_view                        │
│ owner: bob@example.com             │
│ counterparty: alice@keypears.com    │
│ secretId: "01JFA..." (for vault)    │
└─────────────────────────────────────┘
</code></pre>
<p>Each user manages their own copy. The server-generated <code>secretId</code> ensures all of
Alice's devices see the same channel ID when syncing.</p>
<h2>No Server-Side Outbox</h2>
<p>When Alice sends a message to Bob:</p>
<ol>
<li>Alice's client connects directly to Bob's server</li>
<li>Message is stored in Bob's inbox</li>
<li>Alice saves her sent message to her own vault</li>
</ol>
<p>Bob's server only sees incoming messages. Alice's server doesn't know who Alice
is messaging—she saves sent messages locally. This is more private than email,
where your outgoing mail server sees everything.</p>
<h2>The Complete Flow</h2>
<p>Here's what happens when Alice sends a message to Bob:</p>
<pre><code>1. Alice gets her engagement key (from her server, purpose: "send")
2. Alice requests Bob's engagement key (from Bob's server, purpose: "receive")
   → Bob's server returns the key + required PoW difficulty
3. Alice solves PoW challenge (WebGPU or WASM fallback)
4. Alice encrypts message with ECDH shared secret
5. Alice sends to Bob's server with PoW proof
6. Bob's server validates PoW and stores in inbox
7. Bob's sync service moves message from inbox to vault
8. Bob reads message (decrypted client-side)
</code></pre>
<h2>Implementation Details</h2>
<p>The messaging system spans both the API server and the Tauri client:</p>
<p><strong>API Server (TypeScript/orpc)</strong>:</p>
<ul>
<li><code>getCounterpartyEngagementKey</code> - Public endpoint, returns recipient's key</li>
<li><code>sendMessage</code> - PoW-authenticated message delivery</li>
<li><code>getChannels</code> / <code>getChannelMessages</code> - Session-authenticated queries</li>
<li><code>getInboxMessagesForSync</code> / <code>deleteInboxMessages</code> - Vault sync integration</li>
</ul>
<p><strong>Client (TypeScript/React)</strong>:</p>
<ul>
<li>ECDH shared secret computation via <code>@keypears/lib</code></li>
<li>ACS2 encryption (AES-256-CBC + SHA-256-HMAC)</li>
<li>WebGPU mining with WASM fallback</li>
<li>Background sync to move inbox messages to vault</li>
</ul>
<p><strong>Security Constants</strong>:</p>
<ul>
<li>Default difficulty: 4 million hashes (~4 seconds on GPU)</li>
<li>Max encrypted data size: 10KB per message</li>
<li>Challenge expiration: 15 minutes</li>
</ul>
<h2>What's Next</h2>
<p>This proof-of-concept handles text messages. The next steps:</p>
<ol>
<li><strong>Password attachments</strong>: Attach secrets to messages for sharing</li>
<li><strong>Cross-domain testing</strong>: Verify messaging between <code>keypears.com</code> and
<code>example.com</code></li>
<li><strong>Mobile support</strong>: Test the full flow on Android and iOS</li>
</ol>
<p>The foundation is solid. Messaging between addresses works. Now we build the
secret sharing features that will make KeyPears a truly collaborative password
manager.</p>
<h2>Try It Out</h2>
<p>The messaging system is available in the latest KeyPears development build. You
can:</p>
<ol>
<li>Create a vault at <code>name@keypears.com</code></li>
<li>Navigate to Messages</li>
<li>Click "New Message" and enter a recipient address</li>
<li>Watch the PoW mining (your GPU will spin up briefly)</li>
<li>Send your message</li>
</ol>
<p>For technical details, see our documentation:</p>
<ul>
<li><a href="https://keypears.com/docs/dh">Diffie-Hellman Protocol</a></li>
<li><a href="https://keypears.com/docs/messages">Messaging System</a></li>
</ul>
<p>The complete implementation is open source in our
<a href="https://github.com/keypears/keypears">GitHub repository</a>.</p>
<p><em>KeyPears: Your secrets, everywhere, owned by you.</em></p>]]></content>
        <author>
            <name>KeyPears Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why and How KeyPears Uses Proof-of-Work for Vault Registration]]></title>
        <id>https://keypears.com/blog/2025-12-14-pow-vault-registration</id>
        <link href="https://keypears.com/blog/2025-12-14-pow-vault-registration"/>
        <updated>2025-12-14T12:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and
cryptocurrency wallet. The design decisions described here represent our
development approach and may evolve b...]]></summary>
        <content type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and
cryptocurrency wallet. The design decisions described here represent our
development approach and may evolve before our official release.</p>
<p>This week we shipped proof-of-work for vault registration. When you create a new
vault, your device now mines a cryptographic puzzle before the server accepts
your registration. This post explains why we chose PoW over traditional
anti-spam measures and how the system works.</p>
<h2>The Problem: Spam Prevention Without Surveillance</h2>
<p>Every online service faces the same problem: how do you prevent abuse without
making legitimate users jump through hoops?</p>
<p>Traditional solutions all have tradeoffs:</p>
<p><strong>CAPTCHAs</strong> require users to identify traffic lights and crosswalks, degrading
the experience. They also send your browsing behavior to Google or hCaptcha,
compromising privacy. And determined attackers can pay humans pennies to solve
them at scale.</p>
<p><strong>Email verification</strong> requires users to have an email address and wait for a
confirmation link. It tracks users, creates friction, and doesn't work well for
privacy-conscious users who want to use temporary email addresses.</p>
<p><strong>Phone verification</strong> is even more invasive—requiring a phone number ties your
identity to a physical device and excludes users without phones.</p>
<p><strong>Rate limiting by IP</strong> blocks legitimate users behind shared networks (coffee
shops, universities, corporate NATs) while sophisticated attackers rotate
through proxy networks.</p>
<p>None of these solutions fit KeyPears. We're building a federated password
manager where users can run their own servers or choose from multiple providers.
We can't require Google reCAPTCHA on a self-hosted server. We can't demand phone
verification from privacy-conscious users. We need something that works
everywhere, respects privacy, and actually prevents abuse.</p>
<h2>The Solution: Proof-of-Work</h2>
<p>Proof-of-work flips the model. Instead of proving you're human by identifying
buses or providing personal information, you prove you're willing to spend
computational resources. Your device performs work—real, measurable, verifiable
work—before the server accepts your request.</p>
<p>This approach has several properties we care about:</p>
<p><strong>Privacy-preserving.</strong> No tracking, no third parties, no personal information.
The server only sees that you solved a cryptographic puzzle. It doesn't know who
you are, where you're from, or what browser you're using.</p>
<p><strong>Fair.</strong> Everyone follows the same rules. There's no "I'm a real person, trust
me" appeal process. You either did the work or you didn't.</p>
<p><strong>Decentralized.</strong> Works across federated servers without coordination. Each
server generates its own challenges and verifies its own proofs. No central
authority required.</p>
<p><strong>Economically effective.</strong> Mass registration attacks become expensive.
Registering 1,000 vaults requires 1,000× the computational work. At some point,
the cost exceeds the value of the attack.</p>
<p>Bitcoin popularized this idea for a different purpose (consensus), but the
underlying principle applies broadly: computational work is a scarce resource
that can't be faked.</p>
<h2>The Algorithm: pow5-64b</h2>
<p>KeyPears uses <strong>pow5</strong>, an ASIC-resistant proof-of-work algorithm originally
developed for <a href="https://earthbucks.com">EarthBucks</a>. We adapted it for our use
case while preserving the core algorithm.</p>
<h3>Why ASIC Resistance Matters</h3>
<p>A naive PoW algorithm (just hash repeatedly until you find a low value) can be
accelerated dramatically by custom hardware. Bitcoin ASICs compute SHA-256
hashes millions of times faster than GPUs. If KeyPears used simple SHA-256 PoW,
attackers with ASICs could register vaults instantly while legitimate users on
laptops waited minutes.</p>
<p>pow5 resists this by combining BLAKE3 hashing with matrix multiplication:</p>
<ol>
<li><strong>Hash the input</strong> with BLAKE3 to get a 32-byte row vector</li>
<li><strong>Iterate 32 times</strong>, each time:
<ul>
<li>Hash the working column with BLAKE3</li>
<li>Multiply-and-add each byte of the row against the new column (matmul-style)</li>
</ul>
</li>
<li><strong>Expand</strong> the 32 u32 result to 128 bytes</li>
<li><strong>Hash</strong> the expanded result with BLAKE3 for the final output</li>
</ol>
<p>The matrix multiplication step is the key. GPUs are already optimized for
matmul—it's the core operation in machine learning. Building an ASIC that
outperforms a GPU at matmul is extremely difficult and expensive. This means
legitimate users with consumer GPUs can mine efficiently, while attackers can't
gain a massive advantage with custom hardware.</p>
<h3>The pow5-64b Variant</h3>
<p>KeyPears uses pow5-64b, which takes a 64-byte input:</p>
<ul>
<li><strong>Bytes 0-31</strong>: Nonce region (the miner searches this space)</li>
<li><strong>Bytes 32-63</strong>: Challenge from the server (fixed)</li>
</ul>
<p>The GPU iterates through nonce values, computing pow5 hashes until it finds one
below the target threshold. With 32 bytes of nonce space (2^256 possibilities),
there's always a solution—the question is how long it takes to find one.</p>
<h2>Variable Difficulty: Short Names Cost More</h2>
<p>Not all vault registrations are equal. A vault named <code>alice</code> is more valuable
than one named <code>alice-johnson-2024</code>—just like <code>cars.com</code> is more valuable than
<code>alice-johnson-cars-for-sale-2024.com</code>.</p>
<p>We encode this into the difficulty formula:</p>
<pre><code>difficulty = 4,194,304 × 2^(10 - name_length)
</code></pre>
<p>For names with 10 or more characters, the base difficulty (4,194,304, or 2^22)
applies. For shorter names, difficulty doubles with each character removed:</p>
<p>| Name Length | Difficulty | GPU Time | CPU Time |
| ----------- | ---------- | -------- | -------- |
| 3 chars     | 512M       | ~8 min   | ~85 min  |
| 4 chars     | 256M       | ~4 min   | ~43 min  |
| 5 chars     | 128M       | ~2 min   | ~21 min  |
| 6 chars     | 64M        | ~1 min   | ~11 min  |
| 7 chars     | 32M        | ~32 sec  | ~5 min   |
| 8 chars     | 16M        | ~16 sec  | ~3 min   |
| 9 chars     | 8M         | ~8 sec   | ~80 sec  |
| 10+ chars   | 4M         | ~4 sec   | ~40 sec  |</p>
<p>This creates natural economics:</p>
<ul>
<li><strong>Typical users</strong> pick descriptive names (8+ characters) and wait a few
seconds</li>
<li><strong>Premium names</strong> require significant investment, discouraging squatting</li>
<li><strong>Mass registration</strong> of short names becomes computationally prohibitive</li>
</ul>
<p>The difficulty calculation lives in <code>@keypears/lib</code>:</p>
<pre><code class="language-typescript">export const BASE_REGISTRATION_DIFFICULTY = 4194304n; // 2^22

export function difficultyForName(name: string): bigint {
  const length = name.length;

  // Names at or above 10 chars get base difficulty
  if (length >= 10) {
    return BASE_REGISTRATION_DIFFICULTY;
  }

  // Shorter names: double difficulty per character
  const exponent = 10 - length;
  return BASE_REGISTRATION_DIFFICULTY * (1n &#x3C;&#x3C; BigInt(exponent));
}
</code></pre>
<h2>The Mining Experience</h2>
<p>When you create a vault in KeyPears, here's what happens:</p>
<h3>1. Name Selection</h3>
<p>You enter your desired vault name. The UI immediately shows the required
difficulty:</p>
<pre><code>Mining difficulty: 128M (~2 minutes)
Shorter names require more work to prevent squatting.
</code></pre>
<p>This sets expectations before you commit to the process.</p>
<h3>2. Challenge Request</h3>
<p>After you set your master password, the client requests a PoW challenge from the
server:</p>
<pre><code class="language-typescript">const challenge = await client.api.getPowChallenge({
  difficulty: difficultyForName(vaultName).toString(),
});
</code></pre>
<p>The server generates a random 64-byte header, calculates the target threshold
from the difficulty, and stores the challenge in the database with a 15-minute
expiration.</p>
<h3>3. Mining</h3>
<p>The client mines the challenge using WebGPU (if available) or falls back to
WebAssembly:</p>
<p><strong>WebGPU (GPU mining)</strong>:</p>
<ul>
<li>256 threads per workgroup</li>
<li>128 workgroups per dispatch</li>
<li>32,768 hashes per iteration</li>
<li>~10x faster than CPU</li>
</ul>
<p><strong>WebAssembly (CPU mining)</strong>:</p>
<ul>
<li>Sequential execution</li>
<li>1 hash per iteration</li>
<li>Yields to UI every 10,000 iterations</li>
<li>Universal browser support</li>
</ul>
<p>The UI shows progress:</p>
<pre><code>Mining (GPU)...
Difficulty: 128M
Using WebGPU
4,128,768 hashes (2.3s)
</code></pre>
<p>Users can cancel at any time if they change their mind.</p>
<h3>4. Verification and Registration</h3>
<p>Once the miner finds a valid nonce, the client submits the registration request
with the solved challenge:</p>
<pre><code class="language-typescript">await client.api.registerVault({
  vaultId,
  name: vaultName,
  domain,
  vaultPubKeyHash,
  vaultPubKey,
  loginKey,
  encryptedVaultKey,
  // PoW proof
  challengeId: powResult.challengeId,
  solvedHeader: powResult.solvedHeader,
  hash: powResult.hash,
});
</code></pre>
<p>The server verifies the proof before creating the vault.</p>
<h2>Security Properties</h2>
<p>PoW systems have several potential attack vectors. Here's how we address them:</p>
<h3>Challenge Freshness</h3>
<p>Challenges expire after 15 minutes. This extended window accommodates:</p>
<ul>
<li>Long mining times for short (high-difficulty) names</li>
<li>Network latency and retries</li>
<li>Users on slower devices</li>
</ul>
<p>After expiration, challenges cannot be used even if solved. This prevents
attackers from pre-computing solutions.</p>
<h3>Atomic Challenge Consumption</h3>
<p>A critical vulnerability in naive PoW implementations is the TOCTOU
(time-of-check to time-of-use) race condition. If two requests arrive
simultaneously with the same solved challenge, both might pass verification
before either marks the challenge as used.</p>
<p>We prevent this with an atomic database operation:</p>
<pre><code class="language-sql">UPDATE pow_challenge
SET is_used = true, solved_header = ?, verified_at = NOW()
WHERE id = ? AND is_used = false
RETURNING id
</code></pre>
<p>The <code>WHERE is_used = false</code> clause ensures only one concurrent request can claim
a challenge. The database guarantees atomicity—if two requests race, exactly one
succeeds.</p>
<h3>Minimum Difficulty Enforcement</h3>
<p>The server enforces minimum difficulty based on the action. For vault
registration, the challenge difficulty must meet or exceed <code>difficultyForName()</code>
for the requested name. This prevents attackers from requesting easy challenges
and using them for valuable short names.</p>
<h3>Full Verification</h3>
<p>The server doesn't trust the client's claimed hash. It performs complete
verification:</p>
<ol>
<li>Look up the challenge by ID</li>
<li>Check the challenge hasn't expired</li>
<li>Verify difficulty meets the minimum for this action</li>
<li>Validate the solved header length matches the algorithm</li>
<li>Verify non-nonce bytes match the original header</li>
<li>Recompute the hash independently</li>
<li>Verify the hash meets the difficulty target</li>
<li>Atomically claim the challenge</li>
</ol>
<p>If any step fails, the registration is rejected.</p>
<h2>What Users See</h2>
<p>We worked to make the mining experience transparent and non-frustrating:</p>
<p><strong>Before mining</strong>: The difficulty and estimated time are shown when you select a
name. No surprises.</p>
<p><strong>During mining</strong>: Real-time progress shows hash count and elapsed time. A
cancel button is always available.</p>
<p><strong>After mining</strong>: Registration completes immediately once mining finishes. The
PoW is invisible at this point—you just see your new vault.</p>
<p>For typical names (8+ characters), mining takes about 4 seconds on a GPU or 40
seconds on a CPU. Most users won't even notice the delay.</p>
<h2>Future: Cross-Domain Messaging</h2>
<p>Vault registration is just the first use of PoW in KeyPears. We're planning to
use it for cross-domain messaging as well.</p>
<p>When <code>alice@server1.com</code> wants to send a secret to <code>bob@server2.com</code>, she'll
need to complete a PoW challenge. This prevents:</p>
<ul>
<li>Spam flooding between users</li>
<li>Harassment campaigns</li>
<li>Automated message attacks</li>
</ul>
<p>The difficulty for messaging will be lower than registration (messages should be
quick to send), and established relationships can skip PoW entirely. But for
first contact with unknown users, computational work provides a privacy-
preserving throttle.</p>
<h2>Conclusion</h2>
<p>Proof-of-work gives us something unique: spam prevention that respects privacy.</p>
<p>No CAPTCHAs asking you to identify motorcycles. No email verification tracking
your address. No phone numbers tying your identity to a SIM card. Just
computational work—anonymous, verifiable, and fair.</p>
<p>The pow5 algorithm ensures this work can't be shortcut with specialized
hardware. Variable difficulty makes short names valuable without blocking normal
users. Atomic challenge consumption prevents race condition attacks. And the
15-minute expiration window stops pre-computation.</p>
<p>For users, the experience is simple: pick a name, set a password, wait a few
seconds, and your vault is ready. The cryptographic machinery is invisible. But
behind the scenes, that brief wait represents real computational investment—
investment that would be prohibitively expensive to replicate at scale for
attackers.</p>
<p>This is what we mean by "the convenience of cloud sync with the security of
self-custody." Even the anti-spam mechanism preserves your privacy.</p>
<p><em>Next up: the Diffie-Hellman key exchange protocol for cross-user secret
sharing. Stay tuned!</em></p>]]></content>
        <author>
            <name>KeyPears Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Server-Generated Public Keys: How KeyPears Keeps Private Keys Client-Side]]></title>
        <id>https://keypears.com/blog/2025-12-13-server-generated-public-keys</id>
        <link href="https://keypears.com/blog/2025-12-13-server-generated-public-keys"/>
        <updated>2025-12-13T12:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and
cryptocurrency wallet. The design decisions described here represent our
development approach and may evolve b...]]></summary>
        <content type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and
cryptocurrency wallet. The design decisions described here represent our
development approach and may evolve before our official release.</p>
<p>This week we shipped the key derivation system that powers KeyPears' upcoming
Diffie-Hellman key exchange. It solves a fundamental problem: how can a server
generate public keys for users while ensuring only the user can derive the
corresponding private keys?</p>
<p>This post explains the mathematics, the implementation, and why this matters for
federated secret sharing.</p>
<h2>The Problem: Offline Key Generation</h2>
<p>Imagine Alice wants to send an encrypted secret to Bob. She needs Bob's public
key to establish a shared secret via Diffie-Hellman. But Bob is offline—maybe
he's on a plane, or his phone is dead, or he simply hasn't opened the app in
days.</p>
<p>In a centralized system, this is easy: the server stores Bob's public key and
hands it to Alice. But KeyPears is designed so that servers never learn users'
private keys. The server knows Bob's vault public key, but we don't want to
expose that directly—doing so would enable correlation across all of Bob's
relationships and create a single point of failure if that key were ever
compromised.</p>
<p>We need the server to generate a fresh, unique public key for each relationship
(Alice↔Bob, Carol↔Bob, etc.) while ensuring:</p>
<ol>
<li>Only Bob can derive the corresponding private key</li>
<li>The server never learns Bob's vault private key</li>
<li>Each derived key is cryptographically isolated</li>
</ol>
<p>How do we square this circle?</p>
<h2>The Key Mathematical Property</h2>
<p>The answer lies in a beautiful property of elliptic curve cryptography. On the
secp256k1 curve (the same one Bitcoin uses), there's a generator point <code>G</code>. Any
private key <code>a</code> has a corresponding public key <code>A = a * G</code> (scalar
multiplication).</p>
<p>Here's the critical insight:</p>
<p><strong>If you add two private keys together, the result corresponds to adding their
public keys together.</strong></p>
<pre><code>Given:
  Private key a → Public key A (where A = a * G)
  Private key b → Public key B (where B = b * G)

Then:
  (a + b) * G = A + B
</code></pre>
<p>This means we can construct a derived keypair by addition:</p>
<ul>
<li><strong>Derived public key</strong> = Vault public key + Derivation public key</li>
<li><strong>Derived private key</strong> = Vault private key + Derivation private key</li>
</ul>
<p>The server knows the vault public key and can generate the derivation keypair.
So the server can compute the derived public key. But only Bob knows his vault
private key, so only Bob can compute the derived private key.</p>
<h2>Three Entropy Sources</h2>
<p>KeyPears combines three sources of entropy to generate derived keys:</p>
<p><strong>1. Server Entropy</strong> (from <code>DERIVATION_ENTROPY_N</code> environment variables)</p>
<p>The server maintains one or more 32-byte entropy values, numbered sequentially.
The highest-numbered entropy is used for new keys; older entropy is retained for
re-derivation. This enables rotation: add <code>DERIVATION_ENTROPY_2</code>, and new keys
use it while old keys remain derivable.</p>
<p><strong>2. DB Entropy</strong> (random 32 bytes per derived key)</p>
<p>Each derived key gets fresh random entropy, stored in the database. This ensures
every key is unique, even for the same user. Compromise of one key's entropy
doesn't help attack others.</p>
<p><strong>3. Vault Key</strong> (user's master private key)</p>
<p>The user's 32-byte secp256k1 private key never leaves their device. It's the
"secret ingredient" that only the client possesses. The server knows the
corresponding public key but can't reverse it.</p>
<h2>The Cryptographic Flow</h2>
<h3>Server-Side: Generate Derived Public Key</h3>
<p>When Alice requests a key for Bob, Bob's server performs:</p>
<pre><code>1. db_entropy = random(32 bytes)
2. derivation_privkey = HMAC-SHA256(key: server_entropy, data: db_entropy)
3. derivation_pubkey = derivation_privkey * G
4. derived_pubkey = vault_pubkey + derivation_pubkey
</code></pre>
<p>The server stores <code>db_entropy</code> and <code>server_entropy_index</code> in the database, then
returns <code>derived_pubkey</code> to Alice. Alice can now encrypt to Bob using this
public key.</p>
<h3>Client-Side: Derive Private Key</h3>
<p>When Bob comes online and needs to decrypt Alice's message:</p>
<pre><code>1. Bob requests derivation_privkey from server
2. Server recomputes: derivation_privkey = HMAC-SHA256(server_entropy, db_entropy)
3. Server returns derivation_privkey to Bob
4. Bob computes: derived_privkey = vault_privkey + derivation_privkey
5. Bob verifies: derived_privkey * G == derived_pubkey
</code></pre>
<p>The verification step ensures nothing went wrong. If the derived public key
matches, Bob knows he has the correct private key and can proceed with
decryption.</p>
<h2>Code Examples with @webbuf Packages</h2>
<p>KeyPears uses the <code>@webbuf</code> family of packages for cryptographic operations.
These are Rust implementations compiled to WebAssembly, providing both memory
safety and cross-platform consistency.</p>
<h3>Deriving the Derivation Private Key</h3>
<p>The server computes the derivation private key using HMAC-SHA256:</p>
<pre><code class="language-typescript">import { sha256Hmac } from "@webbuf/sha256";
import { FixedBuf } from "@webbuf/fixedbuf";

function deriveDerivationPrivKey(
  serverEntropy: FixedBuf&#x3C;32>,
  dbEntropy: FixedBuf&#x3C;32>,
): FixedBuf&#x3C;32> {
  return sha256Hmac(serverEntropy.buf, dbEntropy.buf);
}
</code></pre>
<p>HMAC provides domain separation: even if an attacker knew <code>db_entropy</code>, they
couldn't compute <code>derivation_privkey</code> without <code>server_entropy</code>.</p>
<h3>Computing Public Keys</h3>
<p>The server derives the derivation public key and adds it to the vault public
key:</p>
<pre><code class="language-typescript">import { publicKeyCreate, publicKeyAdd } from "@webbuf/secp256k1";

// derivation_pubkey = derivation_privkey * G
const derivationPubKey = publicKeyCreate(derivationPrivKey);

// derived_pubkey = vault_pubkey + derivation_pubkey
const derivedPubKey = publicKeyAdd(vaultPubKey, derivationPubKey);
</code></pre>
<h3>Client-Side Private Key Addition</h3>
<p>When the user needs the derived private key:</p>
<pre><code class="language-typescript">import { privateKeyAdd, publicKeyCreate } from "@webbuf/secp256k1";

// derived_privkey = vault_privkey + derivation_privkey
const derivedPrivKey = privateKeyAdd(vaultPrivKey, derivationPrivKey);

// Verify: derived_privkey * G should equal derived_pubkey
const verifyPubKey = publicKeyCreate(derivedPrivKey);
if (verifyPubKey.toHex() !== derivedPubKey.toHex()) {
  throw new Error("Derived key verification failed");
}
</code></pre>
<p>The verification step is crucial. It confirms that the addition was performed
correctly and that all parties agree on the final keypair.</p>
<h2>Connection to Diffie-Hellman Key Exchange</h2>
<p>This key derivation system is the foundation for KeyPears' federated
Diffie-Hellman key exchange. Here's how the pieces fit together:</p>
<p><strong>Per-Relationship Keys:</strong> When Alice wants to communicate with Bob, Bob's
server generates a derived key specifically for the Alice↔Bob relationship. This
key is mathematically linked to Bob's vault key but reveals nothing about it.</p>
<p><strong>Privacy Isolation:</strong> If an attacker compromises the Alice↔Bob derived key,
they learn nothing about Bob's vault key or his keys for other relationships.
Each relationship is cryptographically isolated.</p>
<p><strong>Offline Operation:</strong> Alice can initiate contact with Bob even when Bob is
offline. The server provides Bob's derived public key immediately. Bob derives
the matching private key whenever he comes online.</p>
<p><strong>Quantum Resistance:</strong> By never exposing the primary vault public key, we limit
the attack surface. A quantum computer would need to target each derived key
individually rather than compromising all relationships at once.</p>
<p>The full Diffie-Hellman protocol builds on this: Alice and Bob each get derived
keys for their relationship, compute a shared secret via ECDH, and use that
secret to encrypt communications. The servers coordinate but never see plaintext.</p>
<h2>Security Properties</h2>
<p><strong>Server Never Learns Vault Private Key</strong></p>
<p>The server knows <code>vault_pubkey</code>, <code>derivation_privkey</code>, and <code>derived_pubkey</code>. But
computing <code>vault_privkey</code> would require solving the discrete logarithm
problem—computationally infeasible on secp256k1.</p>
<p><strong>Per-Key Entropy Isolation</strong></p>
<p>Each derived key uses fresh <code>db_entropy</code>. Even if an attacker compromised the
database and obtained all <code>db_entropy</code> values, they'd still need <code>server_entropy</code>
to compute any <code>derivation_privkey</code>. And even with both, they'd need
<code>vault_privkey</code> to compute <code>derived_privkey</code>.</p>
<p><strong>Entropy Rotation Limits Blast Radius</strong></p>
<p>Server entropy rotates periodically (we recommend every 90 days). Each derived
key records which entropy index was used. If entropy N is somehow compromised,
only keys using index N are affected—and the attacker still needs per-key
<code>db_entropy</code> and user <code>vault_privkey</code>.</p>
<p><strong>Verification Prevents Subtle Attacks</strong></p>
<p>The client always verifies that <code>derived_privkey * G == derived_pubkey</code>. This
catches any corruption, miscalculation, or tampering. If verification fails, the
client rejects the key.</p>
<h2>What We Built</h2>
<p>This week we implemented the complete key derivation system:</p>
<ul>
<li><strong><code>@keypears/lib</code></strong>: Exported <code>privateKeyAdd</code>, <code>publicKeyAdd</code>, and
<code>deriveDerivationPrivKey</code> functions</li>
<li><strong><code>@keypears/api-server</code></strong>: Created <code>createDerivedKey</code>, <code>getDerivedKeys</code>, and
<code>getDerivationPrivKey</code> procedures</li>
<li><strong><code>@keypears/tauri-ts</code></strong>: Built a "Keys" page where users can generate derived
keys and reveal private keys on demand</li>
</ul>
<p>The infrastructure is in place. Next, we'll build the full Diffie-Hellman key
exchange protocol on top of it, enabling <code>alice@example.com</code> to securely share
secrets with <code>bob@company.com</code> across different domains and hosting providers.</p>
<h2>Conclusion</h2>
<p>The key derivation system demonstrates a pattern we use throughout KeyPears:
servers coordinate without learning secrets. By exploiting the additive property
of elliptic curve keys, we enable server-side public key generation while
keeping private keys strictly client-side.</p>
<p>This isn't just a clever trick—it's the foundation for federated secret sharing.
When you share a password with a colleague at a different company, both servers
help coordinate the key exchange, but neither server can read the shared secret.
That's the promise of KeyPears: the convenience of cloud sync with the security
of self-custody.</p>
<p>The math is elegant. The implementation is straightforward. And the security
properties are exactly what we need for a federated Diffie-Hellman key exchange
system.</p>
<p><em>Next up: the full DH key exchange protocol for cross-user secret sharing. Stay
tuned!</em></p>]]></content>
        <author>
            <name>KeyPears Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why We Switched from Blake3 to SHA-256]]></title>
        <id>https://keypears.com/blog/2025-12-09-switching-to-sha256</id>
        <link href="https://keypears.com/blog/2025-12-09-switching-to-sha256"/>
        <updated>2025-12-09T12:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and
cryptocurrency wallet. The design decisions described here represent our
development approach and may evolve b...]]></summary>
        <content type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and
cryptocurrency wallet. The design decisions described here represent our
development approach and may evolve before our official release.</p>
<p>Last week we completed a significant migration: replacing Blake3 with SHA-256
throughout the KeyPears codebase. This wasn't because Blake3 failed us—it worked
perfectly. We switched because SHA-256 is the industry standard, and that
matters more than we initially appreciated.</p>
<p>This post explains the technical tradeoffs and why we made this decision.</p>
<h2>What We Changed</h2>
<p>The migration touched every layer of our stack:</p>
<p><strong>Dependencies:</strong></p>
<ul>
<li><code>@webbuf/blake3</code> → <code>@webbuf/sha256</code></li>
<li><code>@webbuf/acb3</code> → <code>@webbuf/acs2</code></li>
</ul>
<p><strong>Functions:</strong></p>
<ul>
<li><code>blake3Hash()</code> → <code>sha256Hash()</code></li>
<li><code>blake3Mac()</code> → <code>sha256Hmac()</code></li>
<li><code>blake3Pbkdf()</code> → <code>sha256Pbkdf()</code></li>
<li><code>acb3Encrypt()</code> / <code>acb3Decrypt()</code> → <code>acs2Encrypt()</code> / <code>acs2Decrypt()</code></li>
</ul>
<p><strong>Encryption scheme:</strong></p>
<ul>
<li>ACB3 (AES-256-CBC + Blake3-MAC) → ACS2 (AES-256-CBC + SHA-256-HMAC)</li>
</ul>
<p>Both hash functions produce 32-byte (256-bit) output, so our <code>FixedBuf&#x3C;32></code>
types remained unchanged. The migration was primarily find-and-replace with
updated test vectors.</p>
<h2>Why Blake3 Is Excellent</h2>
<p>Let's be clear: Blake3 is a technically superior hash function in many respects.</p>
<p><strong>Speed</strong>: Blake3 is significantly faster than SHA-256, especially for large
data. It achieves this through a Merkle tree construction that enables parallel
hashing across multiple CPU cores. Where SHA-256 processes data sequentially,
Blake3 can divide large inputs into chunks and hash them simultaneously.</p>
<p><strong>Modern design</strong>: Blake3 was designed in 2020 with modern cryptographic
insights. It's built on the well-analyzed BLAKE2 (used in Argon2, the
recommended password hashing algorithm) and incorporates lessons from decades of
hash function cryptanalysis.</p>
<p><strong>Versatility</strong>: Blake3 supports keyed hashing (MAC), key derivation (KDF), and
extendable output (XOF) natively. SHA-256 requires wrapper constructions like
HMAC and HKDF for equivalent functionality.</p>
<p><strong>Simplicity</strong>: Despite its speed, Blake3 has a remarkably simple specification.
The reference implementation is ~500 lines of C.</p>
<p>We were happy with Blake3. Our key derivation system worked flawlessly:</p>
<pre><code class="language-typescript">// Three-tier key derivation (unchanged structure, new primitives)
function sha256Pbkdf(
  password: string | WebBuf,
  salt: FixedBuf&#x3C;32>,
  rounds: number = 100_000,
): FixedBuf&#x3C;32> {
  const passwordBuf =
    typeof password === "string" ? WebBuf.fromUtf8(password) : password;

  let result = sha256Hmac(salt, passwordBuf);
  for (let i = 1; i &#x3C; rounds; i++) {
    result = sha256Hmac(salt, result.buf);
  }
  return result;
}
</code></pre>
<h2>Why We Switched Anyway</h2>
<h3>1. The Speed Advantage Doesn't Apply to Our Use Case</h3>
<p>Blake3's killer feature is parallel hashing of large data. But KeyPears doesn't
hash large data. We hash:</p>
<ul>
<li><strong>Passwords</strong>: 8-64 characters (typically under 100 bytes)</li>
<li><strong>Secrets</strong>: API keys, credentials, wallet seeds (typically under 1KB)</li>
<li><strong>Keys</strong>: 32-byte fixed buffers</li>
</ul>
<p>For inputs this small, Blake3's Merkle tree construction provides no benefit.
The parallelization overhead might actually make it slower than sequential
hashing for tiny inputs. And even if Blake3 were faster for small data, the
difference would be measured in microseconds—completely irrelevant when our key
derivation performs 100,000 PBKDF rounds that dominate execution time.</p>
<p>When we profiled vault creation:</p>
<ul>
<li>Key derivation: ~800ms (100k rounds × 2 keys)</li>
<li>Individual hash operations: ~0.001ms</li>
</ul>
<p>The hash function choice affects performance by roughly 0.0001%. Switching from
Blake3 to SHA-256 has no measurable impact on user experience.</p>
<h3>2. Industry Standard Matters for Customer Acquisition</h3>
<p>When enterprise customers evaluate password managers, they ask questions like:</p>
<ul>
<li>"What encryption algorithm do you use?"</li>
<li>"Is your cryptography FIPS 140-2 compliant?"</li>
<li>"Do you use industry-standard algorithms?"</li>
</ul>
<p>With Blake3, our answers required explanation: "We use Blake3, which is a modern
hash function designed in 2020. It's based on BLAKE2, which is used in Argon2.
It's very fast and secure, though it's not yet widely adopted..."</p>
<p>With SHA-256, our answer is: "Yes."</p>
<p>SHA-256 is:</p>
<ul>
<li>Part of the SHA-2 family standardized by NIST</li>
<li>FIPS 140-2 approved</li>
<li>Used by Bitcoin, TLS, HTTPS, and virtually every security system</li>
<li>Understood by every security auditor</li>
<li>Required by many compliance frameworks</li>
</ul>
<p>The security difference between Blake3 and SHA-256 is negligible for our threat
model—both provide 256-bit security against preimage and collision attacks. But
the compliance difference is significant.</p>
<h3>3. Battle-Tested at Scale</h3>
<p>SHA-256 has been deployed in production systems since 2001. It secures:</p>
<ul>
<li>Every Bitcoin transaction ever made (~900 million transactions)</li>
<li>Every HTTPS connection using TLS (trillions daily)</li>
<li>Every Git commit in every repository worldwide</li>
<li>Government systems, financial institutions, healthcare records</li>
</ul>
<p>This deployment scale represents the most extensive real-world cryptanalysis
possible. If SHA-256 had weaknesses, attackers with billions of dollars in
incentive would have found them.</p>
<p>Blake3 is mathematically sound and designed by respected cryptographers. But it
was released in 2020 and hasn't yet accumulated the same scale of real-world
testing. Given that both algorithms provide equivalent security for our use
case, we chose the one with 24 years of battle-testing.</p>
<h3>4. Ecosystem Compatibility</h3>
<p>SHA-256 implementations exist in every programming language, every platform, and
every hardware security module. If we ever need to:</p>
<ul>
<li>Integrate with HSMs for enterprise key management</li>
<li>Support hardware security keys (FIDO2/WebAuthn)</li>
<li>Interface with existing enterprise systems</li>
<li>Pass third-party security audits</li>
</ul>
<p>SHA-256 will be expected and supported. Blake3 might require custom integration
work.</p>
<h2>What We're Not Saying</h2>
<p>This decision is <strong>not</strong> a criticism of Blake3. We want to be explicit:</p>
<p><strong>Blake3 is secure.</strong> There are no known attacks, weaknesses, or concerns about
Blake3's cryptographic security. It was designed by a team including the
creators of Argon2 and BLAKE2.</p>
<p><strong>Blake3 is technically superior for large data.</strong> If we were building a backup
system, a file integrity checker, or a content-addressed storage system, Blake3
would be the obvious choice.</p>
<p><strong>Blake3 may become an industry standard.</strong> It's gaining adoption, and in five
years the "unknown algorithm" concern may disappear. We might even switch back.</p>
<p>We switched because <strong>SHA-256 is good enough for our specific use case, and the
industry standard status provides tangible benefits that Blake3's technical
advantages don't.</strong> This is an engineering tradeoff, not a quality judgment.</p>
<h2>The Migration Process</h2>
<p>The actual migration took about a day:</p>
<ol>
<li>
<p><strong>Update dependencies</strong>: Replace <code>@webbuf/blake3</code> with <code>@webbuf/sha256</code>,
<code>@webbuf/acb3</code> with <code>@webbuf/acs2</code></p>
</li>
<li>
<p><strong>Update function calls</strong>: Find-and-replace across lib, api-server, tauri-ts,
webapp</p>
</li>
<li>
<p><strong>Update test vectors</strong>: SHA-256 produces different output than Blake3, so
test expectations needed updating</p>
</li>
<li>
<p><strong>Update documentation</strong>: Replace Blake3 references in crypto.md, auth.md,
and AGENTS.md</p>
</li>
<li>
<p><strong>Run tests</strong>: All 71 tests pass (40 in lib, 31 in api-server)</p>
</li>
<li>
<p><strong>Manual testing</strong>: Create vaults, sync across devices, verify encryption
works</p>
</li>
</ol>
<p>The migration was straightforward because both hash functions have identical
output sizes (32 bytes) and the <code>@webbuf</code> packages have matching APIs. The
hardest part was updating documentation.</p>
<p><strong>Breaking change note</strong>: This migration breaks compatibility with any existing
encrypted data. Since we're pre-MVP with no real user data, this was acceptable.
A production migration would require a more careful versioning strategy.</p>
<h2>Current Cryptography Stack</h2>
<p>After this migration, KeyPears uses:</p>
<ul>
<li><strong>Hashing</strong>: SHA-256 (via <code>@webbuf/sha256</code>)</li>
<li><strong>MAC</strong>: SHA-256-HMAC (via <code>sha256Hmac()</code>)</li>
<li><strong>KDF</strong>: Custom PBKDF using SHA-256-HMAC (100,000 rounds)</li>
<li><strong>Encryption</strong>: ACS2 = AES-256-CBC + SHA-256-HMAC (via <code>@webbuf/acs2</code>)</li>
</ul>
<p>All cryptographic primitives are NIST-standardized algorithms with decades of
real-world deployment. The underlying implementations are Rust compiled to
WebAssembly, providing both memory safety and cross-platform consistency.</p>
<h2>Lessons Learned</h2>
<p><strong>Technical superiority isn't always the deciding factor.</strong> Blake3 is faster,
more modern, and arguably more elegant. SHA-256 is more widely understood,
trusted, and required. For a security product, trust and compliance matter as
much as technical merit.</p>
<p><strong>Know your actual use case.</strong> Blake3's parallel hashing is irrelevant when
you're hashing 32-byte keys. We spent time with an optimization we couldn't
benefit from.</p>
<p><strong>Industry standards exist for good reasons.</strong> NIST standardization, FIPS
compliance, and widespread adoption aren't bureaucratic checkboxes—they
represent accumulated trust that takes decades to build. Sometimes the "boring"
choice is the right choice.</p>
<p><strong>Migrations are easier early.</strong> Changing cryptographic primitives after MVP
launch would require careful data migration and backward compatibility. Doing it
now, with no real user data, was trivial.</p>
<h2>Conclusion</h2>
<p>We switched from Blake3 to SHA-256 not because Blake3 failed, but because
SHA-256 succeeds in ways that matter more for our product: industry recognition,
compliance compatibility, and customer trust.</p>
<p>Blake3 is an excellent hash function, and we'd recommend it for use cases that
benefit from its parallel performance—large file hashing, content-addressed
storage, or high-throughput data processing.</p>
<p>For a password manager where we're hashing small secrets and deriving keys,
SHA-256 provides identical practical security with better industry positioning.
When both options are secure, we chose the one that makes "Do you use
industry-standard cryptography?" easy to answer.</p>
<p>The migration is complete, all tests pass, and vaults sync correctly. We're back
to building features.</p>]]></content>
        <author>
            <name>KeyPears Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What We Found in Our Codebase Audit]]></title>
        <id>https://keypears.com/blog/2025-12-06-codebase-audit</id>
        <link href="https://keypears.com/blog/2025-12-06-codebase-audit"/>
        <updated>2025-12-06T18:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>We recently paused feature development to audit our entire codebase. KeyPears
shipped fast—cross-device sync, server authentication, encrypted secret
storage—and we wanted to make sure we hadn't ac...]]></summary>
        <content type="html"><![CDATA[<p>We recently paused feature development to audit our entire codebase. KeyPears
shipped fast—cross-device sync, server authentication, encrypted secret
storage—and we wanted to make sure we hadn't accumulated technical debt or,
worse, security issues along the way.</p>
<p>What we found was instructive. The good news: our architecture is sound, our
cryptography is correct, and our zero-knowledge design holds up. The concerning
news: we found debug logging that would have exposed cryptographic keys in
production. This is exactly why we audit.</p>
<h2>What We Looked For</h2>
<p>Our audit covered six packages across the monorepo, examining each for:</p>
<ol>
<li><strong>General Software Best Practices</strong>: Linting, type checking, test coverage,
code quality</li>
<li><strong>Third-Party Dependencies</strong>: Outdated packages, security vulnerabilities</li>
<li><strong>Security Assessment</strong>: Crypto implementation, zero-knowledge verification,
input validation</li>
<li><strong>Scalability</strong>: N+1 queries, memory leaks, re-render optimization</li>
<li><strong>UI/UX Consistency</strong>: Theme compliance, accessibility, loading states</li>
<li><strong>File-Specific Checks</strong>: Config correctness, route typing, component
structure</li>
</ol>
<p>We ran automated tools first (<code>pnpm lint</code>, <code>pnpm typecheck</code>, <code>pnpm test</code>, <code>cargo clippy</code>), then manually reviewed each package against our checklist.</p>
<h2>The Critical Finding: Debug Logging</h2>
<p>The most significant discovery was in our Tauri app's vault creation and import
flows. During development, we'd added extensive console.log statements to debug
the cryptographic key derivation process:</p>
<pre><code class="language-typescript">// What we found and removed:
console.log("Password Key:", passwordKey.buf.toHex());
console.log("Login Key:", loginKey.buf.toHex());
console.log("Encryption Key:", encryptionKey.buf.toHex());
console.log("Decrypted Vault Key:", vaultKey.buf.toHex());
</code></pre>
<p>There were approximately 80 of these statements across two files:
<code>import-vault.tsx</code> and <code>new-vault.3.tsx</code>. Each one logged a sensitive
cryptographic key in hexadecimal format.</p>
<p>In development, this is harmless—helpful, even, for understanding the key
derivation flow. In production, it's a disaster waiting to happen. Anyone with
access to browser developer tools could see every key involved in vault
encryption. The zero-knowledge architecture we carefully designed would be
meaningless if the client itself was leaking keys to the console.</p>
<p>We removed all 80 statements. The code now proceeds silently, as it should.</p>
<p><strong>Lesson learned</strong>: Debug logging during development is fine, but it must be
removed before shipping. Our audit checklist now includes searching for
<code>console.log</code> statements containing key-related terms (<code>Key</code>, <code>password</code>,
<code>secret</code>, <code>token</code>).</p>
<h2>Type Safety Improvements</h2>
<p>React Router v7 provides a <code>href()</code> function that type-checks route paths at
compile time. If you rename a route file, any <code>href("/old-path")</code> calls will
fail to compile—catching errors before they reach production.</p>
<p>We found several places where developers had used string literals instead:</p>
<pre><code class="language-typescript">// What we found:
&#x3C;Link to="/">Home&#x3C;/Link>
&#x3C;Link to="/new-vault/1">Create Vault&#x3C;/Link>

// What it should be:
&#x3C;Link to={href("/")}>Home&#x3C;/Link>
&#x3C;Link to={href("/new-vault/1")}>Create Vault&#x3C;/Link>
</code></pre>
<p>The difference seems minor, but it matters. With string literals, renaming
<code>/new-vault/1</code> to <code>/vault/new/step-1</code> would silently break links. With <code>href()</code>,
the compiler catches it immediately.</p>
<p>We updated all navigation in the Tauri app to use type-safe routes: navbar,
footer, vault creation wizard, import flow. It's a small change that prevents a
category of bugs entirely.</p>
<h2>UI Consistency: Theme Colors</h2>
<p>KeyPears uses the Catppuccin color palette with CSS variables for theming. Error
text should use <code>text-destructive</code>, which maps to the appropriate red in both
light and dark modes.</p>
<p>We found inconsistent usage:</p>
<pre><code class="language-typescript">// Inconsistent:
&#x3C;p className="text-red-500">{error}&#x3C;/p>

// Consistent:
&#x3C;p className="text-destructive">{error}&#x3C;/p>
</code></pre>
<p>The difference is subtle in light mode but significant in dark mode, where
<code>text-red-500</code> might not have sufficient contrast against dark backgrounds.
Using theme variables ensures the design system works correctly across all
themes.</p>
<p>We standardized error colors in several components: the password generator,
password memorizer, and vault name input.</p>
<h2>Config Typo: The Silent Build Breaker</h2>
<p>In <code>tauri.conf.json</code>, we found:</p>
<pre><code class="language-json">{
  "frontendDist": "../ts-tauri/dist"
}
</code></pre>
<p>The correct path is <code>../tauri-ts/dist</code>. The folders are named <code>tauri-ts</code> (for
TypeScript) and <code>tauri-rs</code> (for Rust), not <code>ts-tauri</code>.</p>
<p>This typo hadn't caused problems yet because we typically run the dev server
rather than building production bundles locally. But it would have failed the
first time someone tried to build a release binary, causing confusion and wasted
debugging time.</p>
<p><strong>Lesson learned</strong>: Config files deserve the same scrutiny as code. Paths, URLs,
and identifiers are easy to typo and hard to spot in review.</p>
<h2>What Passed With Flying Colors</h2>
<p>Not everything was problems. Much of the codebase held up well:</p>
<p><strong>Cryptography</strong>: Our three-tier key derivation (password → passwordKey →
encryptionKey + loginKey) is correctly implemented. The server never receives
encryption keys, only login keys—and those are further hashed server-side. The
zero-knowledge architecture is sound.</p>
<p><strong>Memory management</strong>: All intervals, timers, and event listeners in the Tauri
app have proper cleanup in <code>useEffect</code> return functions. No memory leaks.</p>
<p><strong>Sync performance</strong>: The background sync service uses exponential backoff on
errors (5s → 10s → 20s), preventing thundering herd problems. Pagination is
implemented for activity logs.</p>
<p><strong>Accessibility</strong>: Interactive elements have proper <code>aria-label</code> attributes.
Keyboard navigation works throughout the app.</p>
<p><strong>Rust code</strong>: Our Tauri backend is minimal (~43 lines) by design—all business
logic is in TypeScript. <code>cargo clippy</code> passes with no warnings.</p>
<h2>The Audit Process</h2>
<p>For future reference, here's how we structured the audit:</p>
<ol>
<li>
<p><strong>Automated checks first</strong>: Run lint, typecheck, and tests for each package.
Fix any failures before proceeding.</p>
</li>
<li>
<p><strong>Dependency review</strong>: Run <code>pnpm outdated</code> for each package. Update
dependencies to latest patch versions.</p>
</li>
<li>
<p><strong>Manual review by category</strong>: Work through the checklist systematically.
Security issues get fixed immediately; style issues get noted for later.</p>
</li>
<li>
<p><strong>Document findings</strong>: Update the audit guide with lessons learned. Future
audits benefit from past discoveries.</p>
</li>
</ol>
<p>We've published our full audit checklist in the repository at
<a href="https://github.com/keypears/keypears/blob/main/docs/audit.md">docs/audit.md</a>.
It covers everything from TypeScript best practices to zero-knowledge
architecture verification to UI accessibility checks.</p>]]></content>
        <author>
            <name>KeyPears Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Third-Party Hosting: Making KeyPears as Easy as Hosted Email]]></title>
        <id>https://keypears.com/blog/2025-12-01-third-party-hosting</id>
        <link href="https://keypears.com/blog/2025-12-01-third-party-hosting"/>
        <updated>2025-12-01T18:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>Imagine you own <code>example.com</code>. You run your own website there, but you don't run
your own email server—Gmail or Fastmail handles that for you. Your email address
is still <code>you@examp...]]></summary>
        <content type="html"><![CDATA[<p>Imagine you own <code>example.com</code>. You run your own website there, but you don't run
your own email server—Gmail or Fastmail handles that for you. Your email address
is still <code>you@example.com</code>, but Google or Fastmail does the heavy lifting of
running the mail servers, managing spam, and keeping everything online.</p>
<p>What if password management worked the same way?</p>
<p>Today we shipped the foundation for exactly that: the ability to point your
domain's KeyPears protocol at any third-party hosting provider. It's a
proof-of-concept, but it works—and it brings us one step closer to making
decentralized password management as easy as hosted email.</p>
<h2>Why Decentralization Matters</h2>
<p>Email is one of the internet's great success stories in federated architecture.
Anyone can run an email server. Gmail users can email ProtonMail users who can
email self-hosted server users. There's no central authority deciding who gets
to participate. The protocol is open, the address format is universal, and
interoperability is the default.</p>
<p>KeyPears borrows this architecture, but improves on it in a critical way:
<strong>end-to-end encryption by default</strong>. When <code>alice@keypears.com</code> shares a secret
with <code>bob@company.com</code>, the servers never see the plaintext. They're just
coordinators—dumb pipes that route encrypted blobs between clients that hold the
real keys.</p>
<p>The address format mirrors email intentionally. Your vault is
<code>yourname@yourdomain.com</code>. You can use our hosted service at <code>keypears.com</code>, run
your own server, or—with what we built today—point your domain at any KeyPears
hosting provider you trust.</p>
<p>This means companies with different domains and different service providers can
still share secrets securely. Marketing at <code>acme.com</code> (hosted by Provider A) can
share API keys with engineering at <code>partner.io</code> (self-hosted) using the same
Diffie-Hellman key exchange that makes the whole system work.</p>
<h2>The <code>.well-known/keypears.json</code> Protocol</h2>
<p>The implementation is simple. Domain owners create a file at
<code>/.well-known/keypears.json</code> that tells clients where to find the API:</p>
<pre><code class="language-json">{
  "version": 1,
  "apiUrl": "https://keypears.com/api"
}
</code></pre>
<p>That's it. When a KeyPears client needs to interact with vaults at
<code>example.com</code>, it fetches <code>https://example.com/.well-known/keypears.json</code>, reads
the <code>apiUrl</code>, and directs all API calls there.</p>
<p>If you're running your own server, the <code>apiUrl</code> points to yourself:</p>
<pre><code class="language-json">{
  "version": 1,
  "apiUrl": "https://example.com/api"
}
</code></pre>
<p>If you're using a third-party host like <code>keypears.com</code>:</p>
<pre><code class="language-json">{
  "version": 1,
  "apiUrl": "https://keypears.com/api"
}
</code></pre>
<p>The pattern follows the established convention of <code>.well-known</code> files that power
everything from SSL certificate validation (<code>.well-known/acme-challenge</code>) to
security contact information (<code>.well-known/security.txt</code>). It's a proven
approach for domain-level configuration.</p>
<h2>What We Built Today</h2>
<p>This week we implemented the complete infrastructure for this feature:</p>
<p><strong>In the library (<code>@keypears/lib</code>):</strong></p>
<ul>
<li>A Zod schema (<code>KeypearsJsonSchema</code>) that validates the <code>keypears.json</code> format</li>
<li>A <code>buildBaseUrl()</code> helper for constructing domain URLs</li>
</ul>
<p><strong>In the API server (<code>@keypears/api-server</code>):</strong></p>
<ul>
<li>Updated <code>validateKeypearsServer()</code> to parse and return the <code>apiUrl</code></li>
</ul>
<p><strong>In the webapp (<code>@keypears/webapp</code>):</strong></p>
<ul>
<li>A dynamic React Router resource route that serves <code>keypears.json</code></li>
<li>Environment-aware configuration (production vs development URLs)</li>
</ul>
<p><strong>In the Tauri app (<code>@keypears/tauri-ts</code>):</strong></p>
<ul>
<li><code>fetchApiUrl()</code> function that retrieves and caches API URLs from <code>keypears.json</code></li>
<li>Updated all API client calls to use the discovered URL instead of constructing it</li>
</ul>
<p>The key insight is that clients no longer assume the API lives at
<code>https://domain.com/api</code>. They discover it dynamically. This single change
enables the entire third-party hosting model.</p>
<h2>What's Still Needed</h2>
<p>We want to be transparent: this is a proof-of-concept, not a production-ready
feature. There's a critical missing piece.</p>
<p><strong>The problem:</strong> Right now, anyone could create a <code>keypears.json</code> file claiming
that <code>keypears.com/api</code> hosts vaults for <code>example.com</code>. There's no verification
that the owner of <code>example.com</code> actually authorized this.</p>
<p><strong>The solution:</strong> Before launch, we'll add a public key (or public key hash) to
the <code>keypears.json</code> file. The domain owner will need to prove they control this
key, likely through a challenge-response protocol or by publishing the key in
DNS. This cryptographic proof ensures that only the legitimate domain owner can
authorize a hosting provider.</p>
<p>The future format might look like:</p>
<pre><code class="language-json">{
  "version": 2,
  "apiUrl": "https://keypears.com/api",
  "domainPubKeyHash": "a1b2c3d4..."
}
</code></pre>
<p>We haven't implemented this yet because the current proof-of-concept is
sufficient for development and testing. The infrastructure is in place; the
authentication layer comes next.</p>
<h2>The Bigger Picture</h2>
<p>KeyPears is building toward a world where password management works like email
should have worked from the start: decentralized, interoperable, and encrypted
by default.</p>
<ul>
<li><strong>Decentralized:</strong> No single company controls the network. Run your own server
or choose a provider you trust.</li>
<li><strong>Interoperable:</strong> <code>alice@keypears.com</code> can share secrets with
<code>bob@selfhosted.org</code> seamlessly.</li>
<li><strong>End-to-end encrypted:</strong> Servers are dumb coordinators. They never see your
passwords, your keys, or your plaintext secrets.</li>
<li><strong>Self-custody with convenience:</strong> You control your keys, but you get the sync
and sharing features of cloud-based managers.</li>
</ul>
<p>The third-party hosting feature is a key piece of this puzzle. It means you
don't have to choose between running your own infrastructure and using someone
else's domain. You can have your cake and eat it too: your domain, your
identity, someone else's servers.</p>
<h2>What's Next</h2>
<p>With third-party hosting infrastructure in place, our next priority is the
Diffie-Hellman key exchange protocol for secure secret sharing between users.
This is the feature that makes KeyPears more than just a password manager—it's
what enables <code>alice@company.com</code> to securely share credentials with
<code>bob@partner.io</code> without either server ever seeing the plaintext.</p>
<p>After DH key exchange, we'll focus on:</p>
<ul>
<li>Multi-domain support (official KeyPears domains beyond <code>keypears.com</code>)</li>
<li>Domain ownership verification (the public key piece mentioned above)</li>
<li>Payment and business model (freemium with premium custom domain hosting)</li>
</ul>
<p>The architecture is coming together. Each piece we build makes the next piece
possible. Today's proof-of-concept becomes tomorrow's production feature.</p>
<p>If you're interested in following our progress, the code is open source and
available on <a href="https://github.com/keypears/keypears">GitHub</a>. We're building in
public because we believe the best security software is software you can verify.</p>
<p><em>Next up: Diffie-Hellman key exchange for cross-user secret sharing. Stay
tuned!</em></p>]]></content>
        <author>
            <name>KeyPears Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building Secure Cross-Device Sync for a Decentralized Password Manager]]></title>
        <id>https://keypears.com/blog/2025-11-30-cross-device-sync</id>
        <link href="https://keypears.com/blog/2025-11-30-cross-device-sync"/>
        <updated>2025-12-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>After weeks of intensive development, KeyPears now has a fully functional
cross-device synchronization system. This wasn't just about making data appear
on multiple devices—it was about building a ...]]></summary>
        <content type="html"><![CDATA[<p>After weeks of intensive development, KeyPears now has a fully functional
cross-device synchronization system. This wasn't just about making data appear
on multiple devices—it was about building a secure, privacy-preserving sync
architecture that works in a decentralized environment where users can run their
own servers. Here's how we did it.</p>
<h2>The Challenge</h2>
<p>Building sync for a password manager is fundamentally different from typical app
synchronization. Every design decision has security implications. When you're
also committed to a decentralized architecture where users might run their own
servers, the complexity multiplies. We needed to solve several interconnected
problems:</p>
<ol>
<li><strong>Authentication without centralization</strong> - No global user accounts or OAuth
providers</li>
<li><strong>Device identity with privacy</strong> - Track devices without cross-domain
correlation</li>
<li><strong>Session management at scale</strong> - Support hundreds of API calls without
exposing long-term credentials</li>
<li><strong>Sync conflict resolution</strong> - Handle concurrent edits from multiple devices</li>
<li><strong>Zero-knowledge architecture</strong> - Servers should never see passwords or
encryption keys</li>
</ol>
<h2>The Solution: 2,000+ Lines of Carefully Crafted Code</h2>
<p>Over the past week, we've implemented a comprehensive solution spanning 40 files
with over 2,000 lines of new code. Here's what we built:</p>
<h3>1. Session-Based Authentication System</h3>
<p>The biggest change was moving from a "login key with every request" model to
proper session-based authentication. This reduced our attack surface
dramatically:</p>
<p><strong>Before:</strong> Login key sent 100+ times per session <strong>After:</strong> Login key sent once
per 24 hours</p>
<p>The new authentication flow works like this:</p>
<pre><code class="language-typescript">// Login once per day
const { sessionToken, expiresAt } = await client.login({
  vaultId,
  loginKey,
  deviceId,
  deviceDescription: "macOS 14.1 (aarch64)",
});

// Use session token for all subsequent requests
const secrets = await client.getSecretUpdates({
  vaultId,
  lastUpdatedAt: lastSync,
});
</code></pre>
<p>But here's the security innovation: we store session tokens as Blake3 hashes in
the database. Even if someone breaches the server database, they can't use the
stolen hashes—they'd need the original 32-byte random tokens, which exist only
in client memory.</p>
<h3>2. Privacy-Preserving Device Tracking</h3>
<p>Unlike centralized password managers that assign global device IDs, KeyPears
generates a unique device ID for each vault. This means:</p>
<ul>
<li>Your work vault and personal vault have different device IDs</li>
<li>Servers can't correlate devices across domains</li>
<li>Complete privacy preservation in a decentralized architecture</li>
</ul>
<p>Each device gets identified with:</p>
<ul>
<li>A ULID (Universally Unique Lexicographically Sortable Identifier) per vault</li>
<li>Auto-detected OS information: "iPhone (iOS 17.2)" or "Windows 11 (x86_64)"</li>
<li>User-editable friendly names: "Ryan's MacBook Pro"</li>
</ul>
<h3>3. Background Sync Service</h3>
<p>We built a robust background synchronization service that polls for changes
every 5 seconds:</p>
<pre><code class="language-typescript">// Start sync when vault is unlocked
startBackgroundSync(vaultId, vaultDomain, vaultKey, getSession);

// Automatic sync every 5 seconds
// Manual sync after creating/editing secrets
await triggerManualSync();
</code></pre>
<p>The sync service includes sophisticated error handling:</p>
<ul>
<li><strong>401 Unauthorized</strong>: Stop syncing, prompt for re-authentication</li>
<li><strong>500+ Server Error</strong>: Exponential backoff (5s → 10s → 20s)</li>
<li><strong>Network Error</strong>: Keep retrying at normal interval</li>
<li><strong>Session Expiring</strong>: Skip sync, avoid unnecessary 401s</li>
</ul>
<h3>4. Three-Tier Key Derivation System</h3>
<p>We implemented a sophisticated key hierarchy that separates authentication from
encryption:</p>
<pre><code>Master Password
    ↓ (100k rounds PBKDF)
Password Key (cached, PIN-encrypted)
    ├→ Encryption Key (device only, decrypts vault)
    └→ Login Key (sent to server for auth)
        ↓ (1k rounds on server)
    Hashed Login Key (database storage)
</code></pre>
<p>This asymmetric round count (100k client, 1k server) is intentional—the heavy
computation happens client-side for security, while the server just needs to
prevent raw token storage.</p>
<h3>5. Comprehensive Test Coverage</h3>
<p>We didn't just write code; we wrote tests. Lots of them:</p>
<ul>
<li><strong>283 lines</strong> of authentication tests</li>
<li><strong>278 lines</strong> of device session tests</li>
<li>Integration tests for the complete sync flow</li>
<li>Database migration tests for schema changes</li>
</ul>
<h2>Implementation Highlights</h2>
<h3>React Closure Bug Fix</h3>
<p>One of the trickiest bugs involved React closures capturing stale state. The
sync service was always getting a <code>null</code> session token because the closure
captured the initial value:</p>
<pre><code class="language-typescript">// BUG: Closure captures initial null value
startBackgroundSync(vaultId, domain, key, () => session?.token);

// FIX: Use ref to get current value
const sessionRef = useRef(session);
useEffect(() => {
  sessionRef.current = session;
}, [session]);
startBackgroundSync(vaultId, domain, key, () => sessionRef.current?.token);
</code></pre>
<h3>Tauri Plugin Integration</h3>
<p>We integrated Tauri's OS detection plugin to automatically identify devices:</p>
<pre><code class="language-rust">// Rust side
.plugin(tauri_plugin_os::init())

// TypeScript side
import { platform, version, arch } from "@tauri-apps/plugin-os";
const description = `${platform()} ${version()} (${arch()})`;
</code></pre>
<h3>Database Schema Evolution</h3>
<p>Added a new <code>device_session</code> table with careful constraints:</p>
<pre><code class="language-sql">CREATE TABLE device_session (
  id TEXT PRIMARY KEY,           -- ULID
  vault_id TEXT NOT NULL,
  device_id TEXT NOT NULL,        -- Per-vault device ULID
  hashed_session_token TEXT,      -- Blake3 hash, not raw token
  expires_at INTEGER NOT NULL,
  last_activity INTEGER NOT NULL,
  UNIQUE(vault_id, device_id),    -- One session per device
  FOREIGN KEY(vault_id) REFERENCES vault(id) ON DELETE CASCADE
);
</code></pre>
<h2>Security Improvements</h2>
<p>The new system dramatically improves our security posture:</p>
<p>| Attack Vector         | Before             | After                  |
| --------------------- | ------------------ | ---------------------- |
| Token Interception    | Permanent access   | 24-hour maximum window |
| Database Breach       | Login keys exposed | Only unusable hashes   |
| Device Compromise     | No revocation      | Per-device logout      |
| Replay Attacks        | Vulnerable         | Time-limited tokens    |
| Cross-Domain Tracking | Possible           | Prevented by design    |</p>
<h2>Performance Optimizations</h2>
<p>Beyond security, we optimized for performance:</p>
<ol>
<li><strong>Smart Polling</strong>: Only sync when there's an active session</li>
<li><strong>Exponential Backoff</strong>: Reduce server load during errors</li>
<li><strong>Debounced Updates</strong>: Batch rapid changes together</li>
<li><strong>Selective Sync</strong>: Only fetch changes since last sync timestamp</li>
</ol>
<h2>What's Next</h2>
<p>With cross-device sync complete, KeyPears is approaching feature parity with
centralized password managers—while maintaining its decentralized,
privacy-preserving architecture. The foundation we've built enables future
features like:</p>
<ul>
<li>Multi-factor authentication (MFA)</li>
<li>Device trust levels and approval workflows</li>
<li>Geographic anomaly detection</li>
<li>Granular access controls</li>
<li>Offline-first mobile apps</li>
</ul>
<h2>Technical Details</h2>
<p>For the curious, here's the full scope of changes:</p>
<ul>
<li><strong>2,018 lines</strong> of code across <strong>40 files</strong></li>
<li><strong>13 new API endpoints</strong> for authentication and sync</li>
<li><strong>2 new database tables</strong> with migration scripts</li>
<li><strong>166 lines</strong> of sync service implementation</li>
<li><strong>Test coverage</strong> for all critical paths</li>
</ul>
<p>The complete implementation is open source and available in our
<a href="https://github.com/keypears/keypears">GitHub repository</a>.</p>
<h2>Conclusion</h2>
<p>Building secure cross-device sync for a decentralized password manager required
rethinking traditional approaches. By combining session-based authentication,
privacy-preserving device tracking, and sophisticated key derivation, we've
created a system that's both secure and user-friendly.</p>
<p>The key insight: security doesn't require sacrificing usability or privacy. With
careful architecture and attention to detail, we can build systems that protect
users without compromising their autonomy.</p>
<p>KeyPears now syncs your passwords across all your devices—instantly, securely,
and privately. Whether you're using our hosted service or running your own
server, your secrets stay yours.</p>
<p><em>Next up: Implementing the Diffie-Hellman key exchange protocol for secure
secret sharing between users. Stay tuned!</em></p>]]></content>
        <author>
            <name>KeyPears Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TypeScript for the KeyPears MVP: Why We're Not Really Using Rust (Yet)]]></title>
        <id>https://keypears.com/blog/2025-11-16-typescript-for-mvp</id>
        <link href="https://keypears.com/blog/2025-11-16-typescript-for-mvp"/>
        <updated>2025-11-16T12:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and
cryptocurrency wallet. The design decisions described here represent our
development approach and may evolve b...]]></summary>
        <content type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and
cryptocurrency wallet. The design decisions described here represent our
development approach and may evolve before our official release.</p>
<p>Three weeks ago, we published a blog post titled "Building KeyPears with Rust:
Backend Architecture and Blake3 Proof-of-Concept." We were excited about Rust's
performance, memory safety, and type system. We had a working <code>/api/blake3</code>
endpoint. We had plans for <code>rs-lib</code> and <code>rs-node</code> packages.</p>
<p>Today, we're writing to tell you we've changed direction.</p>
<p>The current KeyPears codebase is <strong>almost entirely TypeScript</strong>. The Rust
backend from our October post—<code>rs-lib</code> for cryptography and <code>rs-node</code> for the
API server—was fully built and working. And then we deleted it. After several
weeks of development with both implementations side by side, we concluded that
TypeScript is the right architecture for our MVP.</p>
<p>This post explains why we made that decision.</p>
<h2>What Actually Happened</h2>
<p>Let's start with the facts. Here's what we did:</p>
<p><strong>October 2025:</strong> Built a complete Rust backend</p>
<ul>
<li><code>rs-lib</code>: Full cryptography library (Blake3, ACB3, key derivation)</li>
<li><code>rs-node</code>: Axum-based API server with OpenAPI via utoipa</li>
<li>Dual-server deployment: Node.js webapp proxying to Rust API</li>
<li>Everything worked as described in the October blog post</li>
</ul>
<p><strong>November 2025:</strong> Removed the entire Rust backend</p>
<ul>
<li>Deleted <code>rs-lib</code> package completely</li>
<li>Deleted <code>rs-node</code> package completely</li>
<li>Rewrote cryptography in TypeScript using <code>@webbuf</code> WASM packages</li>
<li>Rewrote API server in TypeScript using orpc</li>
<li>Integrated API server directly into Express webapp</li>
</ul>
<p><strong>Current state:</strong></p>
<ul>
<li><strong>Rust code:</strong> 33 lines total (just the minimal Tauri shell)</li>
<li><strong>TypeScript code:</strong> ~5,400 lines (lib, api-server, tauri app, webapp)</li>
<li>All cryptography now TypeScript + WASM</li>
<li>All API endpoints now orpc (TypeScript RPC)</li>
<li>Single-server deployment (no more Node → Rust proxy)</li>
</ul>
<p>This wasn't a case of the Rust backend "not working out." It worked perfectly.
We had working Blake3 hashing, working key derivation, working API endpoints. We
deleted it anyway because <strong>TypeScript simplifies development in ways that
matter more than Rust's advantages for our MVP.</strong></p>
<h2>Why We Removed the Rust Backend</h2>
<p>With both implementations working, we had to make a choice: continue maintaining
two parallel implementations (Rust for crypto/API, TypeScript for UI) or
consolidate on one language. We chose TypeScript for three critical reasons:</p>
<p><strong>1. Better API tooling</strong> - orpc provides superior type safety compared to
Axum + utoipa + openapi-generator</p>
<p><strong>2. Better database tooling</strong> - Drizzle ORM supports both SQLite and PostgreSQL
with the same API (no Rust equivalent exists)</p>
<p><strong>3. Single-language simplicity</strong> - Avoiding context switching between Rust and
TypeScript saves mental overhead on a side project</p>
<p>Here's what we learned by building and then removing the Rust backend:</p>
<h3>1. orpc vs Axum + utoipa: Type Safety Without Codegen</h3>
<p>We built the Rust API server with Axum and <code>utoipa</code> for OpenAPI generation. It
worked, but the workflow had friction:</p>
<p><strong>The Rust approach we actually used:</strong></p>
<ol>
<li>Define routes in Rust with Axum</li>
<li>Generate OpenAPI spec with <code>utoipa</code> macros</li>
<li>Run <code>openapi-generator</code> to create TypeScript client</li>
<li>Discover generated client doesn't match our TypeScript patterns</li>
<li>Manually adjust generated code or fix Rust annotations</li>
<li>Repeat on every schema change</li>
</ol>
<p><strong>The TypeScript approach (orpc) we switched to:</strong></p>
<pre><code class="language-typescript">// Define the procedure
export const blake3Procedure = os
  .input(Blake3RequestSchema)
  .output(Blake3ResponseSchema)
  .handler(async ({ input }) => {
    const data = WebBuf.fromBase64(input.data);
    const hash = blake3Hash(data);
    return { hash: hash.buf.toHex() };
  });

// Use it in the client with full type safety
const client = createClient({ url: "/api" });
const result = await client.blake3({ data: "..." });
// TypeScript knows `result.hash` is a string
</code></pre>
<p><strong>Zero codegen. Complete type safety. Instant IDE autocomplete.</strong></p>
<p>The difference is night and day. With orpc, the client knows every endpoint,
every parameter type, every response shape—all inferred directly from the server
code. Change the server? Client errors appear immediately in your IDE, not at
runtime. No build step, no generated files, no version mismatches.</p>
<p>This is what made us delete working Rust code. The Axum + utoipa + codegen
workflow worked, but orpc's zero-codegen type safety is so much better that
maintaining the Rust version wasn't worth it.</p>
<h3>2. No Rust ORM Supports Both SQLite and PostgreSQL Well</h3>
<p>KeyPears needs two databases:</p>
<ul>
<li><strong>SQLite</strong> in the Tauri desktop app (client-side storage)</li>
<li><strong>PostgreSQL</strong> on the server (multi-user vault synchronization)</li>
</ul>
<p>In TypeScript, <strong>Drizzle ORM</strong> handles both with the same API:</p>
<pre><code class="language-typescript">// Client (SQLite)
import { drizzle } from "drizzle-orm/sqlite-proxy";
const db = drizzle(/* Tauri SQL plugin */);

// Server (PostgreSQL)
import { drizzle } from "drizzle-orm/node-postgres";
const db = drizzle(/* pg connection */);

// Same schema definition works for both
export const TableVault = sqliteTable("vault", {
  id: text("id").primaryKey(),
  name: text("name").notNull(),
  // ...
});
</code></pre>
<p>We looked for Rust equivalents. <strong>Diesel</strong> supports Postgres and MySQL but has
poor SQLite support. <strong>SeaORM</strong> is newer but still requires separate schema
definitions for different databases. Neither provides the unified, type-safe
query builder that Drizzle gives us.</p>
<p>When you're building a sync protocol where the client and server need matching
schemas, having one ORM that works everywhere is critical. This was the second
reason we deleted the Rust backend—we would have needed two separate database
implementations (one for Tauri's SQLite, one for the server's Postgres) with
manual work to keep them in sync.</p>
<h3>3. Single Language Reduces Mental Overhead</h3>
<p>The final reason we removed the Rust backend: <strong>context switching costs.</strong></p>
<p>With the dual-language architecture, every feature required:</p>
<ul>
<li>Writing Rust for crypto/API logic</li>
<li>Writing TypeScript for UI/database logic</li>
<li>Translating between Rust and TypeScript idioms</li>
<li>Maintaining two build systems (Cargo + pnpm)</li>
<li>Debugging across language boundaries</li>
<li>Different testing frameworks (Cargo test + Vitest)</li>
</ul>
<p>For a side project where development happens in short evening sessions, this
mental overhead compounds. You spend the first 10 minutes remembering whether
you're writing Rust or TypeScript, and the last 10 minutes before bed context
switching back.</p>
<p>With TypeScript-only:</p>
<ul>
<li>One type system</li>
<li>One package manager</li>
<li>One testing framework</li>
<li>One set of idioms</li>
<li>Hot reload in ~100ms (vs 3-10s Rust recompile)</li>
</ul>
<p>The productivity gain isn't just about compile times. It's about flow state.
When you're not context switching between languages, you write more code and
make fewer mistakes.</p>
<h3>4. Deployment Simplification</h3>
<p>The Rust backend also complicated deployment:</p>
<p><strong>With Rust (October architecture):</strong></p>
<ul>
<li>Dual-server: Node.js webapp (port 4273) + Rust API (port 4274)</li>
<li>HTTP proxy from webapp to Rust server</li>
<li>Docker image: Node + Rust toolchain + cross-compilation</li>
<li>Larger image size (~500MB with Rust)</li>
<li>More complex service coordination</li>
</ul>
<p><strong>With TypeScript-only (current):</strong></p>
<ul>
<li>Single Express server (port 4273)</li>
<li>orpc API mounted directly at <code>/api</code></li>
<li>Docker image: Just Node.js (~200MB)</li>
<li>Simpler deployment (one service, one port)</li>
<li>No HTTP proxy overhead</li>
</ul>
<p>Removing the Rust backend made deployment cleaner and faster.</p>
<h2>What We Didn't Lose: Rust Cryptography via WASM</h2>
<p>Here's the critical insight that made removing the Rust backend viable: <strong>We
still use Rust for cryptography. We just use it through WebAssembly instead of
writing it ourselves.</strong></p>
<p>When we deleted <code>rs-lib</code> (our Rust cryptography library), we didn't rewrite
crypto in pure JavaScript. We switched to the <code>@webbuf</code> packages, which compile
Rust cryptography to WebAssembly:</p>
<ul>
<li><strong><code>@webbuf/blake3</code></strong>: Blake3 hashing (Rust → WASM)</li>
<li><strong><code>@webbuf/acb3</code></strong>: AES-256-CBC + Blake3-MAC (Rust → WASM)</li>
<li><strong><code>@webbuf/webbuf</code></strong>: Binary data utilities (Rust → WASM)</li>
<li><strong><code>@webbuf/fixedbuf</code></strong>: Fixed-size buffers (Rust → WASM)</li>
</ul>
<p>These packages compile Rust cryptography to WebAssembly. We get:</p>
<p>✅ <strong>Rust's memory safety</strong> (WASM sandbox) ✅ <strong>Rust's performance</strong>
(near-native speed) ✅ <strong>Cross-platform consistency</strong> (works in Node, browsers,
Tauri) ✅ <strong>TypeScript ergonomics</strong> (native <code>Uint8Array</code> integration)</p>
<p>Here's our complete three-tier key derivation system in TypeScript:</p>
<pre><code class="language-typescript">// 100,000 rounds of Blake3-based PBKDF
export function blake3Pbkdf(
  password: string | WebBuf,
  salt: FixedBuf&#x3C;32>,
  rounds: number = 100_000,
): FixedBuf&#x3C;32> {
  const passwordBuf =
    typeof password === "string" ? WebBuf.fromUtf8(password) : password;

  let result = blake3Mac(salt, passwordBuf);
  for (let i = 1; i &#x3C; rounds; i++) {
    result = blake3Mac(salt, result.buf);
  }
  return result;
}

// Derive password key from user's master password
export function derivePasswordKey(password: string): FixedBuf&#x3C;32> {
  const salt = derivePasswordSalt(password);
  return blake3Pbkdf(password, salt, 100_000);
}

// Derive encryption key (for vault data)
export function deriveEncryptionKey(passwordKey: FixedBuf&#x3C;32>): FixedBuf&#x3C;32> {
  const salt = deriveEncryptionSalt();
  return blake3Pbkdf(passwordKey.buf, salt, 100_000);
}

// Derive login key (sent to server)
export function deriveLoginKey(passwordKey: FixedBuf&#x3C;32>): FixedBuf&#x3C;32> {
  const salt = deriveLoginSalt();
  return blake3Pbkdf(passwordKey.buf, salt, 100_000);
}
</code></pre>
<p>This is production-ready cryptography. It's type-safe. It's fast (200,000 Blake3
operations complete in milliseconds). And the actual hashing happens in
Rust-compiled WASM—the same Rust cryptography we had in <code>rs-lib</code>, just
packaged differently.</p>
<p><strong>We didn't abandon Rust's security properties. We just stopped maintaining our
own Rust codebase.</strong> The cryptography is still Rust. It's just compiled to WASM
and consumed as TypeScript packages, which eliminates the build complexity of a
dual-language project.</p>
<h2>The Architecture That Emerged</h2>
<p>Here's what the current KeyPears stack looks like:</p>
<h3>Package Structure</h3>
<pre><code>@keypears/lib (TypeScript)
├── Blake3 hashing via @webbuf/blake3 (Rust→WASM)
├── ACB3 encryption via @webbuf/acb3 (Rust→WASM)
├── Three-tier key derivation (100k rounds each)
├── Password generation with entropy calculation
└── Zod schemas for validation

@keypears/api-server (TypeScript)
├── orpc router with type-safe procedures
├── Blake3 endpoint (working proof-of-concept)
├── Drizzle ORM + PostgreSQL schema (ready for server DB)
└── Client factory for end-to-end type safety

keypears-tauri (TypeScript + Rust shell)
├── Tauri 2.0 app (33 lines of Rust)
├── Full vault management UI (~5,020 lines TypeScript)
├── SQLite with Drizzle ORM
├── React Router 7 for navigation
├── Shadcn components + Catppuccin theme
└── Calls production API server for crypto endpoints

@keypears/webapp (TypeScript)
├── Production website + blog
├── Integrated API server (orpc mounted at /api)
├── Single Express server on port 4273
└── Deployed on AWS Fargate
</code></pre>
<h3>What Works Today</h3>
<p>The Tauri app has a complete vault management workflow:</p>
<p>✅ Create vault with password ✅ Unlock vault with password verification ✅
Store passwords with encryption ✅ Generate secure passwords ✅ SQLite
persistence via Drizzle ✅ Three-tier key derivation working ✅ Vault encryption
with ACB3 ✅ Multi-step wizards (name → password → confirm → success) ✅ Test
page calling production Blake3 API</p>
<p>The webapp has:</p>
<p>✅ Landing page with blog system ✅ Working <code>/api/blake3</code> endpoint ✅ orpc
integrated with Express ✅ Docker deployment to AWS Fargate ✅ Canonical URL
redirects ✅ Blog posts with TOML frontmatter + Markdown</p>
<h3>What's Not Built (Intentionally Deferred)</h3>
<p>We haven't built server-side features yet because the MVP is <strong>local-first</strong>:</p>
<ul>
<li>⏸️ User authentication (login/logout)</li>
<li>⏸️ Vault synchronization protocol</li>
<li>⏸️ Multi-user server support</li>
<li>⏸️ Diffie-Hellman key exchange across domains</li>
<li>⏸️ Public key infrastructure</li>
</ul>
<p>These are v2 features. The MVP is a password manager that works 100% offline in
the Tauri app. The server is only needed for multi-device sync, which we'll add
after validating the core product.</p>
<h2>The TypeScript Ecosystem Has Caught Up</h2>
<p>Five years ago, this blog post would have been different. Rust was the only way
to get type-safe backends with good performance. But the TypeScript ecosystem
has evolved dramatically:</p>
<p><strong>orpc</strong> gives us end-to-end type safety that Rust can't match (no codegen,
instant IDE feedback)</p>
<p><strong>Drizzle</strong> provides type-safe SQL for both SQLite and PostgreSQL (no Rust ORM
does this well)</p>
<p><strong>WASM</strong> lets us use Rust crypto without writing Rust applications (best of both
worlds)</p>
<p><strong>Vitest</strong> gives us fast ESM-native testing (simpler than Cargo's test framework
for web apps)</p>
<p><strong>React Router 7</strong> provides SSR + type-safe routing (no Rust equivalent)</p>
<p>For building web applications with cryptography, TypeScript + WASM is now a
better choice than native Rust. You get comparable performance, better tooling,
and a much larger ecosystem of web-focused libraries.</p>
<h2>When Would We Use Rust?</h2>
<p>This isn't a rejection of Rust. It's a recognition that <strong>Rust solves the wrong
problems for our MVP.</strong></p>
<p>Rust makes sense when you need:</p>
<ol>
<li><strong>Extreme performance</strong> - Handling 10k+ concurrent WebSocket connections</li>
<li><strong>Embedded systems</strong> - Running on IoT devices with 64MB of RAM</li>
<li><strong>Custom crypto</strong> - Implementing novel cryptographic algorithms</li>
<li><strong>Kernel-level code</strong> - Writing device drivers or OS components</li>
</ol>
<p>KeyPears doesn't need any of these yet. Our server will handle dozens of
concurrent users, not thousands. Our desktop app runs on modern laptops with
gigabytes of RAM. Our cryptography comes from well-tested libraries (Blake3,
AES-256). We're building a user-facing application, not infrastructure.</p>
<p><strong>Later, Rust might make sense for:</strong></p>
<ul>
<li>High-throughput sync server (if we grow to enterprise scale)</li>
<li>Mobile performance optimization (if WASM proves too slow)</li>
<li>Custom Diffie-Hellman implementation (if existing libraries don't fit)</li>
</ul>
<p>But even then, we'd keep the API layer in TypeScript (orpc is too good to give
up) and only move performance-critical sync logic to Rust via FFI.</p>
<h2>The Right Tool for the Right Job</h2>
<p>Software architecture isn't about using the "best" language—it's about using the
right tool for the constraints you're facing.</p>
<p>Our constraints:</p>
<ul>
<li><strong>Side project timeline</strong>: Limited evening/weekend hours</li>
<li><strong>Solo developer</strong>: No team to split Rust vs TypeScript work</li>
<li><strong>MVP goal</strong>: Prove the concept before scaling</li>
<li><strong>Rapid iteration</strong>: Features change based on user feedback</li>
</ul>
<p>For these constraints, TypeScript is objectively better:</p>
<ul>
<li>Faster iteration (100ms hot reload vs 5s compile)</li>
<li>Single mental model (no context switching)</li>
<li>Richer ecosystem (orpc, Drizzle, React Router)</li>
<li>Lower cognitive overhead (one type system, one package manager)</li>
</ul>
<p>We still get Rust's security properties through WASM. We still get type safety
through TypeScript. We still get performance (crypto is WASM, API is fast
enough).</p>
<h2>What We Learned</h2>
<p><strong>1. Working code isn't always the right code</strong></p>
<p>The Rust backend worked perfectly. Blake3 hashing worked. Key derivation worked.
The API server worked. We shipped it to production. But "working" doesn't mean
"optimal for the constraints." When we evaluated developer experience vs
performance gains, TypeScript won decisively for our MVP.</p>
<p><strong>2. Ecosystem maturity matters more than language performance</strong></p>
<p>The Rust language is excellent. But for web applications, the TypeScript
ecosystem is years ahead. orpc's zero-codegen type safety is revolutionary.
Drizzle's unified SQLite + Postgres support is essential for our architecture.
These don't exist in Rust.</p>
<p><strong>3. WASM changes the game</strong></p>
<p>Ten years ago, you had to choose: safe languages (Ruby, Python, JavaScript) or
fast languages (C, C++, Rust). Today, you can write your performance-critical
code in Rust, compile it to WASM, and use it from any language. This is what
made deleting our Rust backend viable—we didn't lose Rust's performance, we just
stopped writing it ourselves.</p>
<p><strong>4. Deleting working code is liberating</strong></p>
<p>We spent weeks building <code>rs-lib</code> and <code>rs-node</code>. They worked. They were deployed.
And we deleted them anyway because the TypeScript alternative was better for our
constraints. This felt wrong at first—"But we already built it!"—but the
productivity gain from consolidating on one language was immediate and
substantial.</p>
<p><strong>5. Side project constraints are different</strong></p>
<p>If KeyPears were a VC-funded startup with a team of 5 engineers, we'd keep the
Rust backend. Someone could own the Rust API while others work on the TypeScript
UI. But for a solo side project with limited evening/weekend hours, the mental
overhead of context switching between Rust and TypeScript was too high. One
language means more velocity.</p>
<h2>The Current Priority: Shipping the MVP</h2>
<p>With this architecture decision settled, we're focused on shipping a working
product:</p>
<p><strong>Next milestones:</strong></p>
<ol>
<li><strong>Server vault CRUD</strong> - Create/read/update vaults via API</li>
<li><strong>User authentication</strong> - Session-based login with hashed login key</li>
<li><strong>Basic sync protocol</strong> - Last-write-wins synchronization</li>
<li><strong>Mobile Tauri build</strong> - iOS + Android apps</li>
<li><strong>Import/export</strong> - Backup and restore vaults</li>
</ol>
<p>All of this will be TypeScript. The API server will use orpc. The database will
use Drizzle (Postgres on server, SQLite on clients). The cryptography will
remain Rust-compiled WASM.</p>
<p>And if we're wrong—if we hit performance walls or need Rust for specific
features—we can always add Rust modules later. The architecture supports it. But
we're not starting there.</p>
<h2>Try It Yourself</h2>
<p>The Blake3 endpoint is live:</p>
<pre><code class="language-bash">curl -X POST https://keypears.com/api/blake3 \
  -H "Content-Type: application/json" \
  -d '{"data": "SGVsbG8sIEtleVBlYXJzIQ=="}'
</code></pre>
<p>That <code>data</code> field is base64-encoded "Hello, KeyPears!". The API will return the
Blake3 hash computed by Rust (via WASM) running in Node.js on our TypeScript
server.</p>
<p>It's a small proof-of-concept, but it validates the entire architecture:
TypeScript for the API layer, Rust-via-WASM for cryptography, type safety
end-to-end.</p>
<h2>Conclusion</h2>
<p>We built a Rust backend. It worked. We deployed it. And then we deleted it.</p>
<p>This wasn't a failure of Rust or a mistake in architecture. It was a deliberate
choice to optimize for <strong>developer velocity over theoretical performance</strong> at
this stage of the project. The Rust backend would have been fine for production,
but the TypeScript backend is better for rapid MVP development.</p>
<p>We're building KeyPears with <strong>TypeScript + WASM</strong>, which gives us Rust's
security properties (via WASM crypto) without the complexity of maintaining a
dual-language codebase.</p>
<p>For a solo side project with MVP goals, this is the right architecture. If we
scale to millions of users and need extreme performance, we can always bring
Rust back for specific hot paths. But we're not starting there.</p>
<p>Rust is an incredible language. We proved that by building a working backend
with it. But for this project, at this stage, TypeScript is the pragmatic
choice—and we're comfortable deleting working Rust code to prove it.</p>
<p>We'll keep sharing our progress—both the wins and the pivots. If you're
interested in following along:</p>
<ul>
<li><strong>Live demo</strong>: Try the Blake3 endpoint at https://keypears.com/api/blake3</li>
<li><strong>Source code</strong>: Coming soon on GitHub under Apache 2.0 license</li>
</ul>
<p>More updates coming soon. Next post: Implementing the vault synchronization
protocol.</p>]]></content>
        <author>
            <name>KeyPears Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building KeyPears with Rust: Backend Architecture and Blake3 Proof-of-Concept]]></title>
        <id>https://keypears.com/blog/2025-10-25-rust-backend-architecture</id>
        <link href="https://keypears.com/blog/2025-10-25-rust-backend-architecture"/>
        <updated>2025-10-25T11:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and cryptocurrency wallet. The design decisions described here represent our development approach and may evolve b...]]></summary>
        <content type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and cryptocurrency wallet. The design decisions described here represent our development approach and may evolve before our official release.</p>
<p>We're excited to share a major architectural milestone: KeyPears now has a working Rust backend with our first proof-of-concept endpoint. This marks a significant shift in our technical approach, bringing the performance, security, and cross-platform benefits of Rust to our core cryptography and API layer.</p>
<h2>Why Rust for the Backend?</h2>
<p>When we started building KeyPears, we knew cryptography and security would be central to everything we do. After evaluating different approaches, we chose to build our backend entirely in Rust for several compelling reasons:</p>
<h3>Performance</h3>
<p>Cryptographic operations—hashing, encryption, key derivation—are CPU-intensive. Rust's zero-cost abstractions and lack of garbage collection mean we can achieve performance comparable to C/C++ without sacrificing safety. For operations users will perform thousands of times (encrypting secrets, computing hashes, deriving keys), this performance matters.</p>
<h3>Memory Safety</h3>
<p>Password managers and cryptocurrency wallets are high-value targets for attackers. Rust's ownership system and borrow checker eliminate entire classes of vulnerabilities at compile time:</p>
<ul>
<li>No buffer overflows</li>
<li>No use-after-free bugs</li>
<li>No data races in concurrent code</li>
<li>No null pointer dereferences</li>
</ul>
<p>These guarantees mean our cryptographic code has fewer attack surfaces by design.</p>
<h3>Cross-Platform Consistency</h3>
<p>KeyPears needs to run everywhere: Windows, macOS, Linux, Android, and iOS. Rust compiles to native code on all these platforms with consistent behavior. The same cryptographic library (<code>rs-lib</code>) that powers our server also powers our Tauri desktop app and will eventually power our mobile apps.</p>
<p>This eliminates the "works on my machine" problem and ensures that a secret encrypted on iOS can be decrypted on Windows with identical cryptographic operations.</p>
<h3>Strong Type System</h3>
<p>Rust's type system helps us encode security invariants at compile time. For example, we can use the type system to ensure that:</p>
<ul>
<li>Encryption keys are never accidentally logged or serialized</li>
<li>Sensitive data is properly zeroed after use</li>
<li>API responses match their OpenAPI specifications exactly</li>
</ul>
<p>This compile-time verification catches bugs before they reach production.</p>
<h2>Architecture Overview</h2>
<p>Our Rust backend consists of two main packages:</p>
<h3><code>rs-lib</code>: Core Cryptography Library</h3>
<p><code>rs-lib</code> is a shared Rust library containing all our cryptographic implementations:</p>
<ul>
<li><strong>Blake3</strong>: Fast, secure hashing and key derivation</li>
<li><strong>ACB3</strong>: AES-256-CBC + Blake3-MAC for authenticated encryption</li>
<li><strong>Key derivation</strong>: Three-tier system separating authentication from encryption</li>
<li><strong>Data structures</strong>: Core types for vaults, secrets, and synchronization</li>
</ul>
<p>This library is pure Rust with no external dependencies beyond well-audited cryptography crates. It's designed to be portable and reusable across all our platforms.</p>
<h3><code>rs-node</code>: KeyPears Node (API Server)</h3>
<p><code>rs-node</code> is our API server—what we call a "KeyPears node." It uses the Axum web framework to expose REST endpoints that clients can use for cryptographic operations and vault synchronization.</p>
<p>Key features:</p>
<ul>
<li><strong>Axum framework</strong>: Modern, type-safe HTTP server from the Tokio team</li>
<li><strong>OpenAPI 3.0</strong>: Full API specification generated from Rust code using <code>utoipa</code></li>
<li><strong>Swagger UI</strong>: Interactive API documentation at <code>/api/docs</code></li>
<li><strong>Type safety</strong>: Request/response types validated at compile time</li>
</ul>
<p>The node is designed to be self-hostable. Anyone can run their own KeyPears node for full sovereignty over their data.</p>
<h2>Blake3 Proof-of-Concept</h2>
<p>Our first working endpoint is a Blake3 hashing service at <code>/api/blake3</code>. You can try it right now:</p>
<pre><code class="language-bash">curl -X POST https://keypears.com/api/blake3 \
  -H "Content-Type: application/json" \
  -d '{"data": "Hello, KeyPears!"}'
</code></pre>
<p>This returns:</p>
<pre><code class="language-json">{
  "hash": "a1b2c3d4..."
}
</code></pre>
<p>Blake3 is our hashing algorithm of choice for KeyPears. It's:</p>
<ul>
<li><strong>Fast</strong>: Significantly faster than SHA-256 or SHA-3</li>
<li><strong>Secure</strong>: 256-bit security with no known attacks</li>
<li><strong>Versatile</strong>: Works as both a hash function and a key derivation function</li>
<li><strong>Modern</strong>: Designed in 2020 with modern CPU features in mind</li>
</ul>
<p>We use Blake3 throughout KeyPears:</p>
<ul>
<li>Deriving encryption keys from passwords</li>
<li>Generating message authentication codes (MACs)</li>
<li>Computing content hashes for deduplication</li>
<li>Creating deterministic IDs</li>
</ul>
<p>This proof-of-concept demonstrates the full stack working:</p>
<ol>
<li>Rust backend (<code>rs-node</code>) receives the request</li>
<li>Rust library (<code>rs-lib</code>) performs the Blake3 hash</li>
<li>Result is serialized and returned via Axum</li>
<li>OpenAPI documentation describes the endpoint</li>
<li>Node.js webapp proxies <code>/api/*</code> requests to the Rust node</li>
</ol>
<h2>TypeScript Frontend + Rust Backend</h2>
<p>While our backend is Rust, our frontend remains TypeScript. This gives us the best of both worlds:</p>
<ul>
<li><strong>Rust</strong>: Performance and security for cryptography and core logic</li>
<li><strong>TypeScript</strong>: Rapid development and rich ecosystem for UI</li>
</ul>
<p>Our architecture uses:</p>
<ul>
<li><strong>Tauri</strong>: Native desktop apps with Rust backend + web frontend</li>
<li><strong>React Router</strong>: Type-safe routing for web and desktop apps</li>
<li><strong>shadcn</strong>: UI components with Catppuccin theme</li>
<li><strong>Type-safe API client</strong>: Generated from OpenAPI spec for compile-time safety</li>
</ul>
<p>The Tauri app embeds the same <code>rs-lib</code> cryptography that powers the KeyPears node. This means the desktop app has full offline capability—it doesn't need a server for cryptographic operations. The server is only needed for synchronization across devices.</p>
<h2>Deployment Architecture</h2>
<p>In production, we run a dual-server setup:</p>
<ol>
<li><strong>KeyPears node (Rust)</strong>: Runs on port 4274, handles API requests</li>
<li><strong>Webapp server (Node.js)</strong>: Runs on port 4273, serves the landing page and proxies API requests</li>
</ol>
<p>The Node.js server forwards all <code>/api/*</code> requests to the Rust node via <code>http-proxy-middleware</code>. This gives us:</p>
<ul>
<li>Single-domain simplicity (no CORS issues)</li>
<li>Independent scaling of API and web traffic</li>
<li>Clean separation of concerns</li>
</ul>
<p>Both services run in a single Docker container on AWS Fargate, deployed via ECS.</p>
<h2>Interactive API Documentation</h2>
<p>One of the benefits of Rust's <code>utoipa</code> library is automatic OpenAPI documentation generation. You can explore our API interactively at:</p>
<p><strong>https://keypears.com/api/docs</strong></p>
<p>This Swagger UI is generated directly from our Rust code. Every endpoint, request type, and response type is documented with examples. As we add new endpoints, the documentation updates automatically.</p>
<h2>What's Next?</h2>
<p>The Blake3 endpoint is just the beginning. We're actively building:</p>
<h3>Vault Operations</h3>
<ul>
<li>Create and encrypt vaults</li>
<li>Derive encryption keys from passwords</li>
<li>Store and retrieve encrypted secrets</li>
</ul>
<h3>Synchronization Protocol</h3>
<ul>
<li>Append-only logs for conflict-free sync</li>
<li>Server-side coordination for multi-device sync</li>
<li>End-to-end encryption (servers never see plaintext)</li>
</ul>
<h3>Diffie-Hellman Key Exchange</h3>
<ul>
<li>Peer-to-peer secret sharing across domains</li>
<li>Email-style addressing (<code>alice@example.com</code> ↔ <code>bob@example2.com</code>)</li>
<li>Public key discovery via federated nodes</li>
</ul>
<h3>Cross-Platform Clients</h3>
<ul>
<li>Desktop apps (Windows, macOS, Linux) via Tauri</li>
<li>Mobile apps (Android, iOS) - coming soon</li>
<li>Web interface for emergency access</li>
</ul>
<p>All of these features will be built on the same foundation: Rust for security-critical operations, TypeScript for user interfaces.</p>
<h2>Open Source and Self-Hostable</h2>
<p>Everything we're building is open source under Apache 2.0. You can:</p>
<ul>
<li>Review the code for security</li>
<li>Run your own KeyPears node</li>
<li>Contribute improvements</li>
<li>Build custom clients</li>
</ul>
<p>The KeyPears node is designed to be self-hostable. Our deployment documentation walks through:</p>
<ul>
<li>Cross-compiling for Linux (even from macOS)</li>
<li>Docker containerization</li>
<li>AWS Fargate deployment</li>
<li>Domain configuration and SSL</li>
</ul>
<p>We want KeyPears to be decentralized by default. Anyone should be able to run a node, just like anyone can run an email server.</p>
<h2>Conclusion</h2>
<p>Building KeyPears with Rust has been an excellent decision. The language's emphasis on safety, performance, and correctness aligns perfectly with our security requirements. The Blake3 proof-of-concept validates our architecture: Rust backend for cryptography, TypeScript frontend for user experience, and a clean API boundary between them.</p>
<p>We're excited to continue building. If you're interested in following along, check out:</p>
<ul>
<li><strong>Live demo</strong>: Try the Blake3 endpoint at https://keypears.com/api/blake3</li>
<li><strong>API docs</strong>: Explore the OpenAPI spec at https://keypears.com/api/docs</li>
<li><strong>Source code</strong>: Coming soon on GitHub</li>
</ul>
<p>We'll continue sharing our progress through these blog posts. Next up: vault encryption and key derivation in Rust.</p>]]></content>
        <author>
            <name>KeyPears Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progress on Secret Synchronization: A Future-Proof Schema]]></title>
        <id>https://keypears.com/blog/2025-10-07-progress-on-secret-synchronization</id>
        <link href="https://keypears.com/blog/2025-10-07-progress-on-secret-synchronization"/>
        <updated>2025-10-07T11:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The
design decisions described here represent our development approach and may
evolve before our official release...]]></summary>
        <content type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The
design decisions described here represent our development approach and may
evolve before our official release.</p>
<p>We've made significant progress on KeyPears' secret synchronization
architecture. Today we're sharing how we redesigned our schema to support
diverse secret types while maintaining the small-sync-unit principle that makes
our synchronization protocol efficient and reliable.</p>
<h2>The Problem</h2>
<p>Our original schema was built around passwords. It had fields like
<code>encryptedPassword</code>, <code>username</code>, <code>domain</code>, and <code>notes</code>. This worked fine for
basic password management, but it created limitations:</p>
<ul>
<li><strong>Type inflexibility</strong>: How do you store an API key? A cryptocurrency wallet
with multiple components? Environment variables?</li>
<li><strong>No grouping</strong>: Secrets existed in isolation. There was no way to represent
"this API token belongs to this account" or "these 20 environment variables
form one .env file"</li>
<li><strong>No hierarchy</strong>: No folders, no organization beyond a flat list</li>
<li><strong>KeePass import impossible</strong>: KeePass has groups (folders) and custom fields
(additional key-value pairs per entry). We couldn't represent either.</li>
</ul>
<p>We needed a more flexible schema without abandoning our core architectural
principle: <strong>every secret must sync independently</strong> to keep network overhead
small and conflict resolution simple.</p>
<h2>The Solution: Three Changes</h2>
<p>We evolved the <code>SecretUpdate</code> schema with three key additions: multi-type
support, dual hierarchy mechanisms, and JSON-based storage.</p>
<h3>1. Multi-Type Support</h3>
<p>First, we made the schema generic enough to handle any small secret:</p>
<pre><code class="language-typescript">type: "password" | "envvar" | "apikey" | "walletkey" | "passkey";
encryptedData: string; // Previously: encryptedPassword
encryptedNotes: string; // Previously: notes
</code></pre>
<p>The <code>type</code> field distinguishes what kind of secret this is. The generic
<code>encryptedData</code> field holds the actual secret value (password, API key, private
key, etc.). Password-specific fields like <code>domain</code>, <code>username</code>, and <code>email</code>
remain in the schema but are optional—used primarily when <code>type</code> is <code>password</code>.</p>
<p>This small change opens up KeyPears to handle:</p>
<ul>
<li><strong>Environment variables</strong>: Type <code>envvar</code>, name <code>DATABASE_URL</code>, encrypted value
in <code>encryptedData</code></li>
<li><strong>API keys</strong>: Type <code>apikey</code>, service name in a <code>label</code> field, key in
<code>encryptedData</code></li>
<li><strong>Wallet keys</strong>: Type <code>walletkey</code>, blockchain type in metadata, private key in
<code>encryptedData</code></li>
<li><strong>Passkeys</strong>: Type <code>passkey</code>, credential ID and public key in metadata,
private key in <code>encryptedData</code></li>
</ul>
<h3>2. Dual Hierarchy: Folders and ParentId</h3>
<p>The second change introduces two different hierarchy mechanisms, each serving a
specific purpose:</p>
<pre><code class="language-typescript">folders: string[]      // ["Work", "Projects", "Client A"]
tags: string[]         // ["production", "critical"]
parentId: string       // ULID of parent secret (max depth 1)
</code></pre>
<p><strong>Folders</strong> provide unlimited-depth organizational hierarchy. They're just an
array of strings representing the path:</p>
<pre><code class="language-typescript">folders: ["Work", "AWS", "Production"];
folders: ["Personal", "Banking"];
folders: []; // Root level
</code></pre>
<p>This maps perfectly to KeePass Groups and lets users organize thousands of
secrets into a familiar folder structure.</p>
<p><strong>Tags</strong> provide orthogonal categorization. A secret can have multiple tags for
cross-cutting concerns:</p>
<pre><code class="language-typescript">tags: ["production-env", "requires-rotation", "shared-with-team"];
</code></pre>
<p><strong>ParentId</strong> creates actual parent-child relationships between secrets. This is
where it gets interesting.</p>
<h2>ParentId: Secrets Containing Secrets</h2>
<p>The <code>parentId</code> field lets one secret "contain" other secrets. A simple example:</p>
<pre><code class="language-typescript">// Parent: The main account
{
  secretId: "abc123",
  name: "GitHub Account",
  type: "password",
  encryptedData: "&#x3C;main password>"
}

// Child: API token for the same account
{
  secretId: "def456",
  name: "API Token",
  type: "apikey",
  parentId: "abc123",
  encryptedData: "&#x3C;token>"
}
</code></pre>
<p>This models KeePass's custom fields—additional key-value pairs that belong to an
entry. In KeePass, you might have a GitHub entry with standard fields (username,
password, URL) plus custom fields for API tokens, 2FA backup codes, or recovery
emails.</p>
<p>In KeyPears, each custom field becomes its own secret with a <code>parentId</code> pointing
to the parent. Each syncs independently (small sync units!), but they're
logically grouped.</p>
<h3>The Depth Limit: Security Through Simplicity</h3>
<p>Here's the critical constraint: <strong>a secret can have a parent, but that parent
cannot have a parent</strong>. Maximum depth is 1. No grandparents allowed.</p>
<p>Why? Three reasons:</p>
<p><strong>1. Security</strong>: Client-generated IDs open an attack vector for malicious
clients creating circular references or extremely deep chains. With depth=1, the
validation is trivial:</p>
<pre><code class="language-typescript">async function validateParentChain(secretId: string, parentId?: string) {
  if (!parentId) return; // No parent, valid
  if (parentId === secretId) throw new Error("Cannot self-reference");

  const parent = await getSecretHistory(parentId);
  if (parent.length > 0 &#x26;&#x26; parent[0].parentId) {
    throw new Error("Cannot nest more than one level deep");
  }
}
</code></pre>
<p>One database lookup. No recursion. No visited sets. O(1) validation that
attackers can't exploit.</p>
<p><strong>2. Performance</strong>: Validating unlimited depth requires recursive queries.
Validating depth=1 requires one query. Simple.</p>
<p><strong>3. Sufficient for real use cases</strong>:</p>
<ul>
<li>Folder with secrets ✓</li>
<li>Entry with custom fields ✓</li>
<li>Environment variable group ✓</li>
</ul>
<p>What we lose: deeply nested folder hierarchies via <code>parentId</code>. But we have
<code>folders</code> for that! The two mechanisms complement each other perfectly.</p>
<h2>Why Two Hierarchy Systems?</h2>
<p>It might seem redundant to have both <code>folders</code> and <code>parentId</code>, but they serve
different purposes:</p>
<p><strong>Folders</strong> are for <strong>organizational hierarchy</strong>. They map to KeePass Groups.
They're pure metadata—just strings representing a path. They have unlimited
depth because they're just labels, not database relationships.</p>
<p><strong>ParentId</strong> is for <strong>data relationships</strong>. It maps to KeePass custom fields. It
creates actual parent-child relationships where one secret logically contains
others. Each child syncs independently, maintaining small sync units.</p>
<p>Together, they enable full KeePass import:</p>
<pre><code class="language-typescript">// KeePass structure:
// Work/Projects/GitHub (Group path)
//   - GitHub Account (Entry)
//     - Username: alice
//     - Password: ••••••
//     - Custom: API Token (protected)
//     - Custom: 2FA Codes (protected)

// KeyPears representation:
{
  secretId: "main",
  name: "GitHub Account",
  type: "password",
  folders: ["Work", "Projects", "GitHub"],
  username: "alice",
  encryptedData: "&#x3C;password>"
}
{
  secretId: "token",
  name: "API Token",
  type: "apikey",
  folders: ["Work", "Projects", "GitHub"], // Inherits folder
  parentId: "main",
  encryptedData: "&#x3C;token>"
}
{
  secretId: "codes",
  name: "2FA Codes",
  type: "password",
  folders: ["Work", "Projects", "GitHub"],
  parentId: "main",
  encryptedData: "&#x3C;codes>"
}
</code></pre>
<p>The folder path provides organization. The <code>parentId</code> relationships show which
secrets belong together. Each secret syncs independently.</p>
<h2>JSON-Based Storage: Migration-Proof Architecture</h2>
<p>The third major change is how we store secrets in the database. We moved to a
hybrid approach:</p>
<pre><code class="language-sql">CREATE TABLE secret_update (
  id TEXT PRIMARY KEY,
  vault_id TEXT NOT NULL,
  secret_id TEXT NOT NULL,
  name TEXT NOT NULL,
  type TEXT NOT NULL DEFAULT 'password',
  parent_id TEXT,
  created_at INTEGER NOT NULL,
  deleted INTEGER NOT NULL DEFAULT 0,

  -- Source of truth: full JSON object
  secret_update_json TEXT NOT NULL
);

CREATE INDEX idx_secret_updates_name ON secret_update(name);
CREATE INDEX idx_secret_updates_type ON secret_update(type);
CREATE INDEX idx_secret_updates_parent_id ON secret_update(parent_id);
</code></pre>
<p>Notice what's happening here. We store the <strong>entire <code>SecretUpdate</code> object</strong> as
JSON in <code>secret_update_json</code>. The other columns (<code>name</code>, <code>type</code>, <code>parent_id</code>,
etc.) are duplicates of data from the JSON, extracted for indexing.</p>
<p>The JSON is the source of truth. The columns are for performance.</p>
<h3>Why This Approach?</h3>
<p><strong>Adding fields requires no migration</strong>. Want to add a <code>label</code> field? Update the
Zod schema, start writing it to the JSON, and you're done. The database doesn't
care—it's just storing JSON.</p>
<p>When we added <code>parentId</code> to the schema, we:</p>
<ol>
<li>Updated the Zod schema in TypeScript</li>
<li>Added <code>parent_id</code> column to the database (for indexing)</li>
<li>Started serializing <code>parentId</code> to the JSON</li>
</ol>
<p>Users with existing vaults see <code>parentId: undefined</code> in their JSON. No
migration, no data transformation. Just works.</p>
<p>This architecture is <strong>future-proof</strong>. We can evolve the schema rapidly during
development without worrying about breaking existing databases.</p>
<h3>When We Ship v1.0</h3>
<p>Before our first production release, we'll generate one clean migration from the
final schema. That becomes our baseline. After that, we'll only add new
migrations—never delete old ones—because users will have the old migrations
applied.</p>
<p>But during development? We delete and regenerate migrations freely. The JSON
storage strategy makes this painless.</p>
<h2>What This Enables</h2>
<p>With these changes in place, KeyPears can now handle:</p>
<h3>KeePass Import (Future Feature)</h3>
<p>Full KeePass <code>.kdbx</code> import support, including:</p>
<ul>
<li>Nested groups → <code>folders</code> array</li>
<li>Entries → secrets with <code>type: "password"</code></li>
<li>Custom protected fields → child secrets with <code>parentId</code></li>
<li>Entry metadata → password-specific fields</li>
</ul>
<p>The only thing we won't import: file attachments. By design. We're optimizing
for small secrets that sync efficiently.</p>
<h3>Environment Variables</h3>
<p>Create a parent secret "Production Environment" and attach child secrets for
each variable:</p>
<pre><code class="language-typescript">{ name: "Production Env", type: "folder" }  // Parent
{ name: "DATABASE_URL", type: "envvar", parentId: "..." }
{ name: "API_SECRET", type: "envvar", parentId: "..." }
{ name: "STRIPE_KEY", type: "envvar", parentId: "..." }
</code></pre>
<p>Or use tags instead:</p>
<pre><code class="language-typescript">{ name: "DATABASE_URL", type: "envvar", tags: ["prod-env"] }
{ name: "API_SECRET", type: "envvar", tags: ["prod-env"] }
</code></pre>
<p>Both approaches work. <code>parentId</code> creates explicit grouping. Tags create implicit
sets.</p>
<h3>Cryptocurrency Wallets</h3>
<p>Store wallet keys with relevant metadata:</p>
<pre><code class="language-typescript">{
  name: "Ethereum Main Wallet",
  type: "walletkey",
  encryptedData: "&#x3C;private key>",
  folders: ["Crypto", "Ethereum"],
  tags: ["high-value", "cold-storage"]
}
</code></pre>
<h3>API Keys with Secrets</h3>
<p>Store API key pairs as parent-child:</p>
<pre><code class="language-typescript">{ name: "Stripe", type: "apikey", encryptedData: "&#x3C;public key>" }
{ name: "Secret Key", type: "apikey", parentId: "...", encryptedData: "&#x3C;secret>" }
</code></pre>
<h2>Synchronization Properties</h2>
<p>These changes maintain our core synchronization principles:</p>
<p><strong>Small sync units</strong>: Each secret syncs independently. A 50-entry KeePass import
becomes 50 individual secrets, each a few hundred bytes. If two users edit
different entries, no conflicts.</p>
<p><strong>Atomic updates</strong>: Each <code>SecretUpdate</code> is immutable once created. Updates
create new records in an append-only log. The latest update wins
(last-write-wins conflict resolution).</p>
<p><strong>Efficient</strong>: Only changed secrets sync. If you update one child secret, you
sync one small object, not the entire parent-child group.</p>
<p><strong>Validated</strong>: The <code>parentId</code> depth limit prevents malicious clients from
creating expensive recursive structures.</p>
<h2>Looking Ahead</h2>
<p>This schema evolution lays the groundwork for several future features:</p>
<ul>
<li><strong>UI for child secrets</strong>: Show "API Token" nested under "GitHub Account" in
the secret list</li>
<li><strong>KeePass import</strong>: Full <code>.kdbx</code> file import with groups and custom fields</li>
<li><strong>Environment variable templates</strong>: Quick creation of common .env structures</li>
<li><strong>Bulk operations</strong>: Delete/restore an entire group by operating on all
children</li>
</ul>
<p>The foundation is solid. The architecture is flexible. The sync protocol remains
simple and efficient.</p>
<p>We're building KeyPears to be more than a password manager—it's a secure,
self-custodied secret manager that handles everything from passwords to
environment variables to cryptocurrency keys. And it all syncs seamlessly across
your devices without trusting a central authority with your encryption keys.</p>
<h2>Technical Details</h2>
<p>For those interested in the implementation:</p>
<ul>
<li><strong>Schema definition</strong>: Zod validation in TypeScript, ensuring type safety</li>
<li><strong>Database</strong>: SQLite via Drizzle ORM with the sqlite-proxy adapter</li>
<li><strong>Migration</strong>: Custom migration runner that tracks applied migrations</li>
<li><strong>Validation</strong>: O(1) parent chain validation with single database lookup</li>
<li><strong>Indexes</strong>: <code>name</code>, <code>type</code>, <code>parent_id</code>, and composite
<code>vault_id + secret_id + created_at</code></li>
</ul>
<p>The code is Apache 2.0 licensed and available on GitHub. We're building in the
open, one commit at a time.</p>
<p>More updates coming soon.</p>]]></content>
        <author>
            <name>KeyPears Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How KeyPears Protects Your Vault: Encryption and Key Derivation]]></title>
        <id>https://keypears.com/blog/2025-10-05-vault-encryption-key-derivation</id>
        <link href="https://keypears.com/blog/2025-10-05-vault-encryption-key-derivation"/>
        <updated>2025-10-05T11:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>One of the core security features of KeyPears is how we protect your vault
encryption keys. Today, we're diving deep into our three-tier key derivation
system and explaining how we ensure that even...]]></summary>
        <content type="html"><![CDATA[<p>One of the core security features of KeyPears is how we protect your vault
encryption keys. Today, we're diving deep into our three-tier key derivation
system and explaining how we ensure that even if a server is compromised, your
encrypted data remains secure.</p>
<h2>The Problem: Authentication vs. Encryption</h2>
<p>Most password managers face a fundamental challenge: you need to send
<em>something</em> to the server to prove who you are, but you also need to keep your
encryption key secret so the server can't decrypt your vault. Using the same key
for both purposes creates a security vulnerability—if the server is compromised,
an attacker gains access to your encryption key.</p>
<p>KeyPears solves this by deriving two separate keys from your password: one for
logging in to the server, and one for encrypting your vault. The server only
ever sees the login key, never the encryption key.</p>
<h2>Three-Tier Key Derivation</h2>
<p>When you create a vault in KeyPears, we don't just hash your password once.
Instead, we use a three-tier key derivation system:</p>
<pre><code>Master Password
  ↓ blake3Pbkdf (100,000 rounds)
Password Key (stored encrypted with PIN on device)
  ↓
  ├→ blake3Pbkdf (100,000 rounds) → Encryption Key
  └→ blake3Pbkdf (100,000 rounds) → Login Key
</code></pre>
<h3>1. Password Key: The Root of Trust</h3>
<p>The first step derives a <strong>password key</strong> from your master password using
100,000 rounds of our Blake3-based PBKDF. This intermediate key is stored on
your device, encrypted with your PIN for quick unlock. It never leaves your
device and is never sent to any server.</p>
<p>The password key acts as the root of trust for deriving the other two keys.</p>
<h3>2. Encryption Key: Protecting Your Vault</h3>
<p>From the password key, we derive an <strong>encryption key</strong> through another 100,000
rounds of Blake3 PBKDF. This key is used for one purpose only: encrypting and
decrypting your master vault key.</p>
<p>Wait—encrypting a key with another key? Yes! Your vault itself is encrypted with
a randomly generated <strong>master vault key</strong>. This master key is immutable and
never changes. The encryption key derived from your password is used to encrypt
this master vault key before storing it in the database.</p>
<p>This architecture allows you to change your password without re-encrypting your
entire vault—we just re-encrypt the master vault key with the new encryption
key.</p>
<p>The encryption key is ephemeral. We derive it when needed, use it immediately,
and discard it. It is never persisted to disk and never sent anywhere.</p>
<h3>3. Login Key: Server Authentication</h3>
<p>The third key in our hierarchy is the <strong>login key</strong>, also derived from the
password key through 100,000 rounds of Blake3 PBKDF. This is the only key that
gets sent to the server for authentication.</p>
<p>Because the login key is derived separately from the encryption key,
compromising one doesn't compromise the other. Even if a server is breached and
the login key is stolen, the attacker cannot derive the encryption key needed to
decrypt your vault.</p>
<h2>Blake3 PBKDF: Fast and Secure</h2>
<p>You might notice we're using 100,000 rounds of Blake3 PBKDF rather than a
standard algorithm like PBKDF2. Blake3 is a modern, extremely fast cryptographic
hash function. Even at 100,000 rounds, the entire key derivation completes in
milliseconds on modern hardware.</p>
<p>Our Blake3-based PBKDF works by iteratively applying Blake3's keyed MAC mode:</p>
<pre><code>Round 1: result = blake3Mac(salt, password)
Round 2: result = blake3Mac(salt, result_from_round_1)
...
Round 100,000: result = blake3Mac(salt, result_from_round_99,999)
</code></pre>
<p>Each round adds computational cost for attackers trying to brute-force your
password, while remaining fast enough for legitimate use.</p>
<h2>Salt Derivation</h2>
<p>Each key derivation uses a different salt to ensure cryptographic separation:</p>
<ul>
<li>
<p><strong>Password Salt</strong>: Derived deterministically from your password using
<code>blake3Mac(blake3Hash("KeyPears password salt v1"), password)</code>. This ensures
the same password always produces the same password key.</p>
</li>
<li>
<p><strong>Encryption Salt</strong>: A global constant
<code>blake3Hash("KeyPears encryption salt v1")</code> used for all users. This is safe
because the encryption key is derived from the password key, not directly from
the password.</p>
</li>
<li>
<p><strong>Login Salt</strong>: Another global constant
<code>blake3Hash("KeyPears login salt v1")</code>. Again, safe because it's derived from
the password key.</p>
</li>
</ul>
<h2>Security Properties</h2>
<p>This architecture provides several important security guarantees:</p>
<h3>Defense Against Server Compromise</h3>
<p>If a KeyPears server is compromised, the attacker gains access to:</p>
<ul>
<li>Encrypted vault data</li>
<li>Login keys for authentication</li>
</ul>
<p>The attacker does NOT gain access to:</p>
<ul>
<li>Master passwords</li>
<li>Password keys</li>
<li>Encryption keys</li>
<li>Master vault keys</li>
<li>Decrypted vault contents</li>
</ul>
<p>Without the encryption key, the encrypted vault data is useless to the attacker.</p>
<h3>Defense Against Encrypted Data Theft</h3>
<p>If someone steals your encrypted vault data but doesn't have your credentials:</p>
<ul>
<li>They cannot decrypt it without the encryption key</li>
<li>The encryption key requires the password key</li>
<li>The password key requires your master password</li>
<li>100,000 rounds of Blake3 PBKDF make brute-forcing expensive</li>
</ul>
<h3>Key Separation</h3>
<p>The three keys are cryptographically isolated. Knowing the login key doesn't
help you derive the encryption key, and vice versa. Both require the password
key, which requires the master password.</p>
<h2>The Vault Key Hash: Verification</h2>
<p>When you enter your password to unlock a vault, KeyPears needs to verify you
entered it correctly. We do this by storing a Blake3 hash of the master vault
key in the database.</p>
<p>When you unlock:</p>
<ol>
<li>Derive password key from your password</li>
<li>Derive encryption key from password key</li>
<li>Decrypt the master vault key using the encryption key</li>
<li>Hash the decrypted master vault key</li>
<li>Compare with the stored hash</li>
</ol>
<p>If the hashes match, you entered the correct password. If not, the password is
wrong. This verification happens entirely on your device—the master vault key
never leaves your device, even temporarily.</p>
<h2>Putting It All Together</h2>
<p>Here's what happens when you create a new vault:</p>
<ol>
<li>You enter a master password</li>
<li>KeyPears derives a password key (100k rounds Blake3)</li>
<li>Derives an encryption key from the password key (100k rounds Blake3)</li>
<li>Generates a random master vault key</li>
<li>Encrypts the master vault key with the encryption key</li>
<li>Hashes the master vault key for verification</li>
<li>Stores the encrypted vault key and hash in your local database</li>
</ol>
<p>When you sync to a server:</p>
<ol>
<li>Derive the login key from your password key (100k rounds Blake3)</li>
<li>Send the login key to the server for authentication</li>
<li>Server returns your encrypted master vault key (and other encrypted vault
data)</li>
<li>Derive the encryption key (never sent to server)</li>
<li>Decrypt the master vault key locally</li>
<li>Use the master vault key to decrypt your secrets</li>
</ol>
<p>The server facilitates synchronization but never has the keys needed to decrypt
your data.</p>
<h2>Looking Ahead</h2>
<p>This architecture lays the foundation for secure sharing between users. In
future posts, we'll explore how KeyPears uses Diffie-Hellman key exchange to
share secrets securely between users, and how the master vault key enables
efficient re-encryption without re-deriving keys.</p>
<p>For now, the key takeaway is simple: KeyPears separates authentication from
encryption. Your server can verify who you are without ever having the ability
to decrypt your data. It's cryptography working exactly as it should.</p>]]></content>
        <author>
            <name>KeyPears Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drizzle SQLite Database Migrations in Tauri 2.0]]></title>
        <id>https://keypears.com/blog/2025-10-04-drizzle-sqlite-tauri</id>
        <link href="https://keypears.com/blog/2025-10-04-drizzle-sqlite-tauri"/>
        <updated>2025-10-04T11:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The
solutions described here are part of our development process and may evolve
before our official release.</p>
...]]></summary>
        <content type="html"><![CDATA[<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The
solutions described here are part of our development process and may evolve
before our official release.</p>
<h2>The Problem</h2>
<p>Building a local-first application with Tauri 2.0, we needed a robust database
solution for storing encrypted vault data on users' devices. We wanted:</p>
<ul>
<li>Type-safe database queries</li>
<li>Proper schema migrations that work in production</li>
<li>Pure TypeScript implementation (no Rust for basic DB operations)</li>
<li>A solution that works across desktop and mobile platforms</li>
</ul>
<p>After evaluating options, we chose <strong>Drizzle ORM</strong> with <strong>SQLite</strong> via the
official <strong>tauri-plugin-sql</strong>. This combination gives us TypeScript-first
development with the reliability of SQLite.</p>
<h2>The Challenge</h2>
<p>Unlike traditional Node.js environments where you have direct filesystem access
and can use drivers like <code>better-sqlite3</code>, Tauri's sandboxed environment
requires a different approach. Drizzle's standard migration tools assume direct
database access, but with Tauri, we need to go through the plugin system.</p>
<p>Here's how we solved it.</p>
<h2>Tech Stack</h2>
<ul>
<li><strong>Tauri 2.0</strong> - Cross-platform app framework</li>
<li><strong>Drizzle ORM</strong> - TypeScript ORM</li>
<li><strong>drizzle-kit</strong> - Schema migration generator</li>
<li><strong>@tauri-apps/plugin-sql</strong> - Official Tauri SQLite plugin</li>
<li><strong>React Router</strong> - For app routing and loaders</li>
</ul>
<h2>Step 1: Install Dependencies</h2>
<p>First, add the necessary packages:</p>
<pre><code class="language-bash"># Production dependencies
pnpm add drizzle-orm @tauri-apps/plugin-sql

# Development dependencies
pnpm add -D drizzle-kit
</code></pre>
<p>Then add the Tauri plugin to your Rust dependencies in <code>src-tauri/Cargo.toml</code>:</p>
<pre><code class="language-toml">[dependencies]
tauri-plugin-sql = { version = "2", features = ["sqlite"] }
</code></pre>
<h2>Step 2: Configure Tauri Permissions</h2>
<p>Tauri 2.0 requires explicit permission grants. Add SQL permissions to
<code>src-tauri/capabilities/default.json</code>:</p>
<pre><code class="language-json">{
  "$schema": "../gen/schemas/desktop-schema.json",
  "identifier": "default",
  "description": "Capability for the main window",
  "windows": ["main"],
  "permissions": [
    "core:default",
    "sql:default",
    "sql:allow-load",
    "sql:allow-execute",
    "sql:allow-select",
    "sql:allow-close"
  ]
}
</code></pre>
<p>Without these permissions, you'll get "not allowed" errors when trying to access
the database.</p>
<h2>Step 3: Define Your Schema</h2>
<p>Create your Drizzle schema at <code>app/db/schema.ts</code>:</p>
<pre><code class="language-typescript">import { sqliteTable, text, integer } from "drizzle-orm/sqlite-core";

export const vaults = sqliteTable("vaults", {
  id: integer("id").primaryKey({ autoIncrement: true }),
  name: text("name").notNull().unique(),
});
</code></pre>
<h2>Step 4: Set Up the SQLite Proxy</h2>
<p>Since we can't use standard SQLite drivers in Tauri, we use Drizzle's
<code>sqlite-proxy</code> adapter. Create <code>app/db/index.ts</code>:</p>
<pre><code class="language-typescript">import { drizzle } from "drizzle-orm/sqlite-proxy";
import Database from "@tauri-apps/plugin-sql";
import * as schema from "./schema";

export async function getDb() {
  return await Database.load("sqlite:keypears.db");
}

function isSelectQuery(sql: string): boolean {
  return sql.trim().toLowerCase().startsWith("select");
}

export const db = drizzle&#x3C;typeof schema>(
  async (sql, params, method) => {
    const sqlite = await getDb();
    let rows: any = [];

    if (isSelectQuery(sql)) {
      rows = await sqlite.select(sql, params).catch((e) => {
        console.error("SQL Error:", e);
        return [];
      });
    } else {
      rows = await sqlite.execute(sql, params).catch((e) => {
        console.error("SQL Error:", e);
        return [];
      });
      return { rows: [] };
    }

    rows = rows.map((row: any) => Object.values(row));
    const results = method === "all" ? rows : rows[0];
    await sqlite.close();
    return { rows: results };
  },
  { schema: schema, logger: true },
);
</code></pre>
<p>The proxy adapter translates Drizzle queries into calls to the Tauri SQL plugin.</p>
<h2>Step 5: Configure Migration Generation</h2>
<p>Create <code>drizzle.config.ts</code>:</p>
<pre><code class="language-typescript">import type { Config } from "drizzle-kit";

export default {
  schema: "./app/db/schema.ts",
  out: "./app/db/migrations",
  dialect: "sqlite",
} satisfies Config;
</code></pre>
<p>Add a script to <code>package.json</code>:</p>
<pre><code class="language-json">{
  "scripts": {
    "db:migrate": "drizzle-kit generate"
  }
}
</code></pre>
<h2>Step 6: Implement Migration Runner</h2>
<p>Here's the key part - implementing our own migration system. Create
<code>app/db/migrate.ts</code>:</p>
<pre><code class="language-typescript">import { getDb } from "./index";

// Dynamically import all SQL migration files
const migrationFiles = import.meta.glob&#x3C;string>("./migrations/*.sql", {
  query: "?raw",
  import: "default",
  eager: true,
});

// Create migrations tracking table
async function ensureMigrationsTable() {
  const sqlite = await getDb();
  await sqlite.execute(`
    CREATE TABLE IF NOT EXISTS __drizzle_migrations (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      hash TEXT NOT NULL UNIQUE,
      created_at INTEGER NOT NULL
    )
  `);
  await sqlite.close();
}

// Get list of applied migrations
async function getAppliedMigrations(): Promise&#x3C;string[]> {
  const sqlite = await getDb();
  const rows = await sqlite
    .select&#x3C;
      Array&#x3C;{ hash: string }>
    >("SELECT hash FROM __drizzle_migrations ORDER BY id")
    .catch(() => []);
  await sqlite.close();
  return rows.map((row) => row.hash);
}

// Record migration as applied
async function recordMigration(hash: string) {
  const sqlite = await getDb();
  const timestamp = Date.now();
  await sqlite.execute(
    "INSERT INTO __drizzle_migrations (hash, created_at) VALUES (?, ?)",
    [hash, timestamp],
  );
  await sqlite.close();
}

// Execute SQL file
async function executeSqlFile(sqlContent: string) {
  const sqlite = await getDb();
  const statements = sqlContent
    .split("--> statement-breakpoint")
    .map((s) => s.trim())
    .filter((s) => s.length > 0);

  for (const statement of statements) {
    await sqlite.execute(statement).catch((e) => {
      console.error("Migration error:", e);
      throw e;
    });
  }

  await sqlite.close();
}

export async function runMigrations() {
  console.log("Running database migrations...");

  try {
    await ensureMigrationsTable();
    const appliedMigrations = await getAppliedMigrations();

    const migrationPaths = Object.keys(migrationFiles).sort();

    const pendingMigrations = migrationPaths.filter((path) => {
      const filename = path.split("/").pop() || path;
      return !appliedMigrations.includes(filename);
    });

    if (pendingMigrations.length === 0) {
      console.log("All migrations already applied");
      return;
    }

    for (const path of pendingMigrations) {
      const filename = path.split("/").pop() || path;
      const migrationContent = migrationFiles[path];

      console.log(`Executing migration: ${filename}`);
      await executeSqlFile(migrationContent);
      await recordMigration(filename);
      console.log(`✓ Applied: ${filename}`);
    }

    console.log(
      `Successfully completed ${pendingMigrations.length} migration(s)`,
    );
  } catch (error) {
    console.error("Migration failed:", error);
    throw error;
  }
}
</code></pre>
<p>This implements Drizzle's migration tracking pattern:</p>
<ul>
<li>Uses <code>__drizzle_migrations</code> table to track applied migrations</li>
<li>Only runs new migrations on subsequent app launches</li>
<li>Supports incremental migrations as your schema evolves</li>
</ul>
<h2>Step 7: Run Migrations on App Startup</h2>
<p>In your root component (<code>app/root.tsx</code>), use a clientLoader to run migrations
before rendering:</p>
<pre><code class="language-typescript">import { runMigrations } from "./db/migrate";

export async function clientLoader() {
  await runMigrations();
  return null;
}

export function HydrateFallback() {
  return (
    &#x3C;div className="flex min-h-screen items-center justify-center">
      &#x3C;h1>Migrating the database...&#x3C;/h1>
    &#x3C;/div>
  );
}
</code></pre>
<p>React Router will show the fallback while migrations run, ensuring the database
is ready before any component renders.</p>
<h2>Step 8: Create Model Functions</h2>
<p>With everything set up, create type-safe model functions at
<code>app/db/models/vault.ts</code>:</p>
<pre><code class="language-typescript">import { db } from "../index";
import { vaults } from "../schema";
import { eq, count } from "drizzle-orm";

export interface Vault {
  id: number;
  name: string;
}

export async function createVault(name: string): Promise&#x3C;Vault> {
  const result = await db.insert(vaults).values({ name }).returning();
  return result[0];
}

export async function getVault(id: number): Promise&#x3C;Vault | undefined> {
  const result = await db.select().from(vaults).where(eq(vaults.id, id));
  return result[0];
}

export async function getVaults(): Promise&#x3C;Vault[]> {
  return await db.select().from(vaults);
}

export async function countVaults(): Promise&#x3C;number> {
  const result = await db.select({ count: count() }).from(vaults);
  return result[0]?.count ?? 0;
}
</code></pre>
<h2>Usage Workflow</h2>
<h3>Development</h3>
<p>When you modify your schema:</p>
<pre><code class="language-bash"># 1. Update app/db/schema.ts
# 2. Generate new migration
pnpm run db:migrate

# 3. Restart app - migration runs automatically
</code></pre>
<p>During development, you can safely delete all migrations and regenerate them
from scratch. Just delete the database file and migration files, then
regenerate.</p>
<h3>Production</h3>
<p>Before releasing v1.0:</p>
<ol>
<li>Delete all development migrations</li>
<li>Generate one clean migration from your final schema</li>
<li>Commit this as your baseline</li>
</ol>
<p>After release, <strong>never delete migrations</strong> - only add new ones. Users will have
the old migrations applied, and new migrations build incrementally.</p>
<h2>Database File Location</h2>
<p>The Tauri SQL plugin creates the database in the app's data directory:</p>
<ul>
<li><strong>macOS</strong>: <code>~/Library/Application Support/{app-identifier}/keypears.db</code></li>
<li><strong>Linux</strong>: <code>~/.local/share/{app-identifier}/keypears.db</code></li>
<li><strong>Windows</strong>: <code>%APPDATA%\{app-identifier}\keypears.db</code></li>
</ul>
<h2>Troubleshooting</h2>
<p><strong>Permission errors</strong>: Make sure you've added all SQL permissions to
<code>capabilities/default.json</code></p>
<p><strong>Migration fails</strong>: Check browser console in the Tauri webview for detailed
error messages</p>
<p><strong>Type errors</strong>: Run <code>pnpm run typecheck</code> to catch issues before runtime</p>
<h2>Conclusion</h2>
<p>This setup gives us:</p>
<ul>
<li>✅ Type-safe database queries with Drizzle</li>
<li>✅ Proper migration tracking that works in production</li>
<li>✅ Pure TypeScript - no Rust code needed for basic operations</li>
<li>✅ Cross-platform compatibility (desktop &#x26; mobile)</li>
<li>✅ Incremental migrations as the schema evolves</li>
</ul>
<p>The combination of Drizzle's <code>sqlite-proxy</code> adapter with Tauri's SQL plugin
provides a robust foundation for local-first data storage. While we had to
implement our own migration runner, we followed Drizzle's patterns to ensure
compatibility and maintainability.</p>
<h2>Resources</h2>
<ul>
<li><a href="https://orm.drizzle.team/">Drizzle ORM</a></li>
<li><a href="https://v2.tauri.app/plugin/sql/">Tauri SQL Plugin</a></li>
<li><a href="https://v2.tauri.app/security/capabilities/">Tauri Capabilities</a></li>
</ul>]]></content>
        <author>
            <name>KeyPears Team</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing KeyPears: A New Approach to Password Management]]></title>
        <id>https://keypears.com/blog/2025-10-03-introducing-keypears</id>
        <link href="https://keypears.com/blog/2025-10-03-introducing-keypears"/>
        <updated>2025-10-03T11:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>We're excited to announce KeyPears, a new password manager designed for the
modern era of digital security and self-custody.</p>
<h2>Why KeyPears?</h2>
<p>Traditional password managers have served ...]]></summary>
        <content type="html"><![CDATA[<p>We're excited to announce KeyPears, a new password manager designed for the
modern era of digital security and self-custody.</p>
<h2>Why KeyPears?</h2>
<p>Traditional password managers have served us well, but they come with
limitations. Most rely on centralized services, creating single points of
failure and raising questions about who truly controls your most sensitive data.
KeyPears takes a different approach.</p>
<h2>Local-First, Sync-Enabled</h2>
<p>KeyPears is built on a local-first architecture. Your secrets live on your
devices, encrypted with keys only you control. But unlike purely local
solutions, KeyPears solves the synchronization problem through a permissionless
marketplace of third-party service providers using an open protocol—similar to
how email works.</p>
<p>Anyone can run a KeyPears node. The protocol is open source. You maintain full
self-custody while enjoying seamless synchronization across all your devices.</p>
<h2>Built for Sharing</h2>
<p>Modern work requires secure secret sharing. KeyPears uses end-to-end encryption
with public/private key pairs for each user. When alice@example.com needs to
share a secret with bob@example2.com, they use Diffie-Hellman key exchange to
derive a shared secret that only they know. The architecture mirrors email, but
with cryptography-first design.</p>
<h2>More Than Passwords</h2>
<p>While we call it a password manager, KeyPears is designed to handle:</p>
<ul>
<li>Passwords</li>
<li>Cryptocurrency wallet keys</li>
<li>API keys</li>
<li>Environment variables</li>
<li>SSH keys</li>
<li>PGP keys</li>
</ul>
<p>For cryptocurrency users seeking self-custody and businesses that need secure
secret sharing without expensive enterprise subscriptions, KeyPears offers a
compelling alternative.</p>
<h2>What's Next</h2>
<p>KeyPears is in active development. We're building native applications for
Windows, macOS, Linux, Android, and iOS using Tauri. The project is Apache 2.0
licensed and open source.</p>
<p>This is just the beginning. We're excited to build KeyPears with the community
and create a new standard for secure, self-custodied secret management.</p>
<p>Stay tuned for more updates as we continue development.</p>]]></content>
        <author>
            <name>KeyPears Team</name>
        </author>
    </entry>
</feed>