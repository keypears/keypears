{
  "version": "https://jsonfeed.org/version/1",
  "title": "KeyPears Blog",
  "home_page_url": "https://keypears.com",
  "feed_url": "https://keypears.com/blog/feed.json",
  "description": "Updates and insights from the KeyPears team",
  "items": [
    {
      "id": "https://keypears.com/blog/2025-12-09-switching-to-sha256",
      "content_html": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and\ncryptocurrency wallet. The design decisions described here represent our\ndevelopment approach and may evolve before our official release.</p>\n<p>Last week we completed a significant migration: replacing Blake3 with SHA-256\nthroughout the KeyPears codebase. This wasn't because Blake3 failed us—it worked\nperfectly. We switched because SHA-256 is the industry standard, and that\nmatters more than we initially appreciated.</p>\n<p>This post explains the technical tradeoffs and why we made this decision.</p>\n<h2>What We Changed</h2>\n<p>The migration touched every layer of our stack:</p>\n<p><strong>Dependencies:</strong></p>\n<ul>\n<li><code>@webbuf/blake3</code> → <code>@webbuf/sha256</code></li>\n<li><code>@webbuf/acb3</code> → <code>@webbuf/acs2</code></li>\n</ul>\n<p><strong>Functions:</strong></p>\n<ul>\n<li><code>blake3Hash()</code> → <code>sha256Hash()</code></li>\n<li><code>blake3Mac()</code> → <code>sha256Hmac()</code></li>\n<li><code>blake3Pbkdf()</code> → <code>sha256Pbkdf()</code></li>\n<li><code>acb3Encrypt()</code> / <code>acb3Decrypt()</code> → <code>acs2Encrypt()</code> / <code>acs2Decrypt()</code></li>\n</ul>\n<p><strong>Encryption scheme:</strong></p>\n<ul>\n<li>ACB3 (AES-256-CBC + Blake3-MAC) → ACS2 (AES-256-CBC + SHA-256-HMAC)</li>\n</ul>\n<p>Both hash functions produce 32-byte (256-bit) output, so our <code>FixedBuf&#x3C;32></code>\ntypes remained unchanged. The migration was primarily find-and-replace with\nupdated test vectors.</p>\n<h2>Why Blake3 Is Excellent</h2>\n<p>Let's be clear: Blake3 is a technically superior hash function in many respects.</p>\n<p><strong>Speed</strong>: Blake3 is significantly faster than SHA-256, especially for large\ndata. It achieves this through a Merkle tree construction that enables parallel\nhashing across multiple CPU cores. Where SHA-256 processes data sequentially,\nBlake3 can divide large inputs into chunks and hash them simultaneously.</p>\n<p><strong>Modern design</strong>: Blake3 was designed in 2020 with modern cryptographic\ninsights. It's built on the well-analyzed BLAKE2 (used in Argon2, the\nrecommended password hashing algorithm) and incorporates lessons from decades of\nhash function cryptanalysis.</p>\n<p><strong>Versatility</strong>: Blake3 supports keyed hashing (MAC), key derivation (KDF), and\nextendable output (XOF) natively. SHA-256 requires wrapper constructions like\nHMAC and HKDF for equivalent functionality.</p>\n<p><strong>Simplicity</strong>: Despite its speed, Blake3 has a remarkably simple specification.\nThe reference implementation is ~500 lines of C.</p>\n<p>We were happy with Blake3. Our key derivation system worked flawlessly:</p>\n<pre><code class=\"language-typescript\">// Three-tier key derivation (unchanged structure, new primitives)\nfunction sha256Pbkdf(\n  password: string | WebBuf,\n  salt: FixedBuf&#x3C;32>,\n  rounds: number = 100_000,\n): FixedBuf&#x3C;32> {\n  const passwordBuf = typeof password === \"string\"\n    ? WebBuf.fromUtf8(password)\n    : password;\n\n  let result = sha256Hmac(salt, passwordBuf);\n  for (let i = 1; i &#x3C; rounds; i++) {\n    result = sha256Hmac(salt, result.buf);\n  }\n  return result;\n}\n</code></pre>\n<h2>Why We Switched Anyway</h2>\n<h3>1. The Speed Advantage Doesn't Apply to Our Use Case</h3>\n<p>Blake3's killer feature is parallel hashing of large data. But KeyPears doesn't\nhash large data. We hash:</p>\n<ul>\n<li><strong>Passwords</strong>: 8-64 characters (typically under 100 bytes)</li>\n<li><strong>Secrets</strong>: API keys, credentials, wallet seeds (typically under 1KB)</li>\n<li><strong>Keys</strong>: 32-byte fixed buffers</li>\n</ul>\n<p>For inputs this small, Blake3's Merkle tree construction provides no benefit.\nThe parallelization overhead might actually make it slower than sequential\nhashing for tiny inputs. And even if Blake3 were faster for small data, the\ndifference would be measured in microseconds—completely irrelevant when our key\nderivation performs 100,000 PBKDF rounds that dominate execution time.</p>\n<p>When we profiled vault creation:</p>\n<ul>\n<li>Key derivation: ~800ms (100k rounds × 2 keys)</li>\n<li>Individual hash operations: ~0.001ms</li>\n</ul>\n<p>The hash function choice affects performance by roughly 0.0001%. Switching from\nBlake3 to SHA-256 has no measurable impact on user experience.</p>\n<h3>2. Industry Standard Matters for Customer Acquisition</h3>\n<p>When enterprise customers evaluate password managers, they ask questions like:</p>\n<ul>\n<li>\"What encryption algorithm do you use?\"</li>\n<li>\"Is your cryptography FIPS 140-2 compliant?\"</li>\n<li>\"Do you use industry-standard algorithms?\"</li>\n</ul>\n<p>With Blake3, our answers required explanation: \"We use Blake3, which is a modern\nhash function designed in 2020. It's based on BLAKE2, which is used in Argon2.\nIt's very fast and secure, though it's not yet widely adopted...\"</p>\n<p>With SHA-256, our answer is: \"Yes.\"</p>\n<p>SHA-256 is:</p>\n<ul>\n<li>Part of the SHA-2 family standardized by NIST</li>\n<li>FIPS 140-2 approved</li>\n<li>Used by Bitcoin, TLS, HTTPS, and virtually every security system</li>\n<li>Understood by every security auditor</li>\n<li>Required by many compliance frameworks</li>\n</ul>\n<p>The security difference between Blake3 and SHA-256 is negligible for our threat\nmodel—both provide 256-bit security against preimage and collision attacks. But\nthe compliance difference is significant.</p>\n<h3>3. Battle-Tested at Scale</h3>\n<p>SHA-256 has been deployed in production systems since 2001. It secures:</p>\n<ul>\n<li>Every Bitcoin transaction ever made (~900 million transactions)</li>\n<li>Every HTTPS connection using TLS (trillions daily)</li>\n<li>Every Git commit in every repository worldwide</li>\n<li>Government systems, financial institutions, healthcare records</li>\n</ul>\n<p>This deployment scale represents the most extensive real-world cryptanalysis\npossible. If SHA-256 had weaknesses, attackers with billions of dollars in\nincentive would have found them.</p>\n<p>Blake3 is mathematically sound and designed by respected cryptographers. But it\nwas released in 2020 and hasn't yet accumulated the same scale of real-world\ntesting. Given that both algorithms provide equivalent security for our use\ncase, we chose the one with 24 years of battle-testing.</p>\n<h3>4. Ecosystem Compatibility</h3>\n<p>SHA-256 implementations exist in every programming language, every platform, and\nevery hardware security module. If we ever need to:</p>\n<ul>\n<li>Integrate with HSMs for enterprise key management</li>\n<li>Support hardware security keys (FIDO2/WebAuthn)</li>\n<li>Interface with existing enterprise systems</li>\n<li>Pass third-party security audits</li>\n</ul>\n<p>SHA-256 will be expected and supported. Blake3 might require custom integration\nwork.</p>\n<h2>What We're Not Saying</h2>\n<p>This decision is <strong>not</strong> a criticism of Blake3. We want to be explicit:</p>\n<p><strong>Blake3 is secure.</strong> There are no known attacks, weaknesses, or concerns about\nBlake3's cryptographic security. It was designed by a team including the\ncreators of Argon2 and BLAKE2.</p>\n<p><strong>Blake3 is technically superior for large data.</strong> If we were building a backup\nsystem, a file integrity checker, or a content-addressed storage system, Blake3\nwould be the obvious choice.</p>\n<p><strong>Blake3 may become an industry standard.</strong> It's gaining adoption, and in five\nyears the \"unknown algorithm\" concern may disappear. We might even switch back.</p>\n<p>We switched because <strong>SHA-256 is good enough for our specific use case, and the\nindustry standard status provides tangible benefits that Blake3's technical\nadvantages don't.</strong> This is an engineering tradeoff, not a quality judgment.</p>\n<h2>The Migration Process</h2>\n<p>The actual migration took about a day:</p>\n<ol>\n<li>\n<p><strong>Update dependencies</strong>: Replace <code>@webbuf/blake3</code> with <code>@webbuf/sha256</code>,\n<code>@webbuf/acb3</code> with <code>@webbuf/acs2</code></p>\n</li>\n<li>\n<p><strong>Update function calls</strong>: Find-and-replace across lib, api-server, tauri-ts,\nwebapp</p>\n</li>\n<li>\n<p><strong>Update test vectors</strong>: SHA-256 produces different output than Blake3, so\ntest expectations needed updating</p>\n</li>\n<li>\n<p><strong>Update documentation</strong>: Replace Blake3 references in crypto.md, auth.md,\nand AGENTS.md</p>\n</li>\n<li>\n<p><strong>Run tests</strong>: All 71 tests pass (40 in lib, 31 in api-server)</p>\n</li>\n<li>\n<p><strong>Manual testing</strong>: Create vaults, sync across devices, verify encryption\nworks</p>\n</li>\n</ol>\n<p>The migration was straightforward because both hash functions have identical\noutput sizes (32 bytes) and the <code>@webbuf</code> packages have matching APIs. The\nhardest part was updating documentation.</p>\n<p><strong>Breaking change note</strong>: This migration breaks compatibility with any existing\nencrypted data. Since we're pre-MVP with no real user data, this was acceptable.\nA production migration would require a more careful versioning strategy.</p>\n<h2>Current Cryptography Stack</h2>\n<p>After this migration, KeyPears uses:</p>\n<ul>\n<li><strong>Hashing</strong>: SHA-256 (via <code>@webbuf/sha256</code>)</li>\n<li><strong>MAC</strong>: SHA-256-HMAC (via <code>sha256Hmac()</code>)</li>\n<li><strong>KDF</strong>: Custom PBKDF using SHA-256-HMAC (100,000 rounds)</li>\n<li><strong>Encryption</strong>: ACS2 = AES-256-CBC + SHA-256-HMAC (via <code>@webbuf/acs2</code>)</li>\n</ul>\n<p>All cryptographic primitives are NIST-standardized algorithms with decades of\nreal-world deployment. The underlying implementations are Rust compiled to\nWebAssembly, providing both memory safety and cross-platform consistency.</p>\n<h2>Lessons Learned</h2>\n<p><strong>Technical superiority isn't always the deciding factor.</strong> Blake3 is faster,\nmore modern, and arguably more elegant. SHA-256 is more widely understood,\ntrusted, and required. For a security product, trust and compliance matter as\nmuch as technical merit.</p>\n<p><strong>Know your actual use case.</strong> Blake3's parallel hashing is irrelevant when\nyou're hashing 32-byte keys. We spent time with an optimization we couldn't\nbenefit from.</p>\n<p><strong>Industry standards exist for good reasons.</strong> NIST standardization, FIPS\ncompliance, and widespread adoption aren't bureaucratic checkboxes—they\nrepresent accumulated trust that takes decades to build. Sometimes the \"boring\"\nchoice is the right choice.</p>\n<p><strong>Migrations are easier early.</strong> Changing cryptographic primitives after MVP\nlaunch would require careful data migration and backward compatibility. Doing it\nnow, with no real user data, was trivial.</p>\n<h2>Conclusion</h2>\n<p>We switched from Blake3 to SHA-256 not because Blake3 failed, but because\nSHA-256 succeeds in ways that matter more for our product: industry recognition,\ncompliance compatibility, and customer trust.</p>\n<p>Blake3 is an excellent hash function, and we'd recommend it for use cases that\nbenefit from its parallel performance—large file hashing, content-addressed\nstorage, or high-throughput data processing.</p>\n<p>For a password manager where we're hashing small secrets and deriving keys,\nSHA-256 provides identical practical security with better industry positioning.\nWhen both options are secure, we chose the one that makes \"Do you use\nindustry-standard cryptography?\" easy to answer.</p>\n<p>The migration is complete, all tests pass, and vaults sync correctly. We're back\nto building features.</p>",
      "url": "https://keypears.com/blog/2025-12-09-switching-to-sha256",
      "title": "Why We Switched from Blake3 to SHA-256",
      "summary": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and\ncryptocurrency wallet. The design decisions described here represent our\ndevelopment approach and may evolve b...",
      "date_modified": "2025-12-09T12:00:00.000Z",
      "author": {
        "name": "KeyPears Team"
      }
    },
    {
      "id": "https://keypears.com/blog/2025-12-06-codebase-audit",
      "content_html": "<p>We recently paused feature development to audit our entire codebase. KeyPears\nshipped fast—cross-device sync, server authentication, encrypted secret\nstorage—and we wanted to make sure we hadn't accumulated technical debt or,\nworse, security issues along the way.</p>\n<p>What we found was instructive. The good news: our architecture is sound, our\ncryptography is correct, and our zero-knowledge design holds up. The concerning\nnews: we found debug logging that would have exposed cryptographic keys in\nproduction. This is exactly why we audit.</p>\n<h2>What We Looked For</h2>\n<p>Our audit covered six packages across the monorepo, examining each for:</p>\n<ol>\n<li><strong>General Software Best Practices</strong>: Linting, type checking, test coverage,\ncode quality</li>\n<li><strong>Third-Party Dependencies</strong>: Outdated packages, security vulnerabilities</li>\n<li><strong>Security Assessment</strong>: Crypto implementation, zero-knowledge verification,\ninput validation</li>\n<li><strong>Scalability</strong>: N+1 queries, memory leaks, re-render optimization</li>\n<li><strong>UI/UX Consistency</strong>: Theme compliance, accessibility, loading states</li>\n<li><strong>File-Specific Checks</strong>: Config correctness, route typing, component\nstructure</li>\n</ol>\n<p>We ran automated tools first (<code>pnpm lint</code>, <code>pnpm typecheck</code>, <code>pnpm test</code>, <code>cargo clippy</code>), then manually reviewed each package against our checklist.</p>\n<h2>The Critical Finding: Debug Logging</h2>\n<p>The most significant discovery was in our Tauri app's vault creation and import\nflows. During development, we'd added extensive console.log statements to debug\nthe cryptographic key derivation process:</p>\n<pre><code class=\"language-typescript\">// What we found and removed:\nconsole.log(\"Password Key:\", passwordKey.buf.toHex());\nconsole.log(\"Login Key:\", loginKey.buf.toHex());\nconsole.log(\"Encryption Key:\", encryptionKey.buf.toHex());\nconsole.log(\"Decrypted Vault Key:\", vaultKey.buf.toHex());\n</code></pre>\n<p>There were approximately 80 of these statements across two files:\n<code>import-vault.tsx</code> and <code>new-vault.3.tsx</code>. Each one logged a sensitive\ncryptographic key in hexadecimal format.</p>\n<p>In development, this is harmless—helpful, even, for understanding the key\nderivation flow. In production, it's a disaster waiting to happen. Anyone with\naccess to browser developer tools could see every key involved in vault\nencryption. The zero-knowledge architecture we carefully designed would be\nmeaningless if the client itself was leaking keys to the console.</p>\n<p>We removed all 80 statements. The code now proceeds silently, as it should.</p>\n<p><strong>Lesson learned</strong>: Debug logging during development is fine, but it must be\nremoved before shipping. Our audit checklist now includes searching for\n<code>console.log</code> statements containing key-related terms (<code>Key</code>, <code>password</code>,\n<code>secret</code>, <code>token</code>).</p>\n<h2>Type Safety Improvements</h2>\n<p>React Router v7 provides a <code>href()</code> function that type-checks route paths at\ncompile time. If you rename a route file, any <code>href(\"/old-path\")</code> calls will\nfail to compile—catching errors before they reach production.</p>\n<p>We found several places where developers had used string literals instead:</p>\n<pre><code class=\"language-typescript\">// What we found:\n&#x3C;Link to=\"/\">Home&#x3C;/Link>\n&#x3C;Link to=\"/new-vault/1\">Create Vault&#x3C;/Link>\n\n// What it should be:\n&#x3C;Link to={href(\"/\")}>Home&#x3C;/Link>\n&#x3C;Link to={href(\"/new-vault/1\")}>Create Vault&#x3C;/Link>\n</code></pre>\n<p>The difference seems minor, but it matters. With string literals, renaming\n<code>/new-vault/1</code> to <code>/vault/new/step-1</code> would silently break links. With <code>href()</code>,\nthe compiler catches it immediately.</p>\n<p>We updated all navigation in the Tauri app to use type-safe routes: navbar,\nfooter, vault creation wizard, import flow. It's a small change that prevents a\ncategory of bugs entirely.</p>\n<h2>UI Consistency: Theme Colors</h2>\n<p>KeyPears uses the Catppuccin color palette with CSS variables for theming. Error\ntext should use <code>text-destructive</code>, which maps to the appropriate red in both\nlight and dark modes.</p>\n<p>We found inconsistent usage:</p>\n<pre><code class=\"language-typescript\">// Inconsistent:\n&#x3C;p className=\"text-red-500\">{error}&#x3C;/p>\n\n// Consistent:\n&#x3C;p className=\"text-destructive\">{error}&#x3C;/p>\n</code></pre>\n<p>The difference is subtle in light mode but significant in dark mode, where\n<code>text-red-500</code> might not have sufficient contrast against dark backgrounds.\nUsing theme variables ensures the design system works correctly across all\nthemes.</p>\n<p>We standardized error colors in several components: the password generator,\npassword memorizer, and vault name input.</p>\n<h2>Config Typo: The Silent Build Breaker</h2>\n<p>In <code>tauri.conf.json</code>, we found:</p>\n<pre><code class=\"language-json\">{\n  \"frontendDist\": \"../ts-tauri/dist\"\n}\n</code></pre>\n<p>The correct path is <code>../tauri-ts/dist</code>. The folders are named <code>tauri-ts</code> (for\nTypeScript) and <code>tauri-rs</code> (for Rust), not <code>ts-tauri</code>.</p>\n<p>This typo hadn't caused problems yet because we typically run the dev server\nrather than building production bundles locally. But it would have failed the\nfirst time someone tried to build a release binary, causing confusion and wasted\ndebugging time.</p>\n<p><strong>Lesson learned</strong>: Config files deserve the same scrutiny as code. Paths, URLs,\nand identifiers are easy to typo and hard to spot in review.</p>\n<h2>What Passed With Flying Colors</h2>\n<p>Not everything was problems. Much of the codebase held up well:</p>\n<p><strong>Cryptography</strong>: Our three-tier key derivation (password → passwordKey →\nencryptionKey + loginKey) is correctly implemented. The server never receives\nencryption keys, only login keys—and those are further hashed server-side. The\nzero-knowledge architecture is sound.</p>\n<p><strong>Memory management</strong>: All intervals, timers, and event listeners in the Tauri\napp have proper cleanup in <code>useEffect</code> return functions. No memory leaks.</p>\n<p><strong>Sync performance</strong>: The background sync service uses exponential backoff on\nerrors (5s → 10s → 20s), preventing thundering herd problems. Pagination is\nimplemented for activity logs.</p>\n<p><strong>Accessibility</strong>: Interactive elements have proper <code>aria-label</code> attributes.\nKeyboard navigation works throughout the app.</p>\n<p><strong>Rust code</strong>: Our Tauri backend is minimal (~43 lines) by design—all business\nlogic is in TypeScript. <code>cargo clippy</code> passes with no warnings.</p>\n<h2>The Audit Process</h2>\n<p>For future reference, here's how we structured the audit:</p>\n<ol>\n<li>\n<p><strong>Automated checks first</strong>: Run lint, typecheck, and tests for each package.\nFix any failures before proceeding.</p>\n</li>\n<li>\n<p><strong>Dependency review</strong>: Run <code>pnpm outdated</code> for each package. Update\ndependencies to latest patch versions.</p>\n</li>\n<li>\n<p><strong>Manual review by category</strong>: Work through the checklist systematically.\nSecurity issues get fixed immediately; style issues get noted for later.</p>\n</li>\n<li>\n<p><strong>Document findings</strong>: Update the audit guide with lessons learned. Future\naudits benefit from past discoveries.</p>\n</li>\n</ol>\n<p>We've published our full audit checklist in the repository at\n<a href=\"https://github.com/keypears/keypears/blob/main/docs/audit.md\">docs/audit.md</a>.\nIt covers everything from TypeScript best practices to zero-knowledge\narchitecture verification to UI accessibility checks.</p>",
      "url": "https://keypears.com/blog/2025-12-06-codebase-audit",
      "title": "What We Found in Our Codebase Audit",
      "summary": "<p>We recently paused feature development to audit our entire codebase. KeyPears\nshipped fast—cross-device sync, server authentication, encrypted secret\nstorage—and we wanted to make sure we hadn't ac...",
      "date_modified": "2025-12-06T18:00:00.000Z",
      "author": {
        "name": "KeyPears Team"
      }
    },
    {
      "id": "https://keypears.com/blog/2025-12-01-third-party-hosting",
      "content_html": "<p>Imagine you own <code>example.com</code>. You run your own website there, but you don't run\nyour own email server—Gmail or Fastmail handles that for you. Your email address\nis still <code>you@example.com</code>, but Google or Fastmail does the heavy lifting of\nrunning the mail servers, managing spam, and keeping everything online.</p>\n<p>What if password management worked the same way?</p>\n<p>Today we shipped the foundation for exactly that: the ability to point your\ndomain's KeyPears protocol at any third-party hosting provider. It's a\nproof-of-concept, but it works—and it brings us one step closer to making\ndecentralized password management as easy as hosted email.</p>\n<h2>Why Decentralization Matters</h2>\n<p>Email is one of the internet's great success stories in federated architecture.\nAnyone can run an email server. Gmail users can email ProtonMail users who can\nemail self-hosted server users. There's no central authority deciding who gets\nto participate. The protocol is open, the address format is universal, and\ninteroperability is the default.</p>\n<p>KeyPears borrows this architecture, but improves on it in a critical way:\n<strong>end-to-end encryption by default</strong>. When <code>alice@keypears.com</code> shares a secret\nwith <code>bob@company.com</code>, the servers never see the plaintext. They're just\ncoordinators—dumb pipes that route encrypted blobs between clients that hold the\nreal keys.</p>\n<p>The address format mirrors email intentionally. Your vault is\n<code>yourname@yourdomain.com</code>. You can use our hosted service at <code>keypears.com</code>, run\nyour own server, or—with what we built today—point your domain at any KeyPears\nhosting provider you trust.</p>\n<p>This means companies with different domains and different service providers can\nstill share secrets securely. Marketing at <code>acme.com</code> (hosted by Provider A) can\nshare API keys with engineering at <code>partner.io</code> (self-hosted) using the same\nDiffie-Hellman key exchange that makes the whole system work.</p>\n<h2>The <code>.well-known/keypears.json</code> Protocol</h2>\n<p>The implementation is simple. Domain owners create a file at\n<code>/.well-known/keypears.json</code> that tells clients where to find the API:</p>\n<pre><code class=\"language-json\">{\n  \"version\": 1,\n  \"apiUrl\": \"https://keypears.com/api\"\n}\n</code></pre>\n<p>That's it. When a KeyPears client needs to interact with vaults at\n<code>example.com</code>, it fetches <code>https://example.com/.well-known/keypears.json</code>, reads\nthe <code>apiUrl</code>, and directs all API calls there.</p>\n<p>If you're running your own server, the <code>apiUrl</code> points to yourself:</p>\n<pre><code class=\"language-json\">{\n  \"version\": 1,\n  \"apiUrl\": \"https://example.com/api\"\n}\n</code></pre>\n<p>If you're using a third-party host like <code>keypears.com</code>:</p>\n<pre><code class=\"language-json\">{\n  \"version\": 1,\n  \"apiUrl\": \"https://keypears.com/api\"\n}\n</code></pre>\n<p>The pattern follows the established convention of <code>.well-known</code> files that power\neverything from SSL certificate validation (<code>.well-known/acme-challenge</code>) to\nsecurity contact information (<code>.well-known/security.txt</code>). It's a proven\napproach for domain-level configuration.</p>\n<h2>What We Built Today</h2>\n<p>This week we implemented the complete infrastructure for this feature:</p>\n<p><strong>In the library (<code>@keypears/lib</code>):</strong></p>\n<ul>\n<li>A Zod schema (<code>KeypearsJsonSchema</code>) that validates the <code>keypears.json</code> format</li>\n<li>A <code>buildBaseUrl()</code> helper for constructing domain URLs</li>\n</ul>\n<p><strong>In the API server (<code>@keypears/api-server</code>):</strong></p>\n<ul>\n<li>Updated <code>validateKeypearsServer()</code> to parse and return the <code>apiUrl</code></li>\n</ul>\n<p><strong>In the webapp (<code>@keypears/webapp</code>):</strong></p>\n<ul>\n<li>A dynamic React Router resource route that serves <code>keypears.json</code></li>\n<li>Environment-aware configuration (production vs development URLs)</li>\n</ul>\n<p><strong>In the Tauri app (<code>@keypears/tauri-ts</code>):</strong></p>\n<ul>\n<li><code>fetchApiUrl()</code> function that retrieves and caches API URLs from <code>keypears.json</code></li>\n<li>Updated all API client calls to use the discovered URL instead of constructing it</li>\n</ul>\n<p>The key insight is that clients no longer assume the API lives at\n<code>https://domain.com/api</code>. They discover it dynamically. This single change\nenables the entire third-party hosting model.</p>\n<h2>What's Still Needed</h2>\n<p>We want to be transparent: this is a proof-of-concept, not a production-ready\nfeature. There's a critical missing piece.</p>\n<p><strong>The problem:</strong> Right now, anyone could create a <code>keypears.json</code> file claiming\nthat <code>keypears.com/api</code> hosts vaults for <code>example.com</code>. There's no verification\nthat the owner of <code>example.com</code> actually authorized this.</p>\n<p><strong>The solution:</strong> Before launch, we'll add a public key (or public key hash) to\nthe <code>keypears.json</code> file. The domain owner will need to prove they control this\nkey, likely through a challenge-response protocol or by publishing the key in\nDNS. This cryptographic proof ensures that only the legitimate domain owner can\nauthorize a hosting provider.</p>\n<p>The future format might look like:</p>\n<pre><code class=\"language-json\">{\n  \"version\": 2,\n  \"apiUrl\": \"https://keypears.com/api\",\n  \"domainPubKeyHash\": \"a1b2c3d4...\"\n}\n</code></pre>\n<p>We haven't implemented this yet because the current proof-of-concept is\nsufficient for development and testing. The infrastructure is in place; the\nauthentication layer comes next.</p>\n<h2>The Bigger Picture</h2>\n<p>KeyPears is building toward a world where password management works like email\nshould have worked from the start: decentralized, interoperable, and encrypted\nby default.</p>\n<ul>\n<li><strong>Decentralized:</strong> No single company controls the network. Run your own server\nor choose a provider you trust.</li>\n<li><strong>Interoperable:</strong> <code>alice@keypears.com</code> can share secrets with\n<code>bob@selfhosted.org</code> seamlessly.</li>\n<li><strong>End-to-end encrypted:</strong> Servers are dumb coordinators. They never see your\npasswords, your keys, or your plaintext secrets.</li>\n<li><strong>Self-custody with convenience:</strong> You control your keys, but you get the sync\nand sharing features of cloud-based managers.</li>\n</ul>\n<p>The third-party hosting feature is a key piece of this puzzle. It means you\ndon't have to choose between running your own infrastructure and using someone\nelse's domain. You can have your cake and eat it too: your domain, your\nidentity, someone else's servers.</p>\n<h2>What's Next</h2>\n<p>With third-party hosting infrastructure in place, our next priority is the\nDiffie-Hellman key exchange protocol for secure secret sharing between users.\nThis is the feature that makes KeyPears more than just a password manager—it's\nwhat enables <code>alice@company.com</code> to securely share credentials with\n<code>bob@partner.io</code> without either server ever seeing the plaintext.</p>\n<p>After DH key exchange, we'll focus on:</p>\n<ul>\n<li>Multi-domain support (official KeyPears domains beyond <code>keypears.com</code>)</li>\n<li>Domain ownership verification (the public key piece mentioned above)</li>\n<li>Payment and business model (freemium with premium custom domain hosting)</li>\n</ul>\n<p>The architecture is coming together. Each piece we build makes the next piece\npossible. Today's proof-of-concept becomes tomorrow's production feature.</p>\n<p>If you're interested in following our progress, the code is open source and\navailable on <a href=\"https://github.com/keypears/keypears\">GitHub</a>. We're building in\npublic because we believe the best security software is software you can verify.</p>\n<p><em>Next up: Diffie-Hellman key exchange for cross-user secret sharing. Stay\ntuned!</em></p>",
      "url": "https://keypears.com/blog/2025-12-01-third-party-hosting",
      "title": "Third-Party Hosting: Making KeyPears as Easy as Hosted Email",
      "summary": "<p>Imagine you own <code>example.com</code>. You run your own website there, but you don't run\nyour own email server—Gmail or Fastmail handles that for you. Your email address\nis still <code>you@examp...",
      "date_modified": "2025-12-01T18:00:00.000Z",
      "author": {
        "name": "KeyPears Team"
      }
    },
    {
      "id": "https://keypears.com/blog/2025-11-30-cross-device-sync",
      "content_html": "<p>After weeks of intensive development, KeyPears now has a fully functional\ncross-device synchronization system. This wasn't just about making data appear\non multiple devices—it was about building a secure, privacy-preserving sync\narchitecture that works in a decentralized environment where users can run their\nown servers. Here's how we did it.</p>\n<h2>The Challenge</h2>\n<p>Building sync for a password manager is fundamentally different from typical app\nsynchronization. Every design decision has security implications. When you're\nalso committed to a decentralized architecture where users might run their own\nservers, the complexity multiplies. We needed to solve several interconnected\nproblems:</p>\n<ol>\n<li><strong>Authentication without centralization</strong> - No global user accounts or OAuth\nproviders</li>\n<li><strong>Device identity with privacy</strong> - Track devices without cross-domain\ncorrelation</li>\n<li><strong>Session management at scale</strong> - Support hundreds of API calls without\nexposing long-term credentials</li>\n<li><strong>Sync conflict resolution</strong> - Handle concurrent edits from multiple devices</li>\n<li><strong>Zero-knowledge architecture</strong> - Servers should never see passwords or\nencryption keys</li>\n</ol>\n<h2>The Solution: 2,000+ Lines of Carefully Crafted Code</h2>\n<p>Over the past week, we've implemented a comprehensive solution spanning 40 files\nwith over 2,000 lines of new code. Here's what we built:</p>\n<h3>1. Session-Based Authentication System</h3>\n<p>The biggest change was moving from a \"login key with every request\" model to\nproper session-based authentication. This reduced our attack surface\ndramatically:</p>\n<p><strong>Before:</strong> Login key sent 100+ times per session <strong>After:</strong> Login key sent once\nper 24 hours</p>\n<p>The new authentication flow works like this:</p>\n<pre><code class=\"language-typescript\">// Login once per day\nconst { sessionToken, expiresAt } = await client.login({\n  vaultId,\n  loginKey,\n  deviceId,\n  deviceDescription: \"macOS 14.1 (aarch64)\"\n});\n\n// Use session token for all subsequent requests\nconst secrets = await client.getSecretUpdates({\n  vaultId,\n  lastUpdatedAt: lastSync\n});\n</code></pre>\n<p>But here's the security innovation: we store session tokens as Blake3 hashes in\nthe database. Even if someone breaches the server database, they can't use the\nstolen hashes—they'd need the original 32-byte random tokens, which exist only\nin client memory.</p>\n<h3>2. Privacy-Preserving Device Tracking</h3>\n<p>Unlike centralized password managers that assign global device IDs, KeyPears\ngenerates a unique device ID for each vault. This means:</p>\n<ul>\n<li>Your work vault and personal vault have different device IDs</li>\n<li>Servers can't correlate devices across domains</li>\n<li>Complete privacy preservation in a decentralized architecture</li>\n</ul>\n<p>Each device gets identified with:</p>\n<ul>\n<li>A ULID (Universally Unique Lexicographically Sortable Identifier) per vault</li>\n<li>Auto-detected OS information: \"iPhone (iOS 17.2)\" or \"Windows 11 (x86_64)\"</li>\n<li>User-editable friendly names: \"Ryan's MacBook Pro\"</li>\n</ul>\n<h3>3. Background Sync Service</h3>\n<p>We built a robust background synchronization service that polls for changes\nevery 5 seconds:</p>\n<pre><code class=\"language-typescript\">// Start sync when vault is unlocked\nstartBackgroundSync(vaultId, vaultDomain, vaultKey, getSession);\n\n// Automatic sync every 5 seconds\n// Manual sync after creating/editing secrets\nawait triggerManualSync();\n</code></pre>\n<p>The sync service includes sophisticated error handling:</p>\n<ul>\n<li><strong>401 Unauthorized</strong>: Stop syncing, prompt for re-authentication</li>\n<li><strong>500+ Server Error</strong>: Exponential backoff (5s → 10s → 20s)</li>\n<li><strong>Network Error</strong>: Keep retrying at normal interval</li>\n<li><strong>Session Expiring</strong>: Skip sync, avoid unnecessary 401s</li>\n</ul>\n<h3>4. Three-Tier Key Derivation System</h3>\n<p>We implemented a sophisticated key hierarchy that separates authentication from\nencryption:</p>\n<pre><code>Master Password\n    ↓ (100k rounds PBKDF)\nPassword Key (cached, PIN-encrypted)\n    ├→ Encryption Key (device only, decrypts vault)\n    └→ Login Key (sent to server for auth)\n        ↓ (1k rounds on server)\n    Hashed Login Key (database storage)\n</code></pre>\n<p>This asymmetric round count (100k client, 1k server) is intentional—the heavy\ncomputation happens client-side for security, while the server just needs to\nprevent raw token storage.</p>\n<h3>5. Comprehensive Test Coverage</h3>\n<p>We didn't just write code; we wrote tests. Lots of them:</p>\n<ul>\n<li><strong>283 lines</strong> of authentication tests</li>\n<li><strong>278 lines</strong> of device session tests</li>\n<li>Integration tests for the complete sync flow</li>\n<li>Database migration tests for schema changes</li>\n</ul>\n<h2>Implementation Highlights</h2>\n<h3>React Closure Bug Fix</h3>\n<p>One of the trickiest bugs involved React closures capturing stale state. The\nsync service was always getting a <code>null</code> session token because the closure\ncaptured the initial value:</p>\n<pre><code class=\"language-typescript\">// BUG: Closure captures initial null value\nstartBackgroundSync(vaultId, domain, key, () => session?.token);\n\n// FIX: Use ref to get current value\nconst sessionRef = useRef(session);\nuseEffect(() => { sessionRef.current = session; }, [session]);\nstartBackgroundSync(vaultId, domain, key, () => sessionRef.current?.token);\n</code></pre>\n<h3>Tauri Plugin Integration</h3>\n<p>We integrated Tauri's OS detection plugin to automatically identify devices:</p>\n<pre><code class=\"language-rust\">// Rust side\n.plugin(tauri_plugin_os::init())\n\n// TypeScript side\nimport { platform, version, arch } from \"@tauri-apps/plugin-os\";\nconst description = `${platform()} ${version()} (${arch()})`;\n</code></pre>\n<h3>Database Schema Evolution</h3>\n<p>Added a new <code>device_session</code> table with careful constraints:</p>\n<pre><code class=\"language-sql\">CREATE TABLE device_session (\n  id TEXT PRIMARY KEY,           -- ULID\n  vault_id TEXT NOT NULL,\n  device_id TEXT NOT NULL,        -- Per-vault device ULID\n  hashed_session_token TEXT,      -- Blake3 hash, not raw token\n  expires_at INTEGER NOT NULL,\n  last_activity INTEGER NOT NULL,\n  UNIQUE(vault_id, device_id),    -- One session per device\n  FOREIGN KEY(vault_id) REFERENCES vault(id) ON DELETE CASCADE\n);\n</code></pre>\n<h2>Security Improvements</h2>\n<p>The new system dramatically improves our security posture:</p>\n<p>| Attack Vector         | Before             | After                  |\n| --------------------- | ------------------ | ---------------------- |\n| Token Interception    | Permanent access   | 24-hour maximum window |\n| Database Breach       | Login keys exposed | Only unusable hashes   |\n| Device Compromise     | No revocation      | Per-device logout      |\n| Replay Attacks        | Vulnerable         | Time-limited tokens    |\n| Cross-Domain Tracking | Possible           | Prevented by design    |</p>\n<h2>Performance Optimizations</h2>\n<p>Beyond security, we optimized for performance:</p>\n<ol>\n<li><strong>Smart Polling</strong>: Only sync when there's an active session</li>\n<li><strong>Exponential Backoff</strong>: Reduce server load during errors</li>\n<li><strong>Debounced Updates</strong>: Batch rapid changes together</li>\n<li><strong>Selective Sync</strong>: Only fetch changes since last sync timestamp</li>\n</ol>\n<h2>What's Next</h2>\n<p>With cross-device sync complete, KeyPears is approaching feature parity with\ncentralized password managers—while maintaining its decentralized,\nprivacy-preserving architecture. The foundation we've built enables future\nfeatures like:</p>\n<ul>\n<li>Multi-factor authentication (MFA)</li>\n<li>Device trust levels and approval workflows</li>\n<li>Geographic anomaly detection</li>\n<li>Granular access controls</li>\n<li>Offline-first mobile apps</li>\n</ul>\n<h2>Technical Details</h2>\n<p>For the curious, here's the full scope of changes:</p>\n<ul>\n<li><strong>2,018 lines</strong> of code across <strong>40 files</strong></li>\n<li><strong>13 new API endpoints</strong> for authentication and sync</li>\n<li><strong>2 new database tables</strong> with migration scripts</li>\n<li><strong>166 lines</strong> of sync service implementation</li>\n<li><strong>Test coverage</strong> for all critical paths</li>\n</ul>\n<p>The complete implementation is open source and available in our\n<a href=\"https://github.com/keypears/keypears\">GitHub repository</a>.</p>\n<h2>Conclusion</h2>\n<p>Building secure cross-device sync for a decentralized password manager required\nrethinking traditional approaches. By combining session-based authentication,\nprivacy-preserving device tracking, and sophisticated key derivation, we've\ncreated a system that's both secure and user-friendly.</p>\n<p>The key insight: security doesn't require sacrificing usability or privacy. With\ncareful architecture and attention to detail, we can build systems that protect\nusers without compromising their autonomy.</p>\n<p>KeyPears now syncs your passwords across all your devices—instantly, securely,\nand privately. Whether you're using our hosted service or running your own\nserver, your secrets stay yours.</p>\n<p><em>Next up: Implementing the Diffie-Hellman key exchange protocol for secure\nsecret sharing between users. Stay tuned!</em></p>",
      "url": "https://keypears.com/blog/2025-11-30-cross-device-sync",
      "title": "Building Secure Cross-Device Sync for a Decentralized Password Manager",
      "summary": "<p>After weeks of intensive development, KeyPears now has a fully functional\ncross-device synchronization system. This wasn't just about making data appear\non multiple devices—it was about building a ...",
      "date_modified": "2025-12-01T00:00:00.000Z",
      "author": {
        "name": "KeyPears Team"
      }
    },
    {
      "id": "https://keypears.com/blog/2025-11-16-typescript-for-mvp",
      "content_html": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and\ncryptocurrency wallet. The design decisions described here represent our\ndevelopment approach and may evolve before our official release.</p>\n<p>Three weeks ago, we published a blog post titled \"Building KeyPears with Rust:\nBackend Architecture and Blake3 Proof-of-Concept.\" We were excited about Rust's\nperformance, memory safety, and type system. We had a working <code>/api/blake3</code>\nendpoint. We had plans for <code>rs-lib</code> and <code>rs-node</code> packages.</p>\n<p>Today, we're writing to tell you we've changed direction.</p>\n<p>The current KeyPears codebase is <strong>almost entirely TypeScript</strong>. The Rust\nbackend from our October post—<code>rs-lib</code> for cryptography and <code>rs-node</code> for the\nAPI server—was fully built and working. And then we deleted it. After several\nweeks of development with both implementations side by side, we concluded that\nTypeScript is the right architecture for our MVP.</p>\n<p>This post explains why we made that decision.</p>\n<h2>What Actually Happened</h2>\n<p>Let's start with the facts. Here's what we did:</p>\n<p><strong>October 2025:</strong> Built a complete Rust backend</p>\n<ul>\n<li><code>rs-lib</code>: Full cryptography library (Blake3, ACB3, key derivation)</li>\n<li><code>rs-node</code>: Axum-based API server with OpenAPI via utoipa</li>\n<li>Dual-server deployment: Node.js webapp proxying to Rust API</li>\n<li>Everything worked as described in the October blog post</li>\n</ul>\n<p><strong>November 2025:</strong> Removed the entire Rust backend</p>\n<ul>\n<li>Deleted <code>rs-lib</code> package completely</li>\n<li>Deleted <code>rs-node</code> package completely</li>\n<li>Rewrote cryptography in TypeScript using <code>@webbuf</code> WASM packages</li>\n<li>Rewrote API server in TypeScript using orpc</li>\n<li>Integrated API server directly into Express webapp</li>\n</ul>\n<p><strong>Current state:</strong></p>\n<ul>\n<li><strong>Rust code:</strong> 33 lines total (just the minimal Tauri shell)</li>\n<li><strong>TypeScript code:</strong> ~5,400 lines (lib, api-server, tauri app, webapp)</li>\n<li>All cryptography now TypeScript + WASM</li>\n<li>All API endpoints now orpc (TypeScript RPC)</li>\n<li>Single-server deployment (no more Node → Rust proxy)</li>\n</ul>\n<p>This wasn't a case of the Rust backend \"not working out.\" It worked perfectly.\nWe had working Blake3 hashing, working key derivation, working API endpoints. We\ndeleted it anyway because <strong>TypeScript simplifies development in ways that\nmatter more than Rust's advantages for our MVP.</strong></p>\n<h2>Why We Removed the Rust Backend</h2>\n<p>With both implementations working, we had to make a choice: continue maintaining\ntwo parallel implementations (Rust for crypto/API, TypeScript for UI) or\nconsolidate on one language. We chose TypeScript for three critical reasons:</p>\n<p><strong>1. Better API tooling</strong> - orpc provides superior type safety compared to\nAxum + utoipa + openapi-generator</p>\n<p><strong>2. Better database tooling</strong> - Drizzle ORM supports both SQLite and PostgreSQL\nwith the same API (no Rust equivalent exists)</p>\n<p><strong>3. Single-language simplicity</strong> - Avoiding context switching between Rust and\nTypeScript saves mental overhead on a side project</p>\n<p>Here's what we learned by building and then removing the Rust backend:</p>\n<h3>1. orpc vs Axum + utoipa: Type Safety Without Codegen</h3>\n<p>We built the Rust API server with Axum and <code>utoipa</code> for OpenAPI generation. It\nworked, but the workflow had friction:</p>\n<p><strong>The Rust approach we actually used:</strong></p>\n<ol>\n<li>Define routes in Rust with Axum</li>\n<li>Generate OpenAPI spec with <code>utoipa</code> macros</li>\n<li>Run <code>openapi-generator</code> to create TypeScript client</li>\n<li>Discover generated client doesn't match our TypeScript patterns</li>\n<li>Manually adjust generated code or fix Rust annotations</li>\n<li>Repeat on every schema change</li>\n</ol>\n<p><strong>The TypeScript approach (orpc) we switched to:</strong></p>\n<pre><code class=\"language-typescript\">// Define the procedure\nexport const blake3Procedure = os\n  .input(Blake3RequestSchema)\n  .output(Blake3ResponseSchema)\n  .handler(async ({ input }) => {\n    const data = WebBuf.fromBase64(input.data);\n    const hash = blake3Hash(data);\n    return { hash: hash.buf.toHex() };\n  });\n\n// Use it in the client with full type safety\nconst client = createClient({ url: \"/api\" });\nconst result = await client.blake3({ data: \"...\" });\n// TypeScript knows `result.hash` is a string\n</code></pre>\n<p><strong>Zero codegen. Complete type safety. Instant IDE autocomplete.</strong></p>\n<p>The difference is night and day. With orpc, the client knows every endpoint,\nevery parameter type, every response shape—all inferred directly from the server\ncode. Change the server? Client errors appear immediately in your IDE, not at\nruntime. No build step, no generated files, no version mismatches.</p>\n<p>This is what made us delete working Rust code. The Axum + utoipa + codegen\nworkflow worked, but orpc's zero-codegen type safety is so much better that\nmaintaining the Rust version wasn't worth it.</p>\n<h3>2. No Rust ORM Supports Both SQLite and PostgreSQL Well</h3>\n<p>KeyPears needs two databases:</p>\n<ul>\n<li><strong>SQLite</strong> in the Tauri desktop app (client-side storage)</li>\n<li><strong>PostgreSQL</strong> on the server (multi-user vault synchronization)</li>\n</ul>\n<p>In TypeScript, <strong>Drizzle ORM</strong> handles both with the same API:</p>\n<pre><code class=\"language-typescript\">// Client (SQLite)\nimport { drizzle } from \"drizzle-orm/sqlite-proxy\";\nconst db = drizzle(/* Tauri SQL plugin */);\n\n// Server (PostgreSQL)\nimport { drizzle } from \"drizzle-orm/node-postgres\";\nconst db = drizzle(/* pg connection */);\n\n// Same schema definition works for both\nexport const TableVault = sqliteTable(\"vault\", {\n  id: text(\"id\").primaryKey(),\n  name: text(\"name\").notNull(),\n  // ...\n});\n</code></pre>\n<p>We looked for Rust equivalents. <strong>Diesel</strong> supports Postgres and MySQL but has\npoor SQLite support. <strong>SeaORM</strong> is newer but still requires separate schema\ndefinitions for different databases. Neither provides the unified, type-safe\nquery builder that Drizzle gives us.</p>\n<p>When you're building a sync protocol where the client and server need matching\nschemas, having one ORM that works everywhere is critical. This was the second\nreason we deleted the Rust backend—we would have needed two separate database\nimplementations (one for Tauri's SQLite, one for the server's Postgres) with\nmanual work to keep them in sync.</p>\n<h3>3. Single Language Reduces Mental Overhead</h3>\n<p>The final reason we removed the Rust backend: <strong>context switching costs.</strong></p>\n<p>With the dual-language architecture, every feature required:</p>\n<ul>\n<li>Writing Rust for crypto/API logic</li>\n<li>Writing TypeScript for UI/database logic</li>\n<li>Translating between Rust and TypeScript idioms</li>\n<li>Maintaining two build systems (Cargo + pnpm)</li>\n<li>Debugging across language boundaries</li>\n<li>Different testing frameworks (Cargo test + Vitest)</li>\n</ul>\n<p>For a side project where development happens in short evening sessions, this\nmental overhead compounds. You spend the first 10 minutes remembering whether\nyou're writing Rust or TypeScript, and the last 10 minutes before bed context\nswitching back.</p>\n<p>With TypeScript-only:</p>\n<ul>\n<li>One type system</li>\n<li>One package manager</li>\n<li>One testing framework</li>\n<li>One set of idioms</li>\n<li>Hot reload in ~100ms (vs 3-10s Rust recompile)</li>\n</ul>\n<p>The productivity gain isn't just about compile times. It's about flow state.\nWhen you're not context switching between languages, you write more code and\nmake fewer mistakes.</p>\n<h3>4. Deployment Simplification</h3>\n<p>The Rust backend also complicated deployment:</p>\n<p><strong>With Rust (October architecture):</strong></p>\n<ul>\n<li>Dual-server: Node.js webapp (port 4273) + Rust API (port 4274)</li>\n<li>HTTP proxy from webapp to Rust server</li>\n<li>Docker image: Node + Rust toolchain + cross-compilation</li>\n<li>Larger image size (~500MB with Rust)</li>\n<li>More complex service coordination</li>\n</ul>\n<p><strong>With TypeScript-only (current):</strong></p>\n<ul>\n<li>Single Express server (port 4273)</li>\n<li>orpc API mounted directly at <code>/api</code></li>\n<li>Docker image: Just Node.js (~200MB)</li>\n<li>Simpler deployment (one service, one port)</li>\n<li>No HTTP proxy overhead</li>\n</ul>\n<p>Removing the Rust backend made deployment cleaner and faster.</p>\n<h2>What We Didn't Lose: Rust Cryptography via WASM</h2>\n<p>Here's the critical insight that made removing the Rust backend viable: <strong>We\nstill use Rust for cryptography. We just use it through WebAssembly instead of\nwriting it ourselves.</strong></p>\n<p>When we deleted <code>rs-lib</code> (our Rust cryptography library), we didn't rewrite\ncrypto in pure JavaScript. We switched to the <code>@webbuf</code> packages, which compile\nRust cryptography to WebAssembly:</p>\n<ul>\n<li><strong><code>@webbuf/blake3</code></strong>: Blake3 hashing (Rust → WASM)</li>\n<li><strong><code>@webbuf/acb3</code></strong>: AES-256-CBC + Blake3-MAC (Rust → WASM)</li>\n<li><strong><code>@webbuf/webbuf</code></strong>: Binary data utilities (Rust → WASM)</li>\n<li><strong><code>@webbuf/fixedbuf</code></strong>: Fixed-size buffers (Rust → WASM)</li>\n</ul>\n<p>These packages compile Rust cryptography to WebAssembly. We get:</p>\n<p>✅ <strong>Rust's memory safety</strong> (WASM sandbox) ✅ <strong>Rust's performance</strong>\n(near-native speed) ✅ <strong>Cross-platform consistency</strong> (works in Node, browsers,\nTauri) ✅ <strong>TypeScript ergonomics</strong> (native <code>Uint8Array</code> integration)</p>\n<p>Here's our complete three-tier key derivation system in TypeScript:</p>\n<pre><code class=\"language-typescript\">// 100,000 rounds of Blake3-based PBKDF\nexport function blake3Pbkdf(\n  password: string | WebBuf,\n  salt: FixedBuf&#x3C;32>,\n  rounds: number = 100_000,\n): FixedBuf&#x3C;32> {\n  const passwordBuf = typeof password === \"string\"\n    ? WebBuf.fromUtf8(password)\n    : password;\n\n  let result = blake3Mac(salt, passwordBuf);\n  for (let i = 1; i &#x3C; rounds; i++) {\n    result = blake3Mac(salt, result.buf);\n  }\n  return result;\n}\n\n// Derive password key from user's master password\nexport function derivePasswordKey(password: string): FixedBuf&#x3C;32> {\n  const salt = derivePasswordSalt(password);\n  return blake3Pbkdf(password, salt, 100_000);\n}\n\n// Derive encryption key (for vault data)\nexport function deriveEncryptionKey(passwordKey: FixedBuf&#x3C;32>): FixedBuf&#x3C;32> {\n  const salt = deriveEncryptionSalt();\n  return blake3Pbkdf(passwordKey.buf, salt, 100_000);\n}\n\n// Derive login key (sent to server)\nexport function deriveLoginKey(passwordKey: FixedBuf&#x3C;32>): FixedBuf&#x3C;32> {\n  const salt = deriveLoginSalt();\n  return blake3Pbkdf(passwordKey.buf, salt, 100_000);\n}\n</code></pre>\n<p>This is production-ready cryptography. It's type-safe. It's fast (200,000 Blake3\noperations complete in milliseconds). And the actual hashing happens in\nRust-compiled WASM—the same Rust cryptography we had in <code>rs-lib</code>, just\npackaged differently.</p>\n<p><strong>We didn't abandon Rust's security properties. We just stopped maintaining our\nown Rust codebase.</strong> The cryptography is still Rust. It's just compiled to WASM\nand consumed as TypeScript packages, which eliminates the build complexity of a\ndual-language project.</p>\n<h2>The Architecture That Emerged</h2>\n<p>Here's what the current KeyPears stack looks like:</p>\n<h3>Package Structure</h3>\n<pre><code>@keypears/lib (TypeScript)\n├── Blake3 hashing via @webbuf/blake3 (Rust→WASM)\n├── ACB3 encryption via @webbuf/acb3 (Rust→WASM)\n├── Three-tier key derivation (100k rounds each)\n├── Password generation with entropy calculation\n└── Zod schemas for validation\n\n@keypears/api-server (TypeScript)\n├── orpc router with type-safe procedures\n├── Blake3 endpoint (working proof-of-concept)\n├── Drizzle ORM + PostgreSQL schema (ready for server DB)\n└── Client factory for end-to-end type safety\n\nkeypears-tauri (TypeScript + Rust shell)\n├── Tauri 2.0 app (33 lines of Rust)\n├── Full vault management UI (~5,020 lines TypeScript)\n├── SQLite with Drizzle ORM\n├── React Router 7 for navigation\n├── Shadcn components + Catppuccin theme\n└── Calls production API server for crypto endpoints\n\n@keypears/webapp (TypeScript)\n├── Production website + blog\n├── Integrated API server (orpc mounted at /api)\n├── Single Express server on port 4273\n└── Deployed on AWS Fargate\n</code></pre>\n<h3>What Works Today</h3>\n<p>The Tauri app has a complete vault management workflow:</p>\n<p>✅ Create vault with password ✅ Unlock vault with password verification ✅\nStore passwords with encryption ✅ Generate secure passwords ✅ SQLite\npersistence via Drizzle ✅ Three-tier key derivation working ✅ Vault encryption\nwith ACB3 ✅ Multi-step wizards (name → password → confirm → success) ✅ Test\npage calling production Blake3 API</p>\n<p>The webapp has:</p>\n<p>✅ Landing page with blog system ✅ Working <code>/api/blake3</code> endpoint ✅ orpc\nintegrated with Express ✅ Docker deployment to AWS Fargate ✅ Canonical URL\nredirects ✅ Blog posts with TOML frontmatter + Markdown</p>\n<h3>What's Not Built (Intentionally Deferred)</h3>\n<p>We haven't built server-side features yet because the MVP is <strong>local-first</strong>:</p>\n<ul>\n<li>⏸️ User authentication (login/logout)</li>\n<li>⏸️ Vault synchronization protocol</li>\n<li>⏸️ Multi-user server support</li>\n<li>⏸️ Diffie-Hellman key exchange across domains</li>\n<li>⏸️ Public key infrastructure</li>\n</ul>\n<p>These are v2 features. The MVP is a password manager that works 100% offline in\nthe Tauri app. The server is only needed for multi-device sync, which we'll add\nafter validating the core product.</p>\n<h2>The TypeScript Ecosystem Has Caught Up</h2>\n<p>Five years ago, this blog post would have been different. Rust was the only way\nto get type-safe backends with good performance. But the TypeScript ecosystem\nhas evolved dramatically:</p>\n<p><strong>orpc</strong> gives us end-to-end type safety that Rust can't match (no codegen,\ninstant IDE feedback)</p>\n<p><strong>Drizzle</strong> provides type-safe SQL for both SQLite and PostgreSQL (no Rust ORM\ndoes this well)</p>\n<p><strong>WASM</strong> lets us use Rust crypto without writing Rust applications (best of both\nworlds)</p>\n<p><strong>Vitest</strong> gives us fast ESM-native testing (simpler than Cargo's test framework\nfor web apps)</p>\n<p><strong>React Router 7</strong> provides SSR + type-safe routing (no Rust equivalent)</p>\n<p>For building web applications with cryptography, TypeScript + WASM is now a\nbetter choice than native Rust. You get comparable performance, better tooling,\nand a much larger ecosystem of web-focused libraries.</p>\n<h2>When Would We Use Rust?</h2>\n<p>This isn't a rejection of Rust. It's a recognition that <strong>Rust solves the wrong\nproblems for our MVP.</strong></p>\n<p>Rust makes sense when you need:</p>\n<ol>\n<li><strong>Extreme performance</strong> - Handling 10k+ concurrent WebSocket connections</li>\n<li><strong>Embedded systems</strong> - Running on IoT devices with 64MB of RAM</li>\n<li><strong>Custom crypto</strong> - Implementing novel cryptographic algorithms</li>\n<li><strong>Kernel-level code</strong> - Writing device drivers or OS components</li>\n</ol>\n<p>KeyPears doesn't need any of these yet. Our server will handle dozens of\nconcurrent users, not thousands. Our desktop app runs on modern laptops with\ngigabytes of RAM. Our cryptography comes from well-tested libraries (Blake3,\nAES-256). We're building a user-facing application, not infrastructure.</p>\n<p><strong>Later, Rust might make sense for:</strong></p>\n<ul>\n<li>High-throughput sync server (if we grow to enterprise scale)</li>\n<li>Mobile performance optimization (if WASM proves too slow)</li>\n<li>Custom Diffie-Hellman implementation (if existing libraries don't fit)</li>\n</ul>\n<p>But even then, we'd keep the API layer in TypeScript (orpc is too good to give\nup) and only move performance-critical sync logic to Rust via FFI.</p>\n<h2>The Right Tool for the Right Job</h2>\n<p>Software architecture isn't about using the \"best\" language—it's about using the\nright tool for the constraints you're facing.</p>\n<p>Our constraints:</p>\n<ul>\n<li><strong>Side project timeline</strong>: Limited evening/weekend hours</li>\n<li><strong>Solo developer</strong>: No team to split Rust vs TypeScript work</li>\n<li><strong>MVP goal</strong>: Prove the concept before scaling</li>\n<li><strong>Rapid iteration</strong>: Features change based on user feedback</li>\n</ul>\n<p>For these constraints, TypeScript is objectively better:</p>\n<ul>\n<li>Faster iteration (100ms hot reload vs 5s compile)</li>\n<li>Single mental model (no context switching)</li>\n<li>Richer ecosystem (orpc, Drizzle, React Router)</li>\n<li>Lower cognitive overhead (one type system, one package manager)</li>\n</ul>\n<p>We still get Rust's security properties through WASM. We still get type safety\nthrough TypeScript. We still get performance (crypto is WASM, API is fast\nenough).</p>\n<h2>What We Learned</h2>\n<p><strong>1. Working code isn't always the right code</strong></p>\n<p>The Rust backend worked perfectly. Blake3 hashing worked. Key derivation worked.\nThe API server worked. We shipped it to production. But \"working\" doesn't mean\n\"optimal for the constraints.\" When we evaluated developer experience vs\nperformance gains, TypeScript won decisively for our MVP.</p>\n<p><strong>2. Ecosystem maturity matters more than language performance</strong></p>\n<p>The Rust language is excellent. But for web applications, the TypeScript\necosystem is years ahead. orpc's zero-codegen type safety is revolutionary.\nDrizzle's unified SQLite + Postgres support is essential for our architecture.\nThese don't exist in Rust.</p>\n<p><strong>3. WASM changes the game</strong></p>\n<p>Ten years ago, you had to choose: safe languages (Ruby, Python, JavaScript) or\nfast languages (C, C++, Rust). Today, you can write your performance-critical\ncode in Rust, compile it to WASM, and use it from any language. This is what\nmade deleting our Rust backend viable—we didn't lose Rust's performance, we just\nstopped writing it ourselves.</p>\n<p><strong>4. Deleting working code is liberating</strong></p>\n<p>We spent weeks building <code>rs-lib</code> and <code>rs-node</code>. They worked. They were deployed.\nAnd we deleted them anyway because the TypeScript alternative was better for our\nconstraints. This felt wrong at first—\"But we already built it!\"—but the\nproductivity gain from consolidating on one language was immediate and\nsubstantial.</p>\n<p><strong>5. Side project constraints are different</strong></p>\n<p>If KeyPears were a VC-funded startup with a team of 5 engineers, we'd keep the\nRust backend. Someone could own the Rust API while others work on the TypeScript\nUI. But for a solo side project with limited evening/weekend hours, the mental\noverhead of context switching between Rust and TypeScript was too high. One\nlanguage means more velocity.</p>\n<h2>The Current Priority: Shipping the MVP</h2>\n<p>With this architecture decision settled, we're focused on shipping a working\nproduct:</p>\n<p><strong>Next milestones:</strong></p>\n<ol>\n<li><strong>Server vault CRUD</strong> - Create/read/update vaults via API</li>\n<li><strong>User authentication</strong> - Session-based login with hashed login key</li>\n<li><strong>Basic sync protocol</strong> - Last-write-wins synchronization</li>\n<li><strong>Mobile Tauri build</strong> - iOS + Android apps</li>\n<li><strong>Import/export</strong> - Backup and restore vaults</li>\n</ol>\n<p>All of this will be TypeScript. The API server will use orpc. The database will\nuse Drizzle (Postgres on server, SQLite on clients). The cryptography will\nremain Rust-compiled WASM.</p>\n<p>And if we're wrong—if we hit performance walls or need Rust for specific\nfeatures—we can always add Rust modules later. The architecture supports it. But\nwe're not starting there.</p>\n<h2>Try It Yourself</h2>\n<p>The Blake3 endpoint is live:</p>\n<pre><code class=\"language-bash\">curl -X POST https://keypears.com/api/blake3 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"data\": \"SGVsbG8sIEtleVBlYXJzIQ==\"}'\n</code></pre>\n<p>That <code>data</code> field is base64-encoded \"Hello, KeyPears!\". The API will return the\nBlake3 hash computed by Rust (via WASM) running in Node.js on our TypeScript\nserver.</p>\n<p>It's a small proof-of-concept, but it validates the entire architecture:\nTypeScript for the API layer, Rust-via-WASM for cryptography, type safety\nend-to-end.</p>\n<h2>Conclusion</h2>\n<p>We built a Rust backend. It worked. We deployed it. And then we deleted it.</p>\n<p>This wasn't a failure of Rust or a mistake in architecture. It was a deliberate\nchoice to optimize for <strong>developer velocity over theoretical performance</strong> at\nthis stage of the project. The Rust backend would have been fine for production,\nbut the TypeScript backend is better for rapid MVP development.</p>\n<p>We're building KeyPears with <strong>TypeScript + WASM</strong>, which gives us Rust's\nsecurity properties (via WASM crypto) without the complexity of maintaining a\ndual-language codebase.</p>\n<p>For a solo side project with MVP goals, this is the right architecture. If we\nscale to millions of users and need extreme performance, we can always bring\nRust back for specific hot paths. But we're not starting there.</p>\n<p>Rust is an incredible language. We proved that by building a working backend\nwith it. But for this project, at this stage, TypeScript is the pragmatic\nchoice—and we're comfortable deleting working Rust code to prove it.</p>\n<p>We'll keep sharing our progress—both the wins and the pivots. If you're\ninterested in following along:</p>\n<ul>\n<li><strong>Live demo</strong>: Try the Blake3 endpoint at https://keypears.com/api/blake3</li>\n<li><strong>Source code</strong>: Coming soon on GitHub under Apache 2.0 license</li>\n</ul>\n<p>More updates coming soon. Next post: Implementing the vault synchronization\nprotocol.</p>",
      "url": "https://keypears.com/blog/2025-11-16-typescript-for-mvp",
      "title": "TypeScript for the KeyPears MVP: Why We're Not Really Using Rust (Yet)",
      "summary": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and\ncryptocurrency wallet. The design decisions described here represent our\ndevelopment approach and may evolve b...",
      "date_modified": "2025-11-16T12:00:00.000Z",
      "author": {
        "name": "KeyPears Team"
      }
    },
    {
      "id": "https://keypears.com/blog/2025-10-25-rust-backend-architecture",
      "content_html": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and cryptocurrency wallet. The design decisions described here represent our development approach and may evolve before our official release.</p>\n<p>We're excited to share a major architectural milestone: KeyPears now has a working Rust backend with our first proof-of-concept endpoint. This marks a significant shift in our technical approach, bringing the performance, security, and cross-platform benefits of Rust to our core cryptography and API layer.</p>\n<h2>Why Rust for the Backend?</h2>\n<p>When we started building KeyPears, we knew cryptography and security would be central to everything we do. After evaluating different approaches, we chose to build our backend entirely in Rust for several compelling reasons:</p>\n<h3>Performance</h3>\n<p>Cryptographic operations—hashing, encryption, key derivation—are CPU-intensive. Rust's zero-cost abstractions and lack of garbage collection mean we can achieve performance comparable to C/C++ without sacrificing safety. For operations users will perform thousands of times (encrypting secrets, computing hashes, deriving keys), this performance matters.</p>\n<h3>Memory Safety</h3>\n<p>Password managers and cryptocurrency wallets are high-value targets for attackers. Rust's ownership system and borrow checker eliminate entire classes of vulnerabilities at compile time:</p>\n<ul>\n<li>No buffer overflows</li>\n<li>No use-after-free bugs</li>\n<li>No data races in concurrent code</li>\n<li>No null pointer dereferences</li>\n</ul>\n<p>These guarantees mean our cryptographic code has fewer attack surfaces by design.</p>\n<h3>Cross-Platform Consistency</h3>\n<p>KeyPears needs to run everywhere: Windows, macOS, Linux, Android, and iOS. Rust compiles to native code on all these platforms with consistent behavior. The same cryptographic library (<code>rs-lib</code>) that powers our server also powers our Tauri desktop app and will eventually power our mobile apps.</p>\n<p>This eliminates the \"works on my machine\" problem and ensures that a secret encrypted on iOS can be decrypted on Windows with identical cryptographic operations.</p>\n<h3>Strong Type System</h3>\n<p>Rust's type system helps us encode security invariants at compile time. For example, we can use the type system to ensure that:</p>\n<ul>\n<li>Encryption keys are never accidentally logged or serialized</li>\n<li>Sensitive data is properly zeroed after use</li>\n<li>API responses match their OpenAPI specifications exactly</li>\n</ul>\n<p>This compile-time verification catches bugs before they reach production.</p>\n<h2>Architecture Overview</h2>\n<p>Our Rust backend consists of two main packages:</p>\n<h3><code>rs-lib</code>: Core Cryptography Library</h3>\n<p><code>rs-lib</code> is a shared Rust library containing all our cryptographic implementations:</p>\n<ul>\n<li><strong>Blake3</strong>: Fast, secure hashing and key derivation</li>\n<li><strong>ACB3</strong>: AES-256-CBC + Blake3-MAC for authenticated encryption</li>\n<li><strong>Key derivation</strong>: Three-tier system separating authentication from encryption</li>\n<li><strong>Data structures</strong>: Core types for vaults, secrets, and synchronization</li>\n</ul>\n<p>This library is pure Rust with no external dependencies beyond well-audited cryptography crates. It's designed to be portable and reusable across all our platforms.</p>\n<h3><code>rs-node</code>: KeyPears Node (API Server)</h3>\n<p><code>rs-node</code> is our API server—what we call a \"KeyPears node.\" It uses the Axum web framework to expose REST endpoints that clients can use for cryptographic operations and vault synchronization.</p>\n<p>Key features:</p>\n<ul>\n<li><strong>Axum framework</strong>: Modern, type-safe HTTP server from the Tokio team</li>\n<li><strong>OpenAPI 3.0</strong>: Full API specification generated from Rust code using <code>utoipa</code></li>\n<li><strong>Swagger UI</strong>: Interactive API documentation at <code>/api/docs</code></li>\n<li><strong>Type safety</strong>: Request/response types validated at compile time</li>\n</ul>\n<p>The node is designed to be self-hostable. Anyone can run their own KeyPears node for full sovereignty over their data.</p>\n<h2>Blake3 Proof-of-Concept</h2>\n<p>Our first working endpoint is a Blake3 hashing service at <code>/api/blake3</code>. You can try it right now:</p>\n<pre><code class=\"language-bash\">curl -X POST https://keypears.com/api/blake3 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"data\": \"Hello, KeyPears!\"}'\n</code></pre>\n<p>This returns:</p>\n<pre><code class=\"language-json\">{\n  \"hash\": \"a1b2c3d4...\"\n}\n</code></pre>\n<p>Blake3 is our hashing algorithm of choice for KeyPears. It's:</p>\n<ul>\n<li><strong>Fast</strong>: Significantly faster than SHA-256 or SHA-3</li>\n<li><strong>Secure</strong>: 256-bit security with no known attacks</li>\n<li><strong>Versatile</strong>: Works as both a hash function and a key derivation function</li>\n<li><strong>Modern</strong>: Designed in 2020 with modern CPU features in mind</li>\n</ul>\n<p>We use Blake3 throughout KeyPears:</p>\n<ul>\n<li>Deriving encryption keys from passwords</li>\n<li>Generating message authentication codes (MACs)</li>\n<li>Computing content hashes for deduplication</li>\n<li>Creating deterministic IDs</li>\n</ul>\n<p>This proof-of-concept demonstrates the full stack working:</p>\n<ol>\n<li>Rust backend (<code>rs-node</code>) receives the request</li>\n<li>Rust library (<code>rs-lib</code>) performs the Blake3 hash</li>\n<li>Result is serialized and returned via Axum</li>\n<li>OpenAPI documentation describes the endpoint</li>\n<li>Node.js webapp proxies <code>/api/*</code> requests to the Rust node</li>\n</ol>\n<h2>TypeScript Frontend + Rust Backend</h2>\n<p>While our backend is Rust, our frontend remains TypeScript. This gives us the best of both worlds:</p>\n<ul>\n<li><strong>Rust</strong>: Performance and security for cryptography and core logic</li>\n<li><strong>TypeScript</strong>: Rapid development and rich ecosystem for UI</li>\n</ul>\n<p>Our architecture uses:</p>\n<ul>\n<li><strong>Tauri</strong>: Native desktop apps with Rust backend + web frontend</li>\n<li><strong>React Router</strong>: Type-safe routing for web and desktop apps</li>\n<li><strong>shadcn</strong>: UI components with Catppuccin theme</li>\n<li><strong>Type-safe API client</strong>: Generated from OpenAPI spec for compile-time safety</li>\n</ul>\n<p>The Tauri app embeds the same <code>rs-lib</code> cryptography that powers the KeyPears node. This means the desktop app has full offline capability—it doesn't need a server for cryptographic operations. The server is only needed for synchronization across devices.</p>\n<h2>Deployment Architecture</h2>\n<p>In production, we run a dual-server setup:</p>\n<ol>\n<li><strong>KeyPears node (Rust)</strong>: Runs on port 4274, handles API requests</li>\n<li><strong>Webapp server (Node.js)</strong>: Runs on port 4273, serves the landing page and proxies API requests</li>\n</ol>\n<p>The Node.js server forwards all <code>/api/*</code> requests to the Rust node via <code>http-proxy-middleware</code>. This gives us:</p>\n<ul>\n<li>Single-domain simplicity (no CORS issues)</li>\n<li>Independent scaling of API and web traffic</li>\n<li>Clean separation of concerns</li>\n</ul>\n<p>Both services run in a single Docker container on AWS Fargate, deployed via ECS.</p>\n<h2>Interactive API Documentation</h2>\n<p>One of the benefits of Rust's <code>utoipa</code> library is automatic OpenAPI documentation generation. You can explore our API interactively at:</p>\n<p><strong>https://keypears.com/api/docs</strong></p>\n<p>This Swagger UI is generated directly from our Rust code. Every endpoint, request type, and response type is documented with examples. As we add new endpoints, the documentation updates automatically.</p>\n<h2>What's Next?</h2>\n<p>The Blake3 endpoint is just the beginning. We're actively building:</p>\n<h3>Vault Operations</h3>\n<ul>\n<li>Create and encrypt vaults</li>\n<li>Derive encryption keys from passwords</li>\n<li>Store and retrieve encrypted secrets</li>\n</ul>\n<h3>Synchronization Protocol</h3>\n<ul>\n<li>Append-only logs for conflict-free sync</li>\n<li>Server-side coordination for multi-device sync</li>\n<li>End-to-end encryption (servers never see plaintext)</li>\n</ul>\n<h3>Diffie-Hellman Key Exchange</h3>\n<ul>\n<li>Peer-to-peer secret sharing across domains</li>\n<li>Email-style addressing (<code>alice@example.com</code> ↔ <code>bob@example2.com</code>)</li>\n<li>Public key discovery via federated nodes</li>\n</ul>\n<h3>Cross-Platform Clients</h3>\n<ul>\n<li>Desktop apps (Windows, macOS, Linux) via Tauri</li>\n<li>Mobile apps (Android, iOS) - coming soon</li>\n<li>Web interface for emergency access</li>\n</ul>\n<p>All of these features will be built on the same foundation: Rust for security-critical operations, TypeScript for user interfaces.</p>\n<h2>Open Source and Self-Hostable</h2>\n<p>Everything we're building is open source under Apache 2.0. You can:</p>\n<ul>\n<li>Review the code for security</li>\n<li>Run your own KeyPears node</li>\n<li>Contribute improvements</li>\n<li>Build custom clients</li>\n</ul>\n<p>The KeyPears node is designed to be self-hostable. Our deployment documentation walks through:</p>\n<ul>\n<li>Cross-compiling for Linux (even from macOS)</li>\n<li>Docker containerization</li>\n<li>AWS Fargate deployment</li>\n<li>Domain configuration and SSL</li>\n</ul>\n<p>We want KeyPears to be decentralized by default. Anyone should be able to run a node, just like anyone can run an email server.</p>\n<h2>Conclusion</h2>\n<p>Building KeyPears with Rust has been an excellent decision. The language's emphasis on safety, performance, and correctness aligns perfectly with our security requirements. The Blake3 proof-of-concept validates our architecture: Rust backend for cryptography, TypeScript frontend for user experience, and a clean API boundary between them.</p>\n<p>We're excited to continue building. If you're interested in following along, check out:</p>\n<ul>\n<li><strong>Live demo</strong>: Try the Blake3 endpoint at https://keypears.com/api/blake3</li>\n<li><strong>API docs</strong>: Explore the OpenAPI spec at https://keypears.com/api/docs</li>\n<li><strong>Source code</strong>: Coming soon on GitHub</li>\n</ul>\n<p>We'll continue sharing our progress through these blog posts. Next up: vault encryption and key derivation in Rust.</p>",
      "url": "https://keypears.com/blog/2025-10-25-rust-backend-architecture",
      "title": "Building KeyPears with Rust: Backend Architecture and Blake3 Proof-of-Concept",
      "summary": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager and cryptocurrency wallet. The design decisions described here represent our development approach and may evolve b...",
      "date_modified": "2025-10-25T11:00:00.000Z",
      "author": {
        "name": "KeyPears Team"
      }
    },
    {
      "id": "https://keypears.com/blog/2025-10-07-progress-on-secret-synchronization",
      "content_html": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The\ndesign decisions described here represent our development approach and may\nevolve before our official release.</p>\n<p>We've made significant progress on KeyPears' secret synchronization\narchitecture. Today we're sharing how we redesigned our schema to support\ndiverse secret types while maintaining the small-sync-unit principle that makes\nour synchronization protocol efficient and reliable.</p>\n<h2>The Problem</h2>\n<p>Our original schema was built around passwords. It had fields like\n<code>encryptedPassword</code>, <code>username</code>, <code>domain</code>, and <code>notes</code>. This worked fine for\nbasic password management, but it created limitations:</p>\n<ul>\n<li><strong>Type inflexibility</strong>: How do you store an API key? A cryptocurrency wallet\nwith multiple components? Environment variables?</li>\n<li><strong>No grouping</strong>: Secrets existed in isolation. There was no way to represent\n\"this API token belongs to this account\" or \"these 20 environment variables\nform one .env file\"</li>\n<li><strong>No hierarchy</strong>: No folders, no organization beyond a flat list</li>\n<li><strong>KeePass import impossible</strong>: KeePass has groups (folders) and custom fields\n(additional key-value pairs per entry). We couldn't represent either.</li>\n</ul>\n<p>We needed a more flexible schema without abandoning our core architectural\nprinciple: <strong>every secret must sync independently</strong> to keep network overhead\nsmall and conflict resolution simple.</p>\n<h2>The Solution: Three Changes</h2>\n<p>We evolved the <code>SecretUpdate</code> schema with three key additions: multi-type\nsupport, dual hierarchy mechanisms, and JSON-based storage.</p>\n<h3>1. Multi-Type Support</h3>\n<p>First, we made the schema generic enough to handle any small secret:</p>\n<pre><code class=\"language-typescript\">type: \"password\" | \"envvar\" | \"apikey\" | \"walletkey\" | \"passkey\"\nencryptedData: string  // Previously: encryptedPassword\nencryptedNotes: string // Previously: notes\n</code></pre>\n<p>The <code>type</code> field distinguishes what kind of secret this is. The generic\n<code>encryptedData</code> field holds the actual secret value (password, API key, private\nkey, etc.). Password-specific fields like <code>domain</code>, <code>username</code>, and <code>email</code>\nremain in the schema but are optional—used primarily when <code>type</code> is <code>password</code>.</p>\n<p>This small change opens up KeyPears to handle:</p>\n<ul>\n<li><strong>Environment variables</strong>: Type <code>envvar</code>, name <code>DATABASE_URL</code>, encrypted value\nin <code>encryptedData</code></li>\n<li><strong>API keys</strong>: Type <code>apikey</code>, service name in a <code>label</code> field, key in\n<code>encryptedData</code></li>\n<li><strong>Wallet keys</strong>: Type <code>walletkey</code>, blockchain type in metadata, private key in\n<code>encryptedData</code></li>\n<li><strong>Passkeys</strong>: Type <code>passkey</code>, credential ID and public key in metadata,\nprivate key in <code>encryptedData</code></li>\n</ul>\n<h3>2. Dual Hierarchy: Folders and ParentId</h3>\n<p>The second change introduces two different hierarchy mechanisms, each serving a\nspecific purpose:</p>\n<pre><code class=\"language-typescript\">folders: string[]      // [\"Work\", \"Projects\", \"Client A\"]\ntags: string[]         // [\"production\", \"critical\"]\nparentId: string       // ULID of parent secret (max depth 1)\n</code></pre>\n<p><strong>Folders</strong> provide unlimited-depth organizational hierarchy. They're just an\narray of strings representing the path:</p>\n<pre><code class=\"language-typescript\">folders: [\"Work\", \"AWS\", \"Production\"]\nfolders: [\"Personal\", \"Banking\"]\nfolders: []  // Root level\n</code></pre>\n<p>This maps perfectly to KeePass Groups and lets users organize thousands of\nsecrets into a familiar folder structure.</p>\n<p><strong>Tags</strong> provide orthogonal categorization. A secret can have multiple tags for\ncross-cutting concerns:</p>\n<pre><code class=\"language-typescript\">tags: [\"production-env\", \"requires-rotation\", \"shared-with-team\"]\n</code></pre>\n<p><strong>ParentId</strong> creates actual parent-child relationships between secrets. This is\nwhere it gets interesting.</p>\n<h2>ParentId: Secrets Containing Secrets</h2>\n<p>The <code>parentId</code> field lets one secret \"contain\" other secrets. A simple example:</p>\n<pre><code class=\"language-typescript\">// Parent: The main account\n{\n  secretId: \"abc123\",\n  name: \"GitHub Account\",\n  type: \"password\",\n  encryptedData: \"&#x3C;main password>\"\n}\n\n// Child: API token for the same account\n{\n  secretId: \"def456\",\n  name: \"API Token\",\n  type: \"apikey\",\n  parentId: \"abc123\",\n  encryptedData: \"&#x3C;token>\"\n}\n</code></pre>\n<p>This models KeePass's custom fields—additional key-value pairs that belong to an\nentry. In KeePass, you might have a GitHub entry with standard fields (username,\npassword, URL) plus custom fields for API tokens, 2FA backup codes, or recovery\nemails.</p>\n<p>In KeyPears, each custom field becomes its own secret with a <code>parentId</code> pointing\nto the parent. Each syncs independently (small sync units!), but they're\nlogically grouped.</p>\n<h3>The Depth Limit: Security Through Simplicity</h3>\n<p>Here's the critical constraint: <strong>a secret can have a parent, but that parent\ncannot have a parent</strong>. Maximum depth is 1. No grandparents allowed.</p>\n<p>Why? Three reasons:</p>\n<p><strong>1. Security</strong>: Client-generated IDs open an attack vector for malicious\nclients creating circular references or extremely deep chains. With depth=1, the\nvalidation is trivial:</p>\n<pre><code class=\"language-typescript\">async function validateParentChain(secretId: string, parentId?: string) {\n  if (!parentId) return; // No parent, valid\n  if (parentId === secretId) throw new Error(\"Cannot self-reference\");\n\n  const parent = await getSecretHistory(parentId);\n  if (parent.length > 0 &#x26;&#x26; parent[0].parentId) {\n    throw new Error(\"Cannot nest more than one level deep\");\n  }\n}\n</code></pre>\n<p>One database lookup. No recursion. No visited sets. O(1) validation that\nattackers can't exploit.</p>\n<p><strong>2. Performance</strong>: Validating unlimited depth requires recursive queries.\nValidating depth=1 requires one query. Simple.</p>\n<p><strong>3. Sufficient for real use cases</strong>:</p>\n<ul>\n<li>Folder with secrets ✓</li>\n<li>Entry with custom fields ✓</li>\n<li>Environment variable group ✓</li>\n</ul>\n<p>What we lose: deeply nested folder hierarchies via <code>parentId</code>. But we have\n<code>folders</code> for that! The two mechanisms complement each other perfectly.</p>\n<h2>Why Two Hierarchy Systems?</h2>\n<p>It might seem redundant to have both <code>folders</code> and <code>parentId</code>, but they serve\ndifferent purposes:</p>\n<p><strong>Folders</strong> are for <strong>organizational hierarchy</strong>. They map to KeePass Groups.\nThey're pure metadata—just strings representing a path. They have unlimited\ndepth because they're just labels, not database relationships.</p>\n<p><strong>ParentId</strong> is for <strong>data relationships</strong>. It maps to KeePass custom fields. It\ncreates actual parent-child relationships where one secret logically contains\nothers. Each child syncs independently, maintaining small sync units.</p>\n<p>Together, they enable full KeePass import:</p>\n<pre><code class=\"language-typescript\">// KeePass structure:\n// Work/Projects/GitHub (Group path)\n//   - GitHub Account (Entry)\n//     - Username: alice\n//     - Password: ••••••\n//     - Custom: API Token (protected)\n//     - Custom: 2FA Codes (protected)\n\n// KeyPears representation:\n{\n  secretId: \"main\",\n  name: \"GitHub Account\",\n  type: \"password\",\n  folders: [\"Work\", \"Projects\", \"GitHub\"],\n  username: \"alice\",\n  encryptedData: \"&#x3C;password>\"\n}\n{\n  secretId: \"token\",\n  name: \"API Token\",\n  type: \"apikey\",\n  folders: [\"Work\", \"Projects\", \"GitHub\"], // Inherits folder\n  parentId: \"main\",\n  encryptedData: \"&#x3C;token>\"\n}\n{\n  secretId: \"codes\",\n  name: \"2FA Codes\",\n  type: \"password\",\n  folders: [\"Work\", \"Projects\", \"GitHub\"],\n  parentId: \"main\",\n  encryptedData: \"&#x3C;codes>\"\n}\n</code></pre>\n<p>The folder path provides organization. The <code>parentId</code> relationships show which\nsecrets belong together. Each secret syncs independently.</p>\n<h2>JSON-Based Storage: Migration-Proof Architecture</h2>\n<p>The third major change is how we store secrets in the database. We moved to a\nhybrid approach:</p>\n<pre><code class=\"language-sql\">CREATE TABLE secret_update (\n  id TEXT PRIMARY KEY,\n  vault_id TEXT NOT NULL,\n  secret_id TEXT NOT NULL,\n  name TEXT NOT NULL,\n  type TEXT NOT NULL DEFAULT 'password',\n  parent_id TEXT,\n  created_at INTEGER NOT NULL,\n  deleted INTEGER NOT NULL DEFAULT 0,\n\n  -- Source of truth: full JSON object\n  secret_update_json TEXT NOT NULL\n);\n\nCREATE INDEX idx_secret_updates_name ON secret_update(name);\nCREATE INDEX idx_secret_updates_type ON secret_update(type);\nCREATE INDEX idx_secret_updates_parent_id ON secret_update(parent_id);\n</code></pre>\n<p>Notice what's happening here. We store the <strong>entire <code>SecretUpdate</code> object</strong> as\nJSON in <code>secret_update_json</code>. The other columns (<code>name</code>, <code>type</code>, <code>parent_id</code>,\netc.) are duplicates of data from the JSON, extracted for indexing.</p>\n<p>The JSON is the source of truth. The columns are for performance.</p>\n<h3>Why This Approach?</h3>\n<p><strong>Adding fields requires no migration</strong>. Want to add a <code>label</code> field? Update the\nZod schema, start writing it to the JSON, and you're done. The database doesn't\ncare—it's just storing JSON.</p>\n<p>When we added <code>parentId</code> to the schema, we:</p>\n<ol>\n<li>Updated the Zod schema in TypeScript</li>\n<li>Added <code>parent_id</code> column to the database (for indexing)</li>\n<li>Started serializing <code>parentId</code> to the JSON</li>\n</ol>\n<p>Users with existing vaults see <code>parentId: undefined</code> in their JSON. No\nmigration, no data transformation. Just works.</p>\n<p>This architecture is <strong>future-proof</strong>. We can evolve the schema rapidly during\ndevelopment without worrying about breaking existing databases.</p>\n<h3>When We Ship v1.0</h3>\n<p>Before our first production release, we'll generate one clean migration from the\nfinal schema. That becomes our baseline. After that, we'll only add new\nmigrations—never delete old ones—because users will have the old migrations\napplied.</p>\n<p>But during development? We delete and regenerate migrations freely. The JSON\nstorage strategy makes this painless.</p>\n<h2>What This Enables</h2>\n<p>With these changes in place, KeyPears can now handle:</p>\n<h3>KeePass Import (Future Feature)</h3>\n<p>Full KeePass <code>.kdbx</code> import support, including:</p>\n<ul>\n<li>Nested groups → <code>folders</code> array</li>\n<li>Entries → secrets with <code>type: \"password\"</code></li>\n<li>Custom protected fields → child secrets with <code>parentId</code></li>\n<li>Entry metadata → password-specific fields</li>\n</ul>\n<p>The only thing we won't import: file attachments. By design. We're optimizing\nfor small secrets that sync efficiently.</p>\n<h3>Environment Variables</h3>\n<p>Create a parent secret \"Production Environment\" and attach child secrets for\neach variable:</p>\n<pre><code class=\"language-typescript\">{ name: \"Production Env\", type: \"folder\" }  // Parent\n{ name: \"DATABASE_URL\", type: \"envvar\", parentId: \"...\" }\n{ name: \"API_SECRET\", type: \"envvar\", parentId: \"...\" }\n{ name: \"STRIPE_KEY\", type: \"envvar\", parentId: \"...\" }\n</code></pre>\n<p>Or use tags instead:</p>\n<pre><code class=\"language-typescript\">{ name: \"DATABASE_URL\", type: \"envvar\", tags: [\"prod-env\"] }\n{ name: \"API_SECRET\", type: \"envvar\", tags: [\"prod-env\"] }\n</code></pre>\n<p>Both approaches work. <code>parentId</code> creates explicit grouping. Tags create implicit\nsets.</p>\n<h3>Cryptocurrency Wallets</h3>\n<p>Store wallet keys with relevant metadata:</p>\n<pre><code class=\"language-typescript\">{\n  name: \"Ethereum Main Wallet\",\n  type: \"walletkey\",\n  encryptedData: \"&#x3C;private key>\",\n  folders: [\"Crypto\", \"Ethereum\"],\n  tags: [\"high-value\", \"cold-storage\"]\n}\n</code></pre>\n<h3>API Keys with Secrets</h3>\n<p>Store API key pairs as parent-child:</p>\n<pre><code class=\"language-typescript\">{ name: \"Stripe\", type: \"apikey\", encryptedData: \"&#x3C;public key>\" }\n{ name: \"Secret Key\", type: \"apikey\", parentId: \"...\", encryptedData: \"&#x3C;secret>\" }\n</code></pre>\n<h2>Synchronization Properties</h2>\n<p>These changes maintain our core synchronization principles:</p>\n<p><strong>Small sync units</strong>: Each secret syncs independently. A 50-entry KeePass import\nbecomes 50 individual secrets, each a few hundred bytes. If two users edit\ndifferent entries, no conflicts.</p>\n<p><strong>Atomic updates</strong>: Each <code>SecretUpdate</code> is immutable once created. Updates\ncreate new records in an append-only log. The latest update wins\n(last-write-wins conflict resolution).</p>\n<p><strong>Efficient</strong>: Only changed secrets sync. If you update one child secret, you\nsync one small object, not the entire parent-child group.</p>\n<p><strong>Validated</strong>: The <code>parentId</code> depth limit prevents malicious clients from\ncreating expensive recursive structures.</p>\n<h2>Looking Ahead</h2>\n<p>This schema evolution lays the groundwork for several future features:</p>\n<ul>\n<li><strong>UI for child secrets</strong>: Show \"API Token\" nested under \"GitHub Account\" in\nthe secret list</li>\n<li><strong>KeePass import</strong>: Full <code>.kdbx</code> file import with groups and custom fields</li>\n<li><strong>Environment variable templates</strong>: Quick creation of common .env structures</li>\n<li><strong>Bulk operations</strong>: Delete/restore an entire group by operating on all\nchildren</li>\n</ul>\n<p>The foundation is solid. The architecture is flexible. The sync protocol remains\nsimple and efficient.</p>\n<p>We're building KeyPears to be more than a password manager—it's a secure,\nself-custodied secret manager that handles everything from passwords to\nenvironment variables to cryptocurrency keys. And it all syncs seamlessly across\nyour devices without trusting a central authority with your encryption keys.</p>\n<h2>Technical Details</h2>\n<p>For those interested in the implementation:</p>\n<ul>\n<li><strong>Schema definition</strong>: Zod validation in TypeScript, ensuring type safety</li>\n<li><strong>Database</strong>: SQLite via Drizzle ORM with the sqlite-proxy adapter</li>\n<li><strong>Migration</strong>: Custom migration runner that tracks applied migrations</li>\n<li><strong>Validation</strong>: O(1) parent chain validation with single database lookup</li>\n<li><strong>Indexes</strong>: <code>name</code>, <code>type</code>, <code>parent_id</code>, and composite\n<code>vault_id + secret_id + created_at</code></li>\n</ul>\n<p>The code is Apache 2.0 licensed and available on GitHub. We're building in the\nopen, one commit at a time.</p>\n<p>More updates coming soon.</p>",
      "url": "https://keypears.com/blog/2025-10-07-progress-on-secret-synchronization",
      "title": "Progress on Secret Synchronization: A Future-Proof Schema",
      "summary": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The\ndesign decisions described here represent our development approach and may\nevolve before our official release...",
      "date_modified": "2025-10-07T11:00:00.000Z",
      "author": {
        "name": "KeyPears Team"
      }
    },
    {
      "id": "https://keypears.com/blog/2025-10-05-vault-encryption-key-derivation",
      "content_html": "<p>One of the core security features of KeyPears is how we protect your vault\nencryption keys. Today, we're diving deep into our three-tier key derivation\nsystem and explaining how we ensure that even if a server is compromised, your\nencrypted data remains secure.</p>\n<h2>The Problem: Authentication vs. Encryption</h2>\n<p>Most password managers face a fundamental challenge: you need to send\n<em>something</em> to the server to prove who you are, but you also need to keep your\nencryption key secret so the server can't decrypt your vault. Using the same key\nfor both purposes creates a security vulnerability—if the server is compromised,\nan attacker gains access to your encryption key.</p>\n<p>KeyPears solves this by deriving two separate keys from your password: one for\nlogging in to the server, and one for encrypting your vault. The server only\never sees the login key, never the encryption key.</p>\n<h2>Three-Tier Key Derivation</h2>\n<p>When you create a vault in KeyPears, we don't just hash your password once.\nInstead, we use a three-tier key derivation system:</p>\n<pre><code>Master Password\n  ↓ blake3Pbkdf (100,000 rounds)\nPassword Key (stored encrypted with PIN on device)\n  ↓\n  ├→ blake3Pbkdf (100,000 rounds) → Encryption Key\n  └→ blake3Pbkdf (100,000 rounds) → Login Key\n</code></pre>\n<h3>1. Password Key: The Root of Trust</h3>\n<p>The first step derives a <strong>password key</strong> from your master password using\n100,000 rounds of our Blake3-based PBKDF. This intermediate key is stored on\nyour device, encrypted with your PIN for quick unlock. It never leaves your\ndevice and is never sent to any server.</p>\n<p>The password key acts as the root of trust for deriving the other two keys.</p>\n<h3>2. Encryption Key: Protecting Your Vault</h3>\n<p>From the password key, we derive an <strong>encryption key</strong> through another 100,000\nrounds of Blake3 PBKDF. This key is used for one purpose only: encrypting and\ndecrypting your master vault key.</p>\n<p>Wait—encrypting a key with another key? Yes! Your vault itself is encrypted with\na randomly generated <strong>master vault key</strong>. This master key is immutable and\nnever changes. The encryption key derived from your password is used to encrypt\nthis master vault key before storing it in the database.</p>\n<p>This architecture allows you to change your password without re-encrypting your\nentire vault—we just re-encrypt the master vault key with the new encryption\nkey.</p>\n<p>The encryption key is ephemeral. We derive it when needed, use it immediately,\nand discard it. It is never persisted to disk and never sent anywhere.</p>\n<h3>3. Login Key: Server Authentication</h3>\n<p>The third key in our hierarchy is the <strong>login key</strong>, also derived from the\npassword key through 100,000 rounds of Blake3 PBKDF. This is the only key that\ngets sent to the server for authentication.</p>\n<p>Because the login key is derived separately from the encryption key,\ncompromising one doesn't compromise the other. Even if a server is breached and\nthe login key is stolen, the attacker cannot derive the encryption key needed to\ndecrypt your vault.</p>\n<h2>Blake3 PBKDF: Fast and Secure</h2>\n<p>You might notice we're using 100,000 rounds of Blake3 PBKDF rather than a\nstandard algorithm like PBKDF2. Blake3 is a modern, extremely fast cryptographic\nhash function. Even at 100,000 rounds, the entire key derivation completes in\nmilliseconds on modern hardware.</p>\n<p>Our Blake3-based PBKDF works by iteratively applying Blake3's keyed MAC mode:</p>\n<pre><code>Round 1: result = blake3Mac(salt, password)\nRound 2: result = blake3Mac(salt, result_from_round_1)\n...\nRound 100,000: result = blake3Mac(salt, result_from_round_99,999)\n</code></pre>\n<p>Each round adds computational cost for attackers trying to brute-force your\npassword, while remaining fast enough for legitimate use.</p>\n<h2>Salt Derivation</h2>\n<p>Each key derivation uses a different salt to ensure cryptographic separation:</p>\n<ul>\n<li>\n<p><strong>Password Salt</strong>: Derived deterministically from your password using\n<code>blake3Mac(blake3Hash(\"KeyPears password salt v1\"), password)</code>. This ensures\nthe same password always produces the same password key.</p>\n</li>\n<li>\n<p><strong>Encryption Salt</strong>: A global constant\n<code>blake3Hash(\"KeyPears encryption salt v1\")</code> used for all users. This is safe\nbecause the encryption key is derived from the password key, not directly from\nthe password.</p>\n</li>\n<li>\n<p><strong>Login Salt</strong>: Another global constant\n<code>blake3Hash(\"KeyPears login salt v1\")</code>. Again, safe because it's derived from\nthe password key.</p>\n</li>\n</ul>\n<h2>Security Properties</h2>\n<p>This architecture provides several important security guarantees:</p>\n<h3>Defense Against Server Compromise</h3>\n<p>If a KeyPears server is compromised, the attacker gains access to:</p>\n<ul>\n<li>Encrypted vault data</li>\n<li>Login keys for authentication</li>\n</ul>\n<p>The attacker does NOT gain access to:</p>\n<ul>\n<li>Master passwords</li>\n<li>Password keys</li>\n<li>Encryption keys</li>\n<li>Master vault keys</li>\n<li>Decrypted vault contents</li>\n</ul>\n<p>Without the encryption key, the encrypted vault data is useless to the attacker.</p>\n<h3>Defense Against Encrypted Data Theft</h3>\n<p>If someone steals your encrypted vault data but doesn't have your credentials:</p>\n<ul>\n<li>They cannot decrypt it without the encryption key</li>\n<li>The encryption key requires the password key</li>\n<li>The password key requires your master password</li>\n<li>100,000 rounds of Blake3 PBKDF make brute-forcing expensive</li>\n</ul>\n<h3>Key Separation</h3>\n<p>The three keys are cryptographically isolated. Knowing the login key doesn't\nhelp you derive the encryption key, and vice versa. Both require the password\nkey, which requires the master password.</p>\n<h2>The Vault Key Hash: Verification</h2>\n<p>When you enter your password to unlock a vault, KeyPears needs to verify you\nentered it correctly. We do this by storing a Blake3 hash of the master vault\nkey in the database.</p>\n<p>When you unlock:</p>\n<ol>\n<li>Derive password key from your password</li>\n<li>Derive encryption key from password key</li>\n<li>Decrypt the master vault key using the encryption key</li>\n<li>Hash the decrypted master vault key</li>\n<li>Compare with the stored hash</li>\n</ol>\n<p>If the hashes match, you entered the correct password. If not, the password is\nwrong. This verification happens entirely on your device—the master vault key\nnever leaves your device, even temporarily.</p>\n<h2>Putting It All Together</h2>\n<p>Here's what happens when you create a new vault:</p>\n<ol>\n<li>You enter a master password</li>\n<li>KeyPears derives a password key (100k rounds Blake3)</li>\n<li>Derives an encryption key from the password key (100k rounds Blake3)</li>\n<li>Generates a random master vault key</li>\n<li>Encrypts the master vault key with the encryption key</li>\n<li>Hashes the master vault key for verification</li>\n<li>Stores the encrypted vault key and hash in your local database</li>\n</ol>\n<p>When you sync to a server:</p>\n<ol>\n<li>Derive the login key from your password key (100k rounds Blake3)</li>\n<li>Send the login key to the server for authentication</li>\n<li>Server returns your encrypted master vault key (and other encrypted vault\ndata)</li>\n<li>Derive the encryption key (never sent to server)</li>\n<li>Decrypt the master vault key locally</li>\n<li>Use the master vault key to decrypt your secrets</li>\n</ol>\n<p>The server facilitates synchronization but never has the keys needed to decrypt\nyour data.</p>\n<h2>Looking Ahead</h2>\n<p>This architecture lays the foundation for secure sharing between users. In\nfuture posts, we'll explore how KeyPears uses Diffie-Hellman key exchange to\nshare secrets securely between users, and how the master vault key enables\nefficient re-encryption without re-deriving keys.</p>\n<p>For now, the key takeaway is simple: KeyPears separates authentication from\nencryption. Your server can verify who you are without ever having the ability\nto decrypt your data. It's cryptography working exactly as it should.</p>",
      "url": "https://keypears.com/blog/2025-10-05-vault-encryption-key-derivation",
      "title": "How KeyPears Protects Your Vault: Encryption and Key Derivation",
      "summary": "<p>One of the core security features of KeyPears is how we protect your vault\nencryption keys. Today, we're diving deep into our three-tier key derivation\nsystem and explaining how we ensure that even...",
      "date_modified": "2025-10-05T11:00:00.000Z",
      "author": {
        "name": "KeyPears Team"
      }
    },
    {
      "id": "https://keypears.com/blog/2025-10-04-drizzle-sqlite-tauri",
      "content_html": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The\nsolutions described here are part of our development process and may evolve\nbefore our official release.</p>\n<h2>The Problem</h2>\n<p>Building a local-first application with Tauri 2.0, we needed a robust database\nsolution for storing encrypted vault data on users' devices. We wanted:</p>\n<ul>\n<li>Type-safe database queries</li>\n<li>Proper schema migrations that work in production</li>\n<li>Pure TypeScript implementation (no Rust for basic DB operations)</li>\n<li>A solution that works across desktop and mobile platforms</li>\n</ul>\n<p>After evaluating options, we chose <strong>Drizzle ORM</strong> with <strong>SQLite</strong> via the\nofficial <strong>tauri-plugin-sql</strong>. This combination gives us TypeScript-first\ndevelopment with the reliability of SQLite.</p>\n<h2>The Challenge</h2>\n<p>Unlike traditional Node.js environments where you have direct filesystem access\nand can use drivers like <code>better-sqlite3</code>, Tauri's sandboxed environment\nrequires a different approach. Drizzle's standard migration tools assume direct\ndatabase access, but with Tauri, we need to go through the plugin system.</p>\n<p>Here's how we solved it.</p>\n<h2>Tech Stack</h2>\n<ul>\n<li><strong>Tauri 2.0</strong> - Cross-platform app framework</li>\n<li><strong>Drizzle ORM</strong> - TypeScript ORM</li>\n<li><strong>drizzle-kit</strong> - Schema migration generator</li>\n<li><strong>@tauri-apps/plugin-sql</strong> - Official Tauri SQLite plugin</li>\n<li><strong>React Router</strong> - For app routing and loaders</li>\n</ul>\n<h2>Step 1: Install Dependencies</h2>\n<p>First, add the necessary packages:</p>\n<pre><code class=\"language-bash\"># Production dependencies\npnpm add drizzle-orm @tauri-apps/plugin-sql\n\n# Development dependencies\npnpm add -D drizzle-kit\n</code></pre>\n<p>Then add the Tauri plugin to your Rust dependencies in <code>src-tauri/Cargo.toml</code>:</p>\n<pre><code class=\"language-toml\">[dependencies]\ntauri-plugin-sql = { version = \"2\", features = [\"sqlite\"] }\n</code></pre>\n<h2>Step 2: Configure Tauri Permissions</h2>\n<p>Tauri 2.0 requires explicit permission grants. Add SQL permissions to\n<code>src-tauri/capabilities/default.json</code>:</p>\n<pre><code class=\"language-json\">{\n  \"$schema\": \"../gen/schemas/desktop-schema.json\",\n  \"identifier\": \"default\",\n  \"description\": \"Capability for the main window\",\n  \"windows\": [\"main\"],\n  \"permissions\": [\n    \"core:default\",\n    \"sql:default\",\n    \"sql:allow-load\",\n    \"sql:allow-execute\",\n    \"sql:allow-select\",\n    \"sql:allow-close\"\n  ]\n}\n</code></pre>\n<p>Without these permissions, you'll get \"not allowed\" errors when trying to access\nthe database.</p>\n<h2>Step 3: Define Your Schema</h2>\n<p>Create your Drizzle schema at <code>app/db/schema.ts</code>:</p>\n<pre><code class=\"language-typescript\">import { sqliteTable, text, integer } from \"drizzle-orm/sqlite-core\";\n\nexport const vaults = sqliteTable(\"vaults\", {\n  id: integer(\"id\").primaryKey({ autoIncrement: true }),\n  name: text(\"name\").notNull().unique(),\n});\n</code></pre>\n<h2>Step 4: Set Up the SQLite Proxy</h2>\n<p>Since we can't use standard SQLite drivers in Tauri, we use Drizzle's\n<code>sqlite-proxy</code> adapter. Create <code>app/db/index.ts</code>:</p>\n<pre><code class=\"language-typescript\">import { drizzle } from \"drizzle-orm/sqlite-proxy\";\nimport Database from \"@tauri-apps/plugin-sql\";\nimport * as schema from \"./schema\";\n\nexport async function getDb() {\n  return await Database.load(\"sqlite:keypears.db\");\n}\n\nfunction isSelectQuery(sql: string): boolean {\n  return sql.trim().toLowerCase().startsWith(\"select\");\n}\n\nexport const db = drizzle&#x3C;typeof schema>(\n  async (sql, params, method) => {\n    const sqlite = await getDb();\n    let rows: any = [];\n\n    if (isSelectQuery(sql)) {\n      rows = await sqlite.select(sql, params).catch((e) => {\n        console.error(\"SQL Error:\", e);\n        return [];\n      });\n    } else {\n      rows = await sqlite.execute(sql, params).catch((e) => {\n        console.error(\"SQL Error:\", e);\n        return [];\n      });\n      return { rows: [] };\n    }\n\n    rows = rows.map((row: any) => Object.values(row));\n    const results = method === \"all\" ? rows : rows[0];\n    await sqlite.close();\n    return { rows: results };\n  },\n  { schema: schema, logger: true }\n);\n</code></pre>\n<p>The proxy adapter translates Drizzle queries into calls to the Tauri SQL plugin.</p>\n<h2>Step 5: Configure Migration Generation</h2>\n<p>Create <code>drizzle.config.ts</code>:</p>\n<pre><code class=\"language-typescript\">import type { Config } from \"drizzle-kit\";\n\nexport default {\n  schema: \"./app/db/schema.ts\",\n  out: \"./app/db/migrations\",\n  dialect: \"sqlite\",\n} satisfies Config;\n</code></pre>\n<p>Add a script to <code>package.json</code>:</p>\n<pre><code class=\"language-json\">{\n  \"scripts\": {\n    \"db:migrate\": \"drizzle-kit generate\"\n  }\n}\n</code></pre>\n<h2>Step 6: Implement Migration Runner</h2>\n<p>Here's the key part - implementing our own migration system. Create\n<code>app/db/migrate.ts</code>:</p>\n<pre><code class=\"language-typescript\">import { getDb } from \"./index\";\n\n// Dynamically import all SQL migration files\nconst migrationFiles = import.meta.glob&#x3C;string>(\"./migrations/*.sql\", {\n  query: \"?raw\",\n  import: \"default\",\n  eager: true,\n});\n\n// Create migrations tracking table\nasync function ensureMigrationsTable() {\n  const sqlite = await getDb();\n  await sqlite.execute(`\n    CREATE TABLE IF NOT EXISTS __drizzle_migrations (\n      id INTEGER PRIMARY KEY AUTOINCREMENT,\n      hash TEXT NOT NULL UNIQUE,\n      created_at INTEGER NOT NULL\n    )\n  `);\n  await sqlite.close();\n}\n\n// Get list of applied migrations\nasync function getAppliedMigrations(): Promise&#x3C;string[]> {\n  const sqlite = await getDb();\n  const rows = await sqlite\n    .select&#x3C;Array&#x3C;{ hash: string }>>(\n      \"SELECT hash FROM __drizzle_migrations ORDER BY id\"\n    )\n    .catch(() => []);\n  await sqlite.close();\n  return rows.map((row) => row.hash);\n}\n\n// Record migration as applied\nasync function recordMigration(hash: string) {\n  const sqlite = await getDb();\n  const timestamp = Date.now();\n  await sqlite.execute(\n    \"INSERT INTO __drizzle_migrations (hash, created_at) VALUES (?, ?)\",\n    [hash, timestamp]\n  );\n  await sqlite.close();\n}\n\n// Execute SQL file\nasync function executeSqlFile(sqlContent: string) {\n  const sqlite = await getDb();\n  const statements = sqlContent\n    .split(\"--> statement-breakpoint\")\n    .map((s) => s.trim())\n    .filter((s) => s.length > 0);\n\n  for (const statement of statements) {\n    await sqlite.execute(statement).catch((e) => {\n      console.error(\"Migration error:\", e);\n      throw e;\n    });\n  }\n\n  await sqlite.close();\n}\n\nexport async function runMigrations() {\n  console.log(\"Running database migrations...\");\n\n  try {\n    await ensureMigrationsTable();\n    const appliedMigrations = await getAppliedMigrations();\n\n    const migrationPaths = Object.keys(migrationFiles).sort();\n\n    const pendingMigrations = migrationPaths.filter((path) => {\n      const filename = path.split(\"/\").pop() || path;\n      return !appliedMigrations.includes(filename);\n    });\n\n    if (pendingMigrations.length === 0) {\n      console.log(\"All migrations already applied\");\n      return;\n    }\n\n    for (const path of pendingMigrations) {\n      const filename = path.split(\"/\").pop() || path;\n      const migrationContent = migrationFiles[path];\n\n      console.log(`Executing migration: ${filename}`);\n      await executeSqlFile(migrationContent);\n      await recordMigration(filename);\n      console.log(`✓ Applied: ${filename}`);\n    }\n\n    console.log(`Successfully completed ${pendingMigrations.length} migration(s)`);\n  } catch (error) {\n    console.error(\"Migration failed:\", error);\n    throw error;\n  }\n}\n</code></pre>\n<p>This implements Drizzle's migration tracking pattern:</p>\n<ul>\n<li>Uses <code>__drizzle_migrations</code> table to track applied migrations</li>\n<li>Only runs new migrations on subsequent app launches</li>\n<li>Supports incremental migrations as your schema evolves</li>\n</ul>\n<h2>Step 7: Run Migrations on App Startup</h2>\n<p>In your root component (<code>app/root.tsx</code>), use a clientLoader to run migrations\nbefore rendering:</p>\n<pre><code class=\"language-typescript\">import { runMigrations } from \"./db/migrate\";\n\nexport async function clientLoader() {\n  await runMigrations();\n  return null;\n}\n\nexport function HydrateFallback() {\n  return (\n    &#x3C;div className=\"flex min-h-screen items-center justify-center\">\n      &#x3C;h1>Migrating the database...&#x3C;/h1>\n    &#x3C;/div>\n  );\n}\n</code></pre>\n<p>React Router will show the fallback while migrations run, ensuring the database\nis ready before any component renders.</p>\n<h2>Step 8: Create Model Functions</h2>\n<p>With everything set up, create type-safe model functions at\n<code>app/db/models/vault.ts</code>:</p>\n<pre><code class=\"language-typescript\">import { db } from \"../index\";\nimport { vaults } from \"../schema\";\nimport { eq, count } from \"drizzle-orm\";\n\nexport interface Vault {\n  id: number;\n  name: string;\n}\n\nexport async function createVault(name: string): Promise&#x3C;Vault> {\n  const result = await db.insert(vaults).values({ name }).returning();\n  return result[0];\n}\n\nexport async function getVault(id: number): Promise&#x3C;Vault | undefined> {\n  const result = await db.select().from(vaults).where(eq(vaults.id, id));\n  return result[0];\n}\n\nexport async function getVaults(): Promise&#x3C;Vault[]> {\n  return await db.select().from(vaults);\n}\n\nexport async function countVaults(): Promise&#x3C;number> {\n  const result = await db.select({ count: count() }).from(vaults);\n  return result[0]?.count ?? 0;\n}\n</code></pre>\n<h2>Usage Workflow</h2>\n<h3>Development</h3>\n<p>When you modify your schema:</p>\n<pre><code class=\"language-bash\"># 1. Update app/db/schema.ts\n# 2. Generate new migration\npnpm run db:migrate\n\n# 3. Restart app - migration runs automatically\n</code></pre>\n<p>During development, you can safely delete all migrations and regenerate them\nfrom scratch. Just delete the database file and migration files, then\nregenerate.</p>\n<h3>Production</h3>\n<p>Before releasing v1.0:</p>\n<ol>\n<li>Delete all development migrations</li>\n<li>Generate one clean migration from your final schema</li>\n<li>Commit this as your baseline</li>\n</ol>\n<p>After release, <strong>never delete migrations</strong> - only add new ones. Users will have\nthe old migrations applied, and new migrations build incrementally.</p>\n<h2>Database File Location</h2>\n<p>The Tauri SQL plugin creates the database in the app's data directory:</p>\n<ul>\n<li><strong>macOS</strong>: <code>~/Library/Application Support/{app-identifier}/keypears.db</code></li>\n<li><strong>Linux</strong>: <code>~/.local/share/{app-identifier}/keypears.db</code></li>\n<li><strong>Windows</strong>: <code>%APPDATA%\\{app-identifier}\\keypears.db</code></li>\n</ul>\n<h2>Troubleshooting</h2>\n<p><strong>Permission errors</strong>: Make sure you've added all SQL permissions to\n<code>capabilities/default.json</code></p>\n<p><strong>Migration fails</strong>: Check browser console in the Tauri webview for detailed\nerror messages</p>\n<p><strong>Type errors</strong>: Run <code>pnpm run typecheck</code> to catch issues before runtime</p>\n<h2>Conclusion</h2>\n<p>This setup gives us:</p>\n<ul>\n<li>✅ Type-safe database queries with Drizzle</li>\n<li>✅ Proper migration tracking that works in production</li>\n<li>✅ Pure TypeScript - no Rust code needed for basic operations</li>\n<li>✅ Cross-platform compatibility (desktop &#x26; mobile)</li>\n<li>✅ Incremental migrations as the schema evolves</li>\n</ul>\n<p>The combination of Drizzle's <code>sqlite-proxy</code> adapter with Tauri's SQL plugin\nprovides a robust foundation for local-first data storage. While we had to\nimplement our own migration runner, we followed Drizzle's patterns to ensure\ncompatibility and maintainability.</p>\n<h2>Resources</h2>\n<ul>\n<li><a href=\"https://orm.drizzle.team/\">Drizzle ORM</a></li>\n<li><a href=\"https://v2.tauri.app/plugin/sql/\">Tauri SQL Plugin</a></li>\n<li><a href=\"https://v2.tauri.app/security/capabilities/\">Tauri Capabilities</a></li>\n</ul>",
      "url": "https://keypears.com/blog/2025-10-04-drizzle-sqlite-tauri",
      "title": "Drizzle SQLite Database Migrations in Tauri 2.0",
      "summary": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The\nsolutions described here are part of our development process and may evolve\nbefore our official release.</p>\n...",
      "date_modified": "2025-10-04T11:00:00.000Z",
      "author": {
        "name": "KeyPears Team"
      }
    },
    {
      "id": "https://keypears.com/blog/2025-10-03-introducing-keypears",
      "content_html": "<p>We're excited to announce KeyPears, a new password manager designed for the\nmodern era of digital security and self-custody.</p>\n<h2>Why KeyPears?</h2>\n<p>Traditional password managers have served us well, but they come with\nlimitations. Most rely on centralized services, creating single points of\nfailure and raising questions about who truly controls your most sensitive data.\nKeyPears takes a different approach.</p>\n<h2>Local-First, Sync-Enabled</h2>\n<p>KeyPears is built on a local-first architecture. Your secrets live on your\ndevices, encrypted with keys only you control. But unlike purely local\nsolutions, KeyPears solves the synchronization problem through a permissionless\nmarketplace of third-party service providers using an open protocol—similar to\nhow email works.</p>\n<p>Anyone can run a KeyPears node. The protocol is open source. You maintain full\nself-custody while enjoying seamless synchronization across all your devices.</p>\n<h2>Built for Sharing</h2>\n<p>Modern work requires secure secret sharing. KeyPears uses end-to-end encryption\nwith public/private key pairs for each user. When alice@example.com needs to\nshare a secret with bob@example2.com, they use Diffie-Hellman key exchange to\nderive a shared secret that only they know. The architecture mirrors email, but\nwith cryptography-first design.</p>\n<h2>More Than Passwords</h2>\n<p>While we call it a password manager, KeyPears is designed to handle:</p>\n<ul>\n<li>Passwords</li>\n<li>Cryptocurrency wallet keys</li>\n<li>API keys</li>\n<li>Environment variables</li>\n<li>SSH keys</li>\n<li>PGP keys</li>\n</ul>\n<p>For cryptocurrency users seeking self-custody and businesses that need secure\nsecret sharing without expensive enterprise subscriptions, KeyPears offers a\ncompelling alternative.</p>\n<h2>What's Next</h2>\n<p>KeyPears is in active development. We're building native applications for\nWindows, macOS, Linux, Android, and iOS using Tauri. The project is Apache 2.0\nlicensed and open source.</p>\n<p>This is just the beginning. We're excited to build KeyPears with the community\nand create a new standard for secure, self-custodied secret management.</p>\n<p>Stay tuned for more updates as we continue development.</p>",
      "url": "https://keypears.com/blog/2025-10-03-introducing-keypears",
      "title": "Introducing KeyPears: A New Approach to Password Management",
      "summary": "<p>We're excited to announce KeyPears, a new password manager designed for the\nmodern era of digital security and self-custody.</p>\n<h2>Why KeyPears?</h2>\n<p>Traditional password managers have served ...",
      "date_modified": "2025-10-03T11:00:00.000Z",
      "author": {
        "name": "KeyPears Team"
      }
    }
  ]
}
