{
    "version": "https://jsonfeed.org/version/1",
    "title": "KeyPears Blog",
    "home_page_url": "https://keypears.com",
    "feed_url": "https://keypears.com/blog/feed.json",
    "description": "Updates and insights from the KeyPears team",
    "items": [
        {
            "id": "https://keypears.com/blog/2025-10-07-progress-on-secret-synchronization",
            "content_html": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The\ndesign decisions described here represent our development approach and may\nevolve before our official release.</p>\n<p>We've made significant progress on KeyPears' secret synchronization\narchitecture. Today we're sharing how we redesigned our schema to support\ndiverse secret types while maintaining the small-sync-unit principle that makes\nour synchronization protocol efficient and reliable.</p>\n<h2>The Problem</h2>\n<p>Our original schema was built around passwords. It had fields like\n<code>encryptedPassword</code>, <code>username</code>, <code>domain</code>, and <code>notes</code>. This worked fine for\nbasic password management, but it created limitations:</p>\n<ul>\n<li><strong>Type inflexibility</strong>: How do you store an API key? A cryptocurrency wallet\nwith multiple components? Environment variables?</li>\n<li><strong>No grouping</strong>: Secrets existed in isolation. There was no way to represent\n\"this API token belongs to this account\" or \"these 20 environment variables\nform one .env file\"</li>\n<li><strong>No hierarchy</strong>: No folders, no organization beyond a flat list</li>\n<li><strong>KeePass import impossible</strong>: KeePass has groups (folders) and custom fields\n(additional key-value pairs per entry). We couldn't represent either.</li>\n</ul>\n<p>We needed a more flexible schema without abandoning our core architectural\nprinciple: <strong>every secret must sync independently</strong> to keep network overhead\nsmall and conflict resolution simple.</p>\n<h2>The Solution: Three Changes</h2>\n<p>We evolved the <code>SecretUpdate</code> schema with three key additions: multi-type\nsupport, dual hierarchy mechanisms, and JSON-based storage.</p>\n<h3>1. Multi-Type Support</h3>\n<p>First, we made the schema generic enough to handle any small secret:</p>\n<pre><code class=\"language-typescript\">type: \"password\" | \"envvar\" | \"apikey\" | \"walletkey\" | \"passkey\"\nencryptedData: string  // Previously: encryptedPassword\nencryptedNotes: string // Previously: notes\n</code></pre>\n<p>The <code>type</code> field distinguishes what kind of secret this is. The generic\n<code>encryptedData</code> field holds the actual secret value (password, API key, private\nkey, etc.). Password-specific fields like <code>domain</code>, <code>username</code>, and <code>email</code>\nremain in the schema but are optional—used primarily when <code>type</code> is <code>password</code>.</p>\n<p>This small change opens up KeyPears to handle:</p>\n<ul>\n<li><strong>Environment variables</strong>: Type <code>envvar</code>, name <code>DATABASE_URL</code>, encrypted value\nin <code>encryptedData</code></li>\n<li><strong>API keys</strong>: Type <code>apikey</code>, service name in a <code>label</code> field, key in\n<code>encryptedData</code></li>\n<li><strong>Wallet keys</strong>: Type <code>walletkey</code>, blockchain type in metadata, private key in\n<code>encryptedData</code></li>\n<li><strong>Passkeys</strong>: Type <code>passkey</code>, credential ID and public key in metadata,\nprivate key in <code>encryptedData</code></li>\n</ul>\n<h3>2. Dual Hierarchy: Folders and ParentId</h3>\n<p>The second change introduces two different hierarchy mechanisms, each serving a\nspecific purpose:</p>\n<pre><code class=\"language-typescript\">folders: string[]      // [\"Work\", \"Projects\", \"Client A\"]\ntags: string[]         // [\"production\", \"critical\"]\nparentId: string       // ULID of parent secret (max depth 1)\n</code></pre>\n<p><strong>Folders</strong> provide unlimited-depth organizational hierarchy. They're just an\narray of strings representing the path:</p>\n<pre><code class=\"language-typescript\">folders: [\"Work\", \"AWS\", \"Production\"]\nfolders: [\"Personal\", \"Banking\"]\nfolders: []  // Root level\n</code></pre>\n<p>This maps perfectly to KeePass Groups and lets users organize thousands of\nsecrets into a familiar folder structure.</p>\n<p><strong>Tags</strong> provide orthogonal categorization. A secret can have multiple tags for\ncross-cutting concerns:</p>\n<pre><code class=\"language-typescript\">tags: [\"production-env\", \"requires-rotation\", \"shared-with-team\"]\n</code></pre>\n<p><strong>ParentId</strong> creates actual parent-child relationships between secrets. This is\nwhere it gets interesting.</p>\n<h2>ParentId: Secrets Containing Secrets</h2>\n<p>The <code>parentId</code> field lets one secret \"contain\" other secrets. A simple example:</p>\n<pre><code class=\"language-typescript\">// Parent: The main account\n{\n  secretId: \"abc123\",\n  name: \"GitHub Account\",\n  type: \"password\",\n  encryptedData: \"&#x3C;main password>\"\n}\n\n// Child: API token for the same account\n{\n  secretId: \"def456\",\n  name: \"API Token\",\n  type: \"apikey\",\n  parentId: \"abc123\",\n  encryptedData: \"&#x3C;token>\"\n}\n</code></pre>\n<p>This models KeePass's custom fields—additional key-value pairs that belong to an\nentry. In KeePass, you might have a GitHub entry with standard fields (username,\npassword, URL) plus custom fields for API tokens, 2FA backup codes, or recovery\nemails.</p>\n<p>In KeyPears, each custom field becomes its own secret with a <code>parentId</code> pointing\nto the parent. Each syncs independently (small sync units!), but they're\nlogically grouped.</p>\n<h3>The Depth Limit: Security Through Simplicity</h3>\n<p>Here's the critical constraint: <strong>a secret can have a parent, but that parent\ncannot have a parent</strong>. Maximum depth is 1. No grandparents allowed.</p>\n<p>Why? Three reasons:</p>\n<p><strong>1. Security</strong>: Client-generated IDs open an attack vector for malicious\nclients creating circular references or extremely deep chains. With depth=1, the\nvalidation is trivial:</p>\n<pre><code class=\"language-typescript\">async function validateParentChain(secretId: string, parentId?: string) {\n  if (!parentId) return; // No parent, valid\n  if (parentId === secretId) throw new Error(\"Cannot self-reference\");\n\n  const parent = await getSecretHistory(parentId);\n  if (parent.length > 0 &#x26;&#x26; parent[0].parentId) {\n    throw new Error(\"Cannot nest more than one level deep\");\n  }\n}\n</code></pre>\n<p>One database lookup. No recursion. No visited sets. O(1) validation that\nattackers can't exploit.</p>\n<p><strong>2. Performance</strong>: Validating unlimited depth requires recursive queries.\nValidating depth=1 requires one query. Simple.</p>\n<p><strong>3. Sufficient for real use cases</strong>:</p>\n<ul>\n<li>Folder with secrets ✓</li>\n<li>Entry with custom fields ✓</li>\n<li>Environment variable group ✓</li>\n</ul>\n<p>What we lose: deeply nested folder hierarchies via <code>parentId</code>. But we have\n<code>folders</code> for that! The two mechanisms complement each other perfectly.</p>\n<h2>Why Two Hierarchy Systems?</h2>\n<p>It might seem redundant to have both <code>folders</code> and <code>parentId</code>, but they serve\ndifferent purposes:</p>\n<p><strong>Folders</strong> are for <strong>organizational hierarchy</strong>. They map to KeePass Groups.\nThey're pure metadata—just strings representing a path. They have unlimited\ndepth because they're just labels, not database relationships.</p>\n<p><strong>ParentId</strong> is for <strong>data relationships</strong>. It maps to KeePass custom fields. It\ncreates actual parent-child relationships where one secret logically contains\nothers. Each child syncs independently, maintaining small sync units.</p>\n<p>Together, they enable full KeePass import:</p>\n<pre><code class=\"language-typescript\">// KeePass structure:\n// Work/Projects/GitHub (Group path)\n//   - GitHub Account (Entry)\n//     - Username: alice\n//     - Password: ••••••\n//     - Custom: API Token (protected)\n//     - Custom: 2FA Codes (protected)\n\n// KeyPears representation:\n{\n  secretId: \"main\",\n  name: \"GitHub Account\",\n  type: \"password\",\n  folders: [\"Work\", \"Projects\", \"GitHub\"],\n  username: \"alice\",\n  encryptedData: \"&#x3C;password>\"\n}\n{\n  secretId: \"token\",\n  name: \"API Token\",\n  type: \"apikey\",\n  folders: [\"Work\", \"Projects\", \"GitHub\"], // Inherits folder\n  parentId: \"main\",\n  encryptedData: \"&#x3C;token>\"\n}\n{\n  secretId: \"codes\",\n  name: \"2FA Codes\",\n  type: \"password\",\n  folders: [\"Work\", \"Projects\", \"GitHub\"],\n  parentId: \"main\",\n  encryptedData: \"&#x3C;codes>\"\n}\n</code></pre>\n<p>The folder path provides organization. The <code>parentId</code> relationships show which\nsecrets belong together. Each secret syncs independently.</p>\n<h2>JSON-Based Storage: Migration-Proof Architecture</h2>\n<p>The third major change is how we store secrets in the database. We moved to a\nhybrid approach:</p>\n<pre><code class=\"language-sql\">CREATE TABLE secret_update (\n  id TEXT PRIMARY KEY,\n  vault_id TEXT NOT NULL,\n  secret_id TEXT NOT NULL,\n  name TEXT NOT NULL,\n  type TEXT NOT NULL DEFAULT 'password',\n  parent_id TEXT,\n  created_at INTEGER NOT NULL,\n  deleted INTEGER NOT NULL DEFAULT 0,\n\n  -- Source of truth: full JSON object\n  secret_update_json TEXT NOT NULL\n);\n\nCREATE INDEX idx_secret_updates_name ON secret_update(name);\nCREATE INDEX idx_secret_updates_type ON secret_update(type);\nCREATE INDEX idx_secret_updates_parent_id ON secret_update(parent_id);\n</code></pre>\n<p>Notice what's happening here. We store the <strong>entire <code>SecretUpdate</code> object</strong> as\nJSON in <code>secret_update_json</code>. The other columns (<code>name</code>, <code>type</code>, <code>parent_id</code>,\netc.) are duplicates of data from the JSON, extracted for indexing.</p>\n<p>The JSON is the source of truth. The columns are for performance.</p>\n<h3>Why This Approach?</h3>\n<p><strong>Adding fields requires no migration</strong>. Want to add a <code>label</code> field? Update the\nZod schema, start writing it to the JSON, and you're done. The database doesn't\ncare—it's just storing JSON.</p>\n<p>When we added <code>parentId</code> to the schema, we:</p>\n<ol>\n<li>Updated the Zod schema in TypeScript</li>\n<li>Added <code>parent_id</code> column to the database (for indexing)</li>\n<li>Started serializing <code>parentId</code> to the JSON</li>\n</ol>\n<p>Users with existing vaults see <code>parentId: undefined</code> in their JSON. No\nmigration, no data transformation. Just works.</p>\n<p>This architecture is <strong>future-proof</strong>. We can evolve the schema rapidly during\ndevelopment without worrying about breaking existing databases.</p>\n<h3>When We Ship v1.0</h3>\n<p>Before our first production release, we'll generate one clean migration from the\nfinal schema. That becomes our baseline. After that, we'll only add new\nmigrations—never delete old ones—because users will have the old migrations\napplied.</p>\n<p>But during development? We delete and regenerate migrations freely. The JSON\nstorage strategy makes this painless.</p>\n<h2>What This Enables</h2>\n<p>With these changes in place, KeyPears can now handle:</p>\n<h3>KeePass Import (Future Feature)</h3>\n<p>Full KeePass <code>.kdbx</code> import support, including:</p>\n<ul>\n<li>Nested groups → <code>folders</code> array</li>\n<li>Entries → secrets with <code>type: \"password\"</code></li>\n<li>Custom protected fields → child secrets with <code>parentId</code></li>\n<li>Entry metadata → password-specific fields</li>\n</ul>\n<p>The only thing we won't import: file attachments. By design. We're optimizing\nfor small secrets that sync efficiently.</p>\n<h3>Environment Variables</h3>\n<p>Create a parent secret \"Production Environment\" and attach child secrets for\neach variable:</p>\n<pre><code class=\"language-typescript\">{ name: \"Production Env\", type: \"folder\" }  // Parent\n{ name: \"DATABASE_URL\", type: \"envvar\", parentId: \"...\" }\n{ name: \"API_SECRET\", type: \"envvar\", parentId: \"...\" }\n{ name: \"STRIPE_KEY\", type: \"envvar\", parentId: \"...\" }\n</code></pre>\n<p>Or use tags instead:</p>\n<pre><code class=\"language-typescript\">{ name: \"DATABASE_URL\", type: \"envvar\", tags: [\"prod-env\"] }\n{ name: \"API_SECRET\", type: \"envvar\", tags: [\"prod-env\"] }\n</code></pre>\n<p>Both approaches work. <code>parentId</code> creates explicit grouping. Tags create implicit\nsets.</p>\n<h3>Cryptocurrency Wallets</h3>\n<p>Store wallet keys with relevant metadata:</p>\n<pre><code class=\"language-typescript\">{\n  name: \"Ethereum Main Wallet\",\n  type: \"walletkey\",\n  encryptedData: \"&#x3C;private key>\",\n  folders: [\"Crypto\", \"Ethereum\"],\n  tags: [\"high-value\", \"cold-storage\"]\n}\n</code></pre>\n<h3>API Keys with Secrets</h3>\n<p>Store API key pairs as parent-child:</p>\n<pre><code class=\"language-typescript\">{ name: \"Stripe\", type: \"apikey\", encryptedData: \"&#x3C;public key>\" }\n{ name: \"Secret Key\", type: \"apikey\", parentId: \"...\", encryptedData: \"&#x3C;secret>\" }\n</code></pre>\n<h2>Synchronization Properties</h2>\n<p>These changes maintain our core synchronization principles:</p>\n<p><strong>Small sync units</strong>: Each secret syncs independently. A 50-entry KeePass import\nbecomes 50 individual secrets, each a few hundred bytes. If two users edit\ndifferent entries, no conflicts.</p>\n<p><strong>Atomic updates</strong>: Each <code>SecretUpdate</code> is immutable once created. Updates\ncreate new records in an append-only log. The latest update wins\n(last-write-wins conflict resolution).</p>\n<p><strong>Efficient</strong>: Only changed secrets sync. If you update one child secret, you\nsync one small object, not the entire parent-child group.</p>\n<p><strong>Validated</strong>: The <code>parentId</code> depth limit prevents malicious clients from\ncreating expensive recursive structures.</p>\n<h2>Looking Ahead</h2>\n<p>This schema evolution lays the groundwork for several future features:</p>\n<ul>\n<li><strong>UI for child secrets</strong>: Show \"API Token\" nested under \"GitHub Account\" in\nthe secret list</li>\n<li><strong>KeePass import</strong>: Full <code>.kdbx</code> file import with groups and custom fields</li>\n<li><strong>Environment variable templates</strong>: Quick creation of common .env structures</li>\n<li><strong>Bulk operations</strong>: Delete/restore an entire group by operating on all\nchildren</li>\n</ul>\n<p>The foundation is solid. The architecture is flexible. The sync protocol remains\nsimple and efficient.</p>\n<p>We're building KeyPears to be more than a password manager—it's a secure,\nself-custodied secret manager that handles everything from passwords to\nenvironment variables to cryptocurrency keys. And it all syncs seamlessly across\nyour devices without trusting a central authority with your encryption keys.</p>\n<h2>Technical Details</h2>\n<p>For those interested in the implementation:</p>\n<ul>\n<li><strong>Schema definition</strong>: Zod validation in TypeScript, ensuring type safety</li>\n<li><strong>Database</strong>: SQLite via Drizzle ORM with the sqlite-proxy adapter</li>\n<li><strong>Migration</strong>: Custom migration runner that tracks applied migrations</li>\n<li><strong>Validation</strong>: O(1) parent chain validation with single database lookup</li>\n<li><strong>Indexes</strong>: <code>name</code>, <code>type</code>, <code>parent_id</code>, and composite\n<code>vault_id + secret_id + created_at</code></li>\n</ul>\n<p>The code is Apache 2.0 licensed and available on GitHub. We're building in the\nopen, one commit at a time.</p>\n<p>More updates coming soon.</p>",
            "url": "https://keypears.com/blog/2025-10-07-progress-on-secret-synchronization",
            "title": "Progress on Secret Synchronization: A Future-Proof Schema",
            "summary": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The\ndesign decisions described here represent our development approach and may\nevolve before our official release...",
            "date_modified": "2025-10-07T00:00:00.000Z",
            "author": {
                "name": "KeyPears Team"
            }
        },
        {
            "id": "https://keypears.com/blog/2025-10-05-vault-encryption-key-derivation",
            "content_html": "<p>One of the core security features of KeyPears is how we protect your vault\nencryption keys. Today, we're diving deep into our three-tier key derivation\nsystem and explaining how we ensure that even if a server is compromised, your\nencrypted data remains secure.</p>\n<h2>The Problem: Authentication vs. Encryption</h2>\n<p>Most password managers face a fundamental challenge: you need to send\n<em>something</em> to the server to prove who you are, but you also need to keep your\nencryption key secret so the server can't decrypt your vault. Using the same key\nfor both purposes creates a security vulnerability—if the server is compromised,\nan attacker gains access to your encryption key.</p>\n<p>KeyPears solves this by deriving two separate keys from your password: one for\nlogging in to the server, and one for encrypting your vault. The server only\never sees the login key, never the encryption key.</p>\n<h2>Three-Tier Key Derivation</h2>\n<p>When you create a vault in KeyPears, we don't just hash your password once.\nInstead, we use a three-tier key derivation system:</p>\n<pre><code>Master Password\n  ↓ blake3Pbkdf (100,000 rounds)\nPassword Key (stored encrypted with PIN on device)\n  ↓\n  ├→ blake3Pbkdf (100,000 rounds) → Encryption Key\n  └→ blake3Pbkdf (100,000 rounds) → Login Key\n</code></pre>\n<h3>1. Password Key: The Root of Trust</h3>\n<p>The first step derives a <strong>password key</strong> from your master password using\n100,000 rounds of our Blake3-based PBKDF. This intermediate key is stored on\nyour device, encrypted with your PIN for quick unlock. It never leaves your\ndevice and is never sent to any server.</p>\n<p>The password key acts as the root of trust for deriving the other two keys.</p>\n<h3>2. Encryption Key: Protecting Your Vault</h3>\n<p>From the password key, we derive an <strong>encryption key</strong> through another 100,000\nrounds of Blake3 PBKDF. This key is used for one purpose only: encrypting and\ndecrypting your master vault key.</p>\n<p>Wait—encrypting a key with another key? Yes! Your vault itself is encrypted with\na randomly generated <strong>master vault key</strong>. This master key is immutable and\nnever changes. The encryption key derived from your password is used to encrypt\nthis master vault key before storing it in the database.</p>\n<p>This architecture allows you to change your password without re-encrypting your\nentire vault—we just re-encrypt the master vault key with the new encryption\nkey.</p>\n<p>The encryption key is ephemeral. We derive it when needed, use it immediately,\nand discard it. It is never persisted to disk and never sent anywhere.</p>\n<h3>3. Login Key: Server Authentication</h3>\n<p>The third key in our hierarchy is the <strong>login key</strong>, also derived from the\npassword key through 100,000 rounds of Blake3 PBKDF. This is the only key that\ngets sent to the server for authentication.</p>\n<p>Because the login key is derived separately from the encryption key,\ncompromising one doesn't compromise the other. Even if a server is breached and\nthe login key is stolen, the attacker cannot derive the encryption key needed to\ndecrypt your vault.</p>\n<h2>Blake3 PBKDF: Fast and Secure</h2>\n<p>You might notice we're using 100,000 rounds of Blake3 PBKDF rather than a\nstandard algorithm like PBKDF2. Blake3 is a modern, extremely fast cryptographic\nhash function. Even at 100,000 rounds, the entire key derivation completes in\nmilliseconds on modern hardware.</p>\n<p>Our Blake3-based PBKDF works by iteratively applying Blake3's keyed MAC mode:</p>\n<pre><code>Round 1: result = blake3Mac(salt, password)\nRound 2: result = blake3Mac(salt, result_from_round_1)\n...\nRound 100,000: result = blake3Mac(salt, result_from_round_99,999)\n</code></pre>\n<p>Each round adds computational cost for attackers trying to brute-force your\npassword, while remaining fast enough for legitimate use.</p>\n<h2>Salt Derivation</h2>\n<p>Each key derivation uses a different salt to ensure cryptographic separation:</p>\n<ul>\n<li>\n<p><strong>Password Salt</strong>: Derived deterministically from your password using\n<code>blake3Mac(blake3Hash(\"KeyPears password salt v1\"), password)</code>. This ensures\nthe same password always produces the same password key.</p>\n</li>\n<li>\n<p><strong>Encryption Salt</strong>: A global constant\n<code>blake3Hash(\"KeyPears encryption salt v1\")</code> used for all users. This is safe\nbecause the encryption key is derived from the password key, not directly from\nthe password.</p>\n</li>\n<li>\n<p><strong>Login Salt</strong>: Another global constant\n<code>blake3Hash(\"KeyPears login salt v1\")</code>. Again, safe because it's derived from\nthe password key.</p>\n</li>\n</ul>\n<h2>Security Properties</h2>\n<p>This architecture provides several important security guarantees:</p>\n<h3>Defense Against Server Compromise</h3>\n<p>If a KeyPears server is compromised, the attacker gains access to:</p>\n<ul>\n<li>Encrypted vault data</li>\n<li>Login keys for authentication</li>\n</ul>\n<p>The attacker does NOT gain access to:</p>\n<ul>\n<li>Master passwords</li>\n<li>Password keys</li>\n<li>Encryption keys</li>\n<li>Master vault keys</li>\n<li>Decrypted vault contents</li>\n</ul>\n<p>Without the encryption key, the encrypted vault data is useless to the attacker.</p>\n<h3>Defense Against Encrypted Data Theft</h3>\n<p>If someone steals your encrypted vault data but doesn't have your credentials:</p>\n<ul>\n<li>They cannot decrypt it without the encryption key</li>\n<li>The encryption key requires the password key</li>\n<li>The password key requires your master password</li>\n<li>100,000 rounds of Blake3 PBKDF make brute-forcing expensive</li>\n</ul>\n<h3>Key Separation</h3>\n<p>The three keys are cryptographically isolated. Knowing the login key doesn't\nhelp you derive the encryption key, and vice versa. Both require the password\nkey, which requires the master password.</p>\n<h2>The Vault Key Hash: Verification</h2>\n<p>When you enter your password to unlock a vault, KeyPears needs to verify you\nentered it correctly. We do this by storing a Blake3 hash of the master vault\nkey in the database.</p>\n<p>When you unlock:</p>\n<ol>\n<li>Derive password key from your password</li>\n<li>Derive encryption key from password key</li>\n<li>Decrypt the master vault key using the encryption key</li>\n<li>Hash the decrypted master vault key</li>\n<li>Compare with the stored hash</li>\n</ol>\n<p>If the hashes match, you entered the correct password. If not, the password is\nwrong. This verification happens entirely on your device—the master vault key\nnever leaves your device, even temporarily.</p>\n<h2>Putting It All Together</h2>\n<p>Here's what happens when you create a new vault:</p>\n<ol>\n<li>You enter a master password</li>\n<li>KeyPears derives a password key (100k rounds Blake3)</li>\n<li>Derives an encryption key from the password key (100k rounds Blake3)</li>\n<li>Generates a random master vault key</li>\n<li>Encrypts the master vault key with the encryption key</li>\n<li>Hashes the master vault key for verification</li>\n<li>Stores the encrypted vault key and hash in your local database</li>\n</ol>\n<p>When you sync to a server:</p>\n<ol>\n<li>Derive the login key from your password key (100k rounds Blake3)</li>\n<li>Send the login key to the server for authentication</li>\n<li>Server returns your encrypted master vault key (and other encrypted vault\ndata)</li>\n<li>Derive the encryption key (never sent to server)</li>\n<li>Decrypt the master vault key locally</li>\n<li>Use the master vault key to decrypt your secrets</li>\n</ol>\n<p>The server facilitates synchronization but never has the keys needed to decrypt\nyour data.</p>\n<h2>Looking Ahead</h2>\n<p>This architecture lays the foundation for secure sharing between users. In\nfuture posts, we'll explore how KeyPears uses Diffie-Hellman key exchange to\nshare secrets securely between users, and how the master vault key enables\nefficient re-encryption without re-deriving keys.</p>\n<p>For now, the key takeaway is simple: KeyPears separates authentication from\nencryption. Your server can verify who you are without ever having the ability\nto decrypt your data. It's cryptography working exactly as it should.</p>",
            "url": "https://keypears.com/blog/2025-10-05-vault-encryption-key-derivation",
            "title": "How KeyPears Protects Your Vault: Encryption and Key Derivation",
            "summary": "<p>One of the core security features of KeyPears is how we protect your vault\nencryption keys. Today, we're diving deep into our three-tier key derivation\nsystem and explaining how we ensure that even...",
            "date_modified": "2025-10-05T00:00:00.000Z",
            "author": {
                "name": "KeyPears Team"
            }
        },
        {
            "id": "https://keypears.com/blog/2025-10-04-drizzle-sqlite-tauri",
            "content_html": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The\nsolutions described here are part of our development process and may evolve\nbefore our official release.</p>\n<h2>The Problem</h2>\n<p>Building a local-first application with Tauri 2.0, we needed a robust database\nsolution for storing encrypted vault data on users' devices. We wanted:</p>\n<ul>\n<li>Type-safe database queries</li>\n<li>Proper schema migrations that work in production</li>\n<li>Pure TypeScript implementation (no Rust for basic DB operations)</li>\n<li>A solution that works across desktop and mobile platforms</li>\n</ul>\n<p>After evaluating options, we chose <strong>Drizzle ORM</strong> with <strong>SQLite</strong> via the\nofficial <strong>tauri-plugin-sql</strong>. This combination gives us TypeScript-first\ndevelopment with the reliability of SQLite.</p>\n<h2>The Challenge</h2>\n<p>Unlike traditional Node.js environments where you have direct filesystem access\nand can use drivers like <code>better-sqlite3</code>, Tauri's sandboxed environment\nrequires a different approach. Drizzle's standard migration tools assume direct\ndatabase access, but with Tauri, we need to go through the plugin system.</p>\n<p>Here's how we solved it.</p>\n<h2>Tech Stack</h2>\n<ul>\n<li><strong>Tauri 2.0</strong> - Cross-platform app framework</li>\n<li><strong>Drizzle ORM</strong> - TypeScript ORM</li>\n<li><strong>drizzle-kit</strong> - Schema migration generator</li>\n<li><strong>@tauri-apps/plugin-sql</strong> - Official Tauri SQLite plugin</li>\n<li><strong>React Router</strong> - For app routing and loaders</li>\n</ul>\n<h2>Step 1: Install Dependencies</h2>\n<p>First, add the necessary packages:</p>\n<pre><code class=\"language-bash\"># Production dependencies\npnpm add drizzle-orm @tauri-apps/plugin-sql\n\n# Development dependencies\npnpm add -D drizzle-kit\n</code></pre>\n<p>Then add the Tauri plugin to your Rust dependencies in <code>src-tauri/Cargo.toml</code>:</p>\n<pre><code class=\"language-toml\">[dependencies]\ntauri-plugin-sql = { version = \"2\", features = [\"sqlite\"] }\n</code></pre>\n<h2>Step 2: Configure Tauri Permissions</h2>\n<p>Tauri 2.0 requires explicit permission grants. Add SQL permissions to\n<code>src-tauri/capabilities/default.json</code>:</p>\n<pre><code class=\"language-json\">{\n  \"$schema\": \"../gen/schemas/desktop-schema.json\",\n  \"identifier\": \"default\",\n  \"description\": \"Capability for the main window\",\n  \"windows\": [\"main\"],\n  \"permissions\": [\n    \"core:default\",\n    \"sql:default\",\n    \"sql:allow-load\",\n    \"sql:allow-execute\",\n    \"sql:allow-select\",\n    \"sql:allow-close\"\n  ]\n}\n</code></pre>\n<p>Without these permissions, you'll get \"not allowed\" errors when trying to access\nthe database.</p>\n<h2>Step 3: Define Your Schema</h2>\n<p>Create your Drizzle schema at <code>app/db/schema.ts</code>:</p>\n<pre><code class=\"language-typescript\">import { sqliteTable, text, integer } from \"drizzle-orm/sqlite-core\";\n\nexport const vaults = sqliteTable(\"vaults\", {\n  id: integer(\"id\").primaryKey({ autoIncrement: true }),\n  name: text(\"name\").notNull().unique(),\n});\n</code></pre>\n<h2>Step 4: Set Up the SQLite Proxy</h2>\n<p>Since we can't use standard SQLite drivers in Tauri, we use Drizzle's\n<code>sqlite-proxy</code> adapter. Create <code>app/db/index.ts</code>:</p>\n<pre><code class=\"language-typescript\">import { drizzle } from \"drizzle-orm/sqlite-proxy\";\nimport Database from \"@tauri-apps/plugin-sql\";\nimport * as schema from \"./schema\";\n\nexport async function getDb() {\n  return await Database.load(\"sqlite:keypears.db\");\n}\n\nfunction isSelectQuery(sql: string): boolean {\n  return sql.trim().toLowerCase().startsWith(\"select\");\n}\n\nexport const db = drizzle&#x3C;typeof schema>(\n  async (sql, params, method) => {\n    const sqlite = await getDb();\n    let rows: any = [];\n\n    if (isSelectQuery(sql)) {\n      rows = await sqlite.select(sql, params).catch((e) => {\n        console.error(\"SQL Error:\", e);\n        return [];\n      });\n    } else {\n      rows = await sqlite.execute(sql, params).catch((e) => {\n        console.error(\"SQL Error:\", e);\n        return [];\n      });\n      return { rows: [] };\n    }\n\n    rows = rows.map((row: any) => Object.values(row));\n    const results = method === \"all\" ? rows : rows[0];\n    await sqlite.close();\n    return { rows: results };\n  },\n  { schema: schema, logger: true }\n);\n</code></pre>\n<p>The proxy adapter translates Drizzle queries into calls to the Tauri SQL plugin.</p>\n<h2>Step 5: Configure Migration Generation</h2>\n<p>Create <code>drizzle.config.ts</code>:</p>\n<pre><code class=\"language-typescript\">import type { Config } from \"drizzle-kit\";\n\nexport default {\n  schema: \"./app/db/schema.ts\",\n  out: \"./app/db/migrations\",\n  dialect: \"sqlite\",\n} satisfies Config;\n</code></pre>\n<p>Add a script to <code>package.json</code>:</p>\n<pre><code class=\"language-json\">{\n  \"scripts\": {\n    \"db:migrate\": \"drizzle-kit generate\"\n  }\n}\n</code></pre>\n<h2>Step 6: Implement Migration Runner</h2>\n<p>Here's the key part - implementing our own migration system. Create\n<code>app/db/migrate.ts</code>:</p>\n<pre><code class=\"language-typescript\">import { getDb } from \"./index\";\n\n// Dynamically import all SQL migration files\nconst migrationFiles = import.meta.glob&#x3C;string>(\"./migrations/*.sql\", {\n  query: \"?raw\",\n  import: \"default\",\n  eager: true,\n});\n\n// Create migrations tracking table\nasync function ensureMigrationsTable() {\n  const sqlite = await getDb();\n  await sqlite.execute(`\n    CREATE TABLE IF NOT EXISTS __drizzle_migrations (\n      id INTEGER PRIMARY KEY AUTOINCREMENT,\n      hash TEXT NOT NULL UNIQUE,\n      created_at INTEGER NOT NULL\n    )\n  `);\n  await sqlite.close();\n}\n\n// Get list of applied migrations\nasync function getAppliedMigrations(): Promise&#x3C;string[]> {\n  const sqlite = await getDb();\n  const rows = await sqlite\n    .select&#x3C;Array&#x3C;{ hash: string }>>(\n      \"SELECT hash FROM __drizzle_migrations ORDER BY id\"\n    )\n    .catch(() => []);\n  await sqlite.close();\n  return rows.map((row) => row.hash);\n}\n\n// Record migration as applied\nasync function recordMigration(hash: string) {\n  const sqlite = await getDb();\n  const timestamp = Date.now();\n  await sqlite.execute(\n    \"INSERT INTO __drizzle_migrations (hash, created_at) VALUES (?, ?)\",\n    [hash, timestamp]\n  );\n  await sqlite.close();\n}\n\n// Execute SQL file\nasync function executeSqlFile(sqlContent: string) {\n  const sqlite = await getDb();\n  const statements = sqlContent\n    .split(\"--> statement-breakpoint\")\n    .map((s) => s.trim())\n    .filter((s) => s.length > 0);\n\n  for (const statement of statements) {\n    await sqlite.execute(statement).catch((e) => {\n      console.error(\"Migration error:\", e);\n      throw e;\n    });\n  }\n\n  await sqlite.close();\n}\n\nexport async function runMigrations() {\n  console.log(\"Running database migrations...\");\n\n  try {\n    await ensureMigrationsTable();\n    const appliedMigrations = await getAppliedMigrations();\n\n    const migrationPaths = Object.keys(migrationFiles).sort();\n\n    const pendingMigrations = migrationPaths.filter((path) => {\n      const filename = path.split(\"/\").pop() || path;\n      return !appliedMigrations.includes(filename);\n    });\n\n    if (pendingMigrations.length === 0) {\n      console.log(\"All migrations already applied\");\n      return;\n    }\n\n    for (const path of pendingMigrations) {\n      const filename = path.split(\"/\").pop() || path;\n      const migrationContent = migrationFiles[path];\n\n      console.log(`Executing migration: ${filename}`);\n      await executeSqlFile(migrationContent);\n      await recordMigration(filename);\n      console.log(`✓ Applied: ${filename}`);\n    }\n\n    console.log(`Successfully completed ${pendingMigrations.length} migration(s)`);\n  } catch (error) {\n    console.error(\"Migration failed:\", error);\n    throw error;\n  }\n}\n</code></pre>\n<p>This implements Drizzle's migration tracking pattern:</p>\n<ul>\n<li>Uses <code>__drizzle_migrations</code> table to track applied migrations</li>\n<li>Only runs new migrations on subsequent app launches</li>\n<li>Supports incremental migrations as your schema evolves</li>\n</ul>\n<h2>Step 7: Run Migrations on App Startup</h2>\n<p>In your root component (<code>app/root.tsx</code>), use a clientLoader to run migrations\nbefore rendering:</p>\n<pre><code class=\"language-typescript\">import { runMigrations } from \"./db/migrate\";\n\nexport async function clientLoader() {\n  await runMigrations();\n  return null;\n}\n\nexport function HydrateFallback() {\n  return (\n    &#x3C;div className=\"flex min-h-screen items-center justify-center\">\n      &#x3C;h1>Migrating the database...&#x3C;/h1>\n    &#x3C;/div>\n  );\n}\n</code></pre>\n<p>React Router will show the fallback while migrations run, ensuring the database\nis ready before any component renders.</p>\n<h2>Step 8: Create Model Functions</h2>\n<p>With everything set up, create type-safe model functions at\n<code>app/db/models/vault.ts</code>:</p>\n<pre><code class=\"language-typescript\">import { db } from \"../index\";\nimport { vaults } from \"../schema\";\nimport { eq, count } from \"drizzle-orm\";\n\nexport interface Vault {\n  id: number;\n  name: string;\n}\n\nexport async function createVault(name: string): Promise&#x3C;Vault> {\n  const result = await db.insert(vaults).values({ name }).returning();\n  return result[0];\n}\n\nexport async function getVault(id: number): Promise&#x3C;Vault | undefined> {\n  const result = await db.select().from(vaults).where(eq(vaults.id, id));\n  return result[0];\n}\n\nexport async function getVaults(): Promise&#x3C;Vault[]> {\n  return await db.select().from(vaults);\n}\n\nexport async function countVaults(): Promise&#x3C;number> {\n  const result = await db.select({ count: count() }).from(vaults);\n  return result[0]?.count ?? 0;\n}\n</code></pre>\n<h2>Usage Workflow</h2>\n<h3>Development</h3>\n<p>When you modify your schema:</p>\n<pre><code class=\"language-bash\"># 1. Update app/db/schema.ts\n# 2. Generate new migration\npnpm run db:migrate\n\n# 3. Restart app - migration runs automatically\n</code></pre>\n<p>During development, you can safely delete all migrations and regenerate them\nfrom scratch. Just delete the database file and migration files, then\nregenerate.</p>\n<h3>Production</h3>\n<p>Before releasing v1.0:</p>\n<ol>\n<li>Delete all development migrations</li>\n<li>Generate one clean migration from your final schema</li>\n<li>Commit this as your baseline</li>\n</ol>\n<p>After release, <strong>never delete migrations</strong> - only add new ones. Users will have\nthe old migrations applied, and new migrations build incrementally.</p>\n<h2>Database File Location</h2>\n<p>The Tauri SQL plugin creates the database in the app's data directory:</p>\n<ul>\n<li><strong>macOS</strong>: <code>~/Library/Application Support/{app-identifier}/keypears.db</code></li>\n<li><strong>Linux</strong>: <code>~/.local/share/{app-identifier}/keypears.db</code></li>\n<li><strong>Windows</strong>: <code>%APPDATA%\\{app-identifier}\\keypears.db</code></li>\n</ul>\n<h2>Troubleshooting</h2>\n<p><strong>Permission errors</strong>: Make sure you've added all SQL permissions to\n<code>capabilities/default.json</code></p>\n<p><strong>Migration fails</strong>: Check browser console in the Tauri webview for detailed\nerror messages</p>\n<p><strong>Type errors</strong>: Run <code>pnpm run typecheck</code> to catch issues before runtime</p>\n<h2>Conclusion</h2>\n<p>This setup gives us:</p>\n<ul>\n<li>✅ Type-safe database queries with Drizzle</li>\n<li>✅ Proper migration tracking that works in production</li>\n<li>✅ Pure TypeScript - no Rust code needed for basic operations</li>\n<li>✅ Cross-platform compatibility (desktop &#x26; mobile)</li>\n<li>✅ Incremental migrations as the schema evolves</li>\n</ul>\n<p>The combination of Drizzle's <code>sqlite-proxy</code> adapter with Tauri's SQL plugin\nprovides a robust foundation for local-first data storage. While we had to\nimplement our own migration runner, we followed Drizzle's patterns to ensure\ncompatibility and maintainability.</p>\n<h2>Resources</h2>\n<ul>\n<li><a href=\"https://orm.drizzle.team/\">Drizzle ORM</a></li>\n<li><a href=\"https://v2.tauri.app/plugin/sql/\">Tauri SQL Plugin</a></li>\n<li><a href=\"https://v2.tauri.app/security/capabilities/\">Tauri Capabilities</a></li>\n</ul>",
            "url": "https://keypears.com/blog/2025-10-04-drizzle-sqlite-tauri",
            "title": "Drizzle SQLite Database Migrations in Tauri 2.0",
            "summary": "<p><strong>Note:</strong> KeyPears is a work-in-progress open-source password manager. The\nsolutions described here are part of our development process and may evolve\nbefore our official release.</p>\n...",
            "date_modified": "2025-10-04T00:00:00.000Z",
            "author": {
                "name": "KeyPears Team"
            }
        },
        {
            "id": "https://keypears.com/blog/2025-10-03-introducing-keypears",
            "content_html": "<p>We're excited to announce KeyPears, a new password manager designed for the\nmodern era of digital security and self-custody.</p>\n<h2>Why KeyPears?</h2>\n<p>Traditional password managers have served us well, but they come with\nlimitations. Most rely on centralized services, creating single points of\nfailure and raising questions about who truly controls your most sensitive data.\nKeyPears takes a different approach.</p>\n<h2>Local-First, Sync-Enabled</h2>\n<p>KeyPears is built on a local-first architecture. Your secrets live on your\ndevices, encrypted with keys only you control. But unlike purely local\nsolutions, KeyPears solves the synchronization problem through a permissionless\nmarketplace of third-party service providers using an open protocol—similar to\nhow email works.</p>\n<p>Anyone can run a KeyPears node. The protocol is open source. You maintain full\nself-custody while enjoying seamless synchronization across all your devices.</p>\n<h2>Built for Sharing</h2>\n<p>Modern work requires secure secret sharing. KeyPears uses end-to-end encryption\nwith public/private key pairs for each user. When alice@example.com needs to\nshare a secret with bob@example2.com, they use Diffie-Hellman key exchange to\nderive a shared secret that only they know. The architecture mirrors email, but\nwith cryptography-first design.</p>\n<h2>More Than Passwords</h2>\n<p>While we call it a password manager, KeyPears is designed to handle:</p>\n<ul>\n<li>Passwords</li>\n<li>Cryptocurrency wallet keys</li>\n<li>API keys</li>\n<li>Environment variables</li>\n<li>SSH keys</li>\n<li>PGP keys</li>\n</ul>\n<p>For cryptocurrency users seeking self-custody and businesses that need secure\nsecret sharing without expensive enterprise subscriptions, KeyPears offers a\ncompelling alternative.</p>\n<h2>What's Next</h2>\n<p>KeyPears is in active development. We're building native applications for\nWindows, macOS, Linux, Android, and iOS using Tauri. The project is Apache 2.0\nlicensed and open source.</p>\n<p>This is just the beginning. We're excited to build KeyPears with the community\nand create a new standard for secure, self-custodied secret management.</p>\n<p>Stay tuned for more updates as we continue development.</p>",
            "url": "https://keypears.com/blog/2025-10-03-introducing-keypears",
            "title": "Introducing KeyPears: A New Approach to Password Management",
            "summary": "<p>We're excited to announce KeyPears, a new password manager designed for the\nmodern era of digital security and self-custody.</p>\n<h2>Why KeyPears?</h2>\n<p>Traditional password managers have served ...",
            "date_modified": "2025-10-03T00:00:00.000Z",
            "author": {
                "name": "KeyPears Team"
            }
        }
    ]
}